## 1. 课程计划

###### 1 、介绍项目背景
###### 2 、项目总体概述
###### 3 、项目功能描述
###### 4 、项目架构
###### 5 、防爬规则
###### 6 、数据采集模块-openresty安装
###### 7 、lua语法入门
###### 8 、openresty案例入门

## 2. 项目背景

  很多订票网站的官网在互联网提供查询、预订等服务，如：各种航空公司的官网、去哪网、携程等，有大量正常用户访问的同时，也存在大量爬虫。爬虫消耗了系统资源，但是却 没有转化成销量，导致系统资源虚耗，严重时会造成系统波动，影响正常用户访问购票。通过系统日志分析等，发现官网访问中存在大量爬虫，且通过大量的 IP 进行伪装。 大量热门路线的好车次和航线的特价舱位吸引正常用户通过官网订票的同时，也存在大 量恶意占座的非法代理。通过不断的订座但不支付，利用这些虚占的座位进行非法盈利，通 过系统日志分析等，发现官网存在大量的非法占座会员及非会员手机号用户。 为了限制伪装技术越来越强的爬虫访问和恶意占座行为，需要开发大数据防爬工具。

## 3. 项目概述

#### 3. 1. 系统功能

###### 数据管理模块化

###### 1 、支持对数据采集和分类的配置和管理

###### 2 、支持对数据各种处理的配置和管理

###### 流程管理模块

###### 1 、支持流程定义和可配置管理

###### 2 、根据监控模块反馈的情况，系统自动执行相应的策略

###### 3 、支持人工执行相应的策略

###### 策略管理模块

###### 1 、支持策略的定义和配置管理

###### 2 、配置爬虫和占座行为匹配成功后的系统或人工应对方法

###### 3 、关联爬虫和占座行为与机器学习算法的选择和算法参数的阈值

###### 规则管理模块

###### 1 、支持规则定义和可配置管理

###### 2 、支持人工制定爬虫和占座过滤规则

###### 3 、支持定制规则参数阈值

###### 实时监控模块

###### 1 、对系统所采集的各种数据和特征进行实时监控和深度分析

###### 2 、通过模型分析和预测，智能识别和判断爬虫活动与性质

###### 3 、通过监控系统反馈的稳定性和负载情况，间接反映可能的爬虫活

###### 动情况

###### 数据可视化管理模块

###### 1 、将人工难以识别的数据制定图表或图形化，利于从中发现规律和

###### 价值

###### 2 、提供各种内置标准报表（转化率、爬取规律、占座规律、爬虫对

###### 查定比及系统稳定性的影响等）

### 3. 2. 数据流程描述

1 ) 数据管理模块对数据进行采集和处理配置，将采集的数据输出到kafka中间件，供实时

```
处理模块消费。
```
2 ) 实时监控监控模块通过监控爬虫相关的指标数据，发现异常数据，找到可能的爬虫数据；

3 ) 将爬虫数据过滤出来，存放到redis，供业务端通过lua过滤爬虫；

## 4. 逻辑架构设计


## 5. 功能描述

### 5. 1. 首页

###### 用户通过账号密码登录系统：

###### 登录系统后进入首页，首页放置实时监控大盘，展示多维度的监控数据，数据展示维度

###### 主要包括：对爬虫的监控与分析、对系统功能运行的监控以及对系统性能的监控。

###### 监控大盘首页示意图


###### 性能监控示意图

###### 首页监控大盘与数据可视化管理模块共同将项目目标及项目技术指标达成情况进行可

###### 视化输出，直观反映用户识别、数据爬取、非法占座、系统稳定、分析速度、数据量及负载、

###### 识别准确率等情况。

### 5. 2. 数据管理产品模块

### 5. 2. 1. 数据采集的配置和管理

本系统数据采集主要是对流量数据的采集，从而获取来自官网用户查询和预定的行为记录，这些行为数据将用于后续防爬行为的匹配和筛选，通过NginX中间件,实时进行流量采集。
界面展示：对于每个Web应用服务器，我们都有一个采集数据到Kafka消息队列的采集脚本。采集配置管理模块主要是完成：对每个脚本所采集的数据条数进行查看【可以每天看到采集的数据流量有没有异常】：

###### 数据采集管理示意图

###### 基础的采集字段如表 1 所示（随时更新）：

#### 表 1 基础字段

#### Requst 请求的连接

#### Request Method 请求的方法

#### Remote Address 客户端地址

#### Requestparameter 请求参数（包括Form 表单）

#### Content-Type "Content-Type" 请求头字段

#### Cookie 请求cookie

#### Server Address 服务器地址

#### Referer 跳转来源

#### User-Agent 用户终端浏览器信息

#### Time-Iso 8601 访问时间ISO格式

#### Time_local 访问时间

## 5. 2. 2. 数据分类和处理的配置和管理

数据采集完成后，进入Kafka缓存队列，然后使用Spark Streaming进行数据处理，
数据流向Spark算法引擎，结合Redis算法规则库分析得到模型结果，从而满足实时监控识
别的业务需求，用于系统实时分析、准实时分析、离线分析等。
界面展示：
采集的主要数据是Json、Xml和From数据，当这种数据的格式发送变化时，为了保障
数据清洗脚本逻辑有效，我们可以对指定要处理的数据字段进行配置化管理。


###### 数据处理界面示意图

#### 数据处理包括数据清洗和统计，清洗字段如下：

#### 数据清洗在这里主要是针对数据采集来的原数据进行处理，包括数据的规整、

#### 数据转换等等，数据清洗新增的字段如表 2 所示。

#### 表 2 数据清洗新增字段

#### 有无关键Cookie 需结合实际数据看看

#### 单次访问携带的Cookie个数

#### Referer是否伪造 需查看该IP的历史记录

#### 该IP是否属于高频的IP段 对统计字段进行查询

#### 由于反爬虫需要用到用户信息，而我们不能直接访问对方的数据库，所以需

#### 要通过流量数据的清洗来获得乘客的信息以及订票字段，并且入库。暂时需要的

#### 字段如表 3 所示。

##### 表 3 乘客信息以及订票字段

#### Booktime 订票时间

#### bookuser 购票人ID，即登录ID，例如明珠会员

#### 卡号、非会员手机号等

#### bookip 购票人IP

#### psgname 乘机人名（用户敏感信息）

#### psgtype 乘机人类型，例如成人、儿童、婴儿

#### idtype 证件类型，例如身份证、护照、其他

#### 等

#### idcard 乘机人证件号（用户敏感信息）

#### contractname 联系人名（用户敏感信息）

#### contractphone 联系人手机号（用户敏感信息）

#### bookagent 销售单位

#### depcity/depairport 始发地

#### arrcity/arrairport 目的地

#### flightdate/deptime 起飞时间

#### cabin 舱位级别


## 5. 3. 数据可视化管理产品模块

###### 将图形数据和属性数据有机结合起来，将客户关注的关键信息以直观的图表进行展示，

###### 改善人机交互体验，实现数据图示化、分析形象化、管理信息化以及从数据中发现规律和价

###### 值。

###### 将数据进行分类展示，用户可以通过自定义选择配置分类和展示方式，根据不同的配

###### 置，系统切换展示不同的图形图表。

###### 数据可视化示意图

###### 内置报表包括购票转化率、查询爬取规律、占座规律、爬虫对查定的影响、爬虫对系统

###### 稳定性影响 5 种：

###### 购票转化率即购票流量与总流量之比

###### 查询爬取规律包括爬虫活跃时间段、爬取频次等；

###### 占座规律可分析出爬虫非法占座情况，包括路线（航线）、日期、车次（航班）、座位、

###### 价格等

###### 查定比影响建立在收集全流量数据基础上，以查询流量除以下单流量得来

###### 爬虫对系统稳定性影响通过各链路流转等情况进行间接体现

# 6. 系统架构

## 6. 1. 设计策略

###### 系统的设计方法是采用面向对象的设计。本系统前端页面交互主要采用B/S的结构，采

用MVC及流式数据的设计思路进行规划设计。数据采集使用nginx采集脚本，并使用
sparkstreaming进行数据流清洗。使用nginx[lua]+kafka+sparkstreaming+spark架构。


设计过程使用到的工具是PowerDesigner 15 和Visio 2013 。
设计过程中对系统的可靠性、可扩展性以及性能进行了充分考虑和研究分析，争取通过
良好的设计，在实现系统功能的前提下，最大化的提高系统性能和扩展性，减少将来的维护
代价和其他成本。

## 6. 2. 技术选型

##### //此处描述系统技术选型，使用列表方式，列出所采用的主要开源组件商业组件等技

##### 术选型内容，应包括技术组件名、版本号、描述，描述部分应提供选择该组件的理由、优劣

##### 势（必须）。示例如下：

###### 技术选型 组件名称 版本号 描述

```
表现层框架 SpringMVC
（Pivotal）
```
4. 0. (^7) 表现层MVC框架，易于上手，高效稳定，是使用
Servlet 和JSP技术的一种MVC实现，可帮助开发
者控制WEB项目中的变化并提高专业化水平，减
少开发者在运用MVC设计模式开发WEB应用的
时间。
逻辑控制框架 Spring
（Pivotal）

###### 4. 0. 7 提供事务管理、逻辑控制等功能，框架的主要优

###### 势之一就是其分层架构，分层架构允许使用者选

###### 择使用哪一个组件，同时为J 2 EE应用程序开发提

###### 供集成的框架。

```
持久层框架 Hibernate
（RedHat）
```
###### 4. 2. 12 持久层框架，对JDBC进行了非常轻量级的对象封

###### 装，使得JAVA程序员可以随心所欲的使用对象编

###### 程思维来操纵数据库。

```
分布式系统架构 Hadoop 2. 7. 0 Hadoop是以一种可靠、高效、可伸缩的方式对大
量数据进行分布式处理的软件框架。它适用于任
何规模的非结构化数据，并具有极强的容错能力。
分布式计算平台 Spark^2.^1.^0 Spark是一种粗粒度数据并行（dataparallel）的计
算范式，同时Spark与Hadoop可以无缝结合，并
且在保证容错的前提下，用内存来承载工作集，
极大地提升了运算速度。
流式处理平台 Kafka 1. 0. 0 Kafka是一个完整的系统，它提供了一个高吞吐
```

###### 量、高可靠性、持久性高、以及多样化的消费处

###### 理模型，可以用来解决百万级别的数据中生产者

###### 和消费者之间数据传输的问题。

###### 实时计算开发语

###### 言

```
Scala
（EPFL）
```
```
2. 11 1 ）Scala较Java语法更丰富，函数式编程方式使
代码量比Java减少至少一个数量级。因此，使用
Scala能提高项目开发效率。
2 ）Spark源代码由Scala开发，Scala具有原生语
言的先天优势。各种特性Scala最先支持，尤其在
Spark开发排查异常时，对Spark源码熟悉，将有
更大优势，从而降低项目开发的风险。
3 ）Scala和Java都基于JVM运行，二者最终都编
译成class文件，使用scala能随意调用Java实现
的类和方法。因此，Scala和Java的混合编程方式
不会有兼容性问题，又能充分利用两者的优点。
目前行业内此种做法也较流行。
```
数据采集脚本 Nginx

```
（OpenResty）
```
```
1. 8. 0 Nginx是一个高性能的HTTP和反向代理服务器，
也是一个IMAP/POP 3 /SMTP服务器。其特点是占
有内存少，并发能力强，而且nginx的并发能力
确实在同类型的网页服务器中表现较好。
```
数据缓存队列脚

本

```
Lua
（OpenResty）
```
```
1. 9. 7. 3 OpenResty是一个基于 Nginx与 Lua 的高性能
Web 平台，其内部集成了大量精良的 Lua库、
第三方模块以及大多数的依赖项。用于方便地搭
建能够处理超高并发、扩展性极高的动态 Web
应用、Web 服务和动态网关。
```

## 6. 3. 主要用例说明

## 6. 3. 1. 数据管理

## 6. 3. 2. 实时监控


## 6. 3. 3. 数据可视化

## 6. 3. 4. 策略管理


## 6. 3. 5. 流程管理

## 6. 3. 6. 规则管理

## 6. 4. 非功能描述

##### //此处描述系统的非功能需求，一般包括系统用户数量、并发量、 TPS 要求、页面响应

##### 速度要求、数据量及 IO 等要求（前提）。示例如下：

###### 对于非功能的性能指标表现在下面一些方面：


###### 功能模块 性能要求 描述

```
数据管理 Kafka缓存队列磁盘要求能处理
一天的积压约为 50 GB的数据
量。
```
```
根据前端采集的数据，一条数据大小约 1 - 2 KB。
按照每日 2700 万的数据量计算，
2 KB* 27000000 / 1024 / 1024 ≈ 50 GB。所以
Kafka队列要缓存一天的数据，需要约 50 GB的
磁盘空间。
SparkStreaming每天要处理的
数据量约为 50 GB
```
```
SparkStreaming将消费Kafka队列中的数据，
故每天要处理约 50 GB的数据量
页面响应
页面响应时间< 5 S。
```
```
对于普通页面内容刷新，页面数据的请求道展现
的过程总体时间不能超过 5 秒。
```
```
系统支撑的并发数： 50 。
```
```
此处的系统用户并发数指防爬防占座系统的
Web应用的用户并发数。该系统采集到的南航
官网数据以流量形式进行转发，无关用户。同时，
Web服务器需要复用做他用，因此系统用户并
发数限制为 50 个，避免占用过多资源。
数据库响应
Sql查询相应时间< 5 S
```
```
对于系统的页面展现的sql查询语句，查询时间
不能大于 5 秒，以免影响前端页面的展示。
```
```
连接池连接数量有效百分
比> 80 %
```
```
对于已经配置好的连接池，已经被占用的连接数
量不能超过总连接数据的 80 %，要保证 20 %的
有效连接未被占用。
外部接口响应 外部接口数据返回时间不超过
5 S.
```
```
对于外部接口的调用，其数据返回的整个时间不
能超过 5 S.
```
## 6. 5. 模块分解策略

## 6. 5. 1. 数据采集

```
官网反爬虫系统的数据采集模块采用lua+nginx+kafka架构，通过lua脚本将官网的http
```
请求数据获取之后，进行数据条数统计和日志记录，之后写入到kafka生成端，给


sparkstreaming进行消费并清洗。

## 6. 5. 2. 数据处理及实时计算

```
本模块主要基于SparkStreaming进行实时流的计算和分析，其中规则配置信息以及统
```
计分析结果主要与Redis进行交互，因为Redis主要是基于内存操作的存储系统，同时可以

将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。从而避免了直接访问

Mysql数据库时不断建立和销毁数据连接而产生的系统损耗。


## 6. 5. 3. 数据可视化

```
对于非实时数据的展现，通过定时器从hdfs读取数据，然后保存到mysql中，前端读
```
取mysql数据进行展现。

```
对于实时数据的展现，从Redis中读取数据进行展现。
```
## 6. 6. 数据库 ER 图



## 6. 7. 浏览器兼容性

###### 对IE 8 以上版本、火狐、谷歌浏览器兼容。

# 7. 防爬规则

## 7. 1. 数据源

###### 1 .行程相关


###### 出发地、目的地、出发时间、预定时间

###### 2 .出行人相关

###### 身份信息（乘机人名、乘机人类型、乘机人证件类型、乘机人证件号）、出行人数

###### 3 .预定相关

###### 购票人信息（登录ID、登录类型、操作IP、浏览器UA、手机设备信息）

###### 联系人信息（联系人名、联系人手机号、联系人邮箱）

###### 其他信息（销售单位）

###### 4 .机票防爬关键字段

```
bookuser购票人ID，即登录ID，例如明珠会员卡号、非会员手机号等
bookip购票人IP
psgname 乘机人名（用户敏感信息）
psgtype 乘机人类型，例如成人、儿童、婴儿
idtype证件类型，例如身份证、护照、其他等
idcard 乘机人证件号（用户敏感信息）
contractname 联系人名（用户敏感信息）
contractphone 联系人手机号（用户敏感信息）
bookagent销售单位
depcity/depairport 始发地
arrcity/arrairport 目的地
flightdate/deptime 起飞时间
cabin 舱位级别
```
## 7. 2. 防爬规则

###### 1 .按单次请求 -UA中出现非浏览器字样

###### 2 .按IP地址聚合 - 任意X分钟内查询超过Y次

###### 3 .按IP地址聚合 - 连续X次查询时间间隔均小于Y秒

###### 4 .按IP地址聚合 - 任意X分钟内查询时间间隔的方差小于Y

###### 5 .按IP地址聚合 - 任意X分钟内，每分钟的查询频次的方差小于Y

###### 6 .按IP地址聚合 - 任意X分钟内，查询不同出发地超过Y处

## 7. 3. 作弊者数量多的规律

###### 1 .热门航线

###### 2 .春节、暑假

###### 3 .会员日

## 7. 4. 作弊者特点：

###### 1 .长期、重复爬取数据

###### 2 .使用多个代理（隔几分钟自动切换UA、IP） 爬取数据

###### 3 .每个IP短时间内爆发较高流量


###### 4 .每个IP白天和夜晚的请求次数均衡

###### 5 .热门航线的更新频率更快

###### 6 .浏览路径不完整（一次命中会话为其典型代表）

# 8. 数据采集模块

## 8. 1. 1. 数据采集

```
官网反爬虫系统的数据采集模块采用lua+nginx+kafka架构，通过lua脚本将官网的http
```
请求数据获取之后，进行数据条数统计和日志记录，之后写入到kafka生成端，给

sparkstreaming进行消费并清洗。

# OpenResty

# 9 .OpenResty 概述

## 9. 1 .OpenResty 介绍

OpenResty® 是一个基于 Nginx与 Lua的高性能Web平台，其内部集成了大量精良的 Lua
库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动
态 Web 应用、Web 服务和动态网关。


OpenResty® 通过汇聚各种设计精良的 Nginx模块（主要由 OpenResty团队自主开发），从
而将Nginx有效地变成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程
师可以使用 Lua脚本语言调动Nginx支持的各种 C以及 Lua模块，快速构造出足以胜任
10 K乃至 1000 K 以上单机并发连接的高性能 Web 应用系统。

OpenResty® 的目标是让你的Web服务直接跑在 Nginx服务内部，充分利用 Nginx的非阻
塞 I/O 模型，不仅仅对 HTTP 客户端请求,甚至于对远程后端诸如 MySQL、PostgreSQL、
Memcached 以及 Redis等都进行一致的高性能响应。

## 9. 2 .OpenResty 常用架构

###### 以下内容为自张开涛 《亿级流量网站架构核心技术》

[http://jinnianshilongnian.iteye.com/blog/](http://jinnianshilongnian.iteye.com/blog/) 2280928

## 9. 2. 1. 负载均衡

LVS+HAProxy将流量转发给核心Nginx 1 和核心Nginx 2 ，即实现了流量的负载均衡

## 9. 2. 2. 单机闭环

###### 所有想要的数据都能从本服务器直接获取，在大多数时候无需通过网络去其他服务器获取


## 9. 2. 3. 分布式闭环

###### 单机闭环会遇到如下两个主要问题： 1 、数据不一致问题（比如没有采用主从架构导致不

###### 同服务器数据不一致）； 2 、遇到存储瓶颈（磁盘或者内存遇到了天花板）。

###### 解决数据不一致的比较好的办法是采用主从或者分布式集中存储；而遇到存储瓶颈就需要进

###### 行按照业务键进行分片，将数据分散到多台服务器

## 9. 2. 4. 接入网关

###### 接入网关也可以叫做接入层，即接收到流量的入口，在入口我们可以进行如下事情：


# 10. OpenResty 开发环境搭建

## 10. 1. 下载 OpenResty

我们可以在官方（https://openresty.org/cn/）下载
Windows下载

Linux下载


## 10. 2. Windows 安装

直接解压安装。启动时，只需要nginx.exe

## 10. 3. Linux 安装 - centOS 6. 5

## 10. 3. 1. 第一步：解压

**tar-xzvfopenresty-VERSION.tar.gz**


## 10. 3. 2. 第二步：配置

进入 openresty-VERSION/目录,然后输入以下命令配置:
**./configure--prefix=/usr/local/openresty--with-http_stub_status_module**
默认,--prefix=/usr/local/openresty程序会被安装到/usr/local/openresty目录。

## 10. 3. 3. 第三步：安装

make&&makeinstall

注意
OpenResty依赖库有： perl 5. 6. 1 +,libreadline,libpcre,libssl。所以我们需要先安装好这些依
赖库
**yuminstallreadline-develpcre-developenssl-develperlgcc**

# 11. OpenResty 快速入门

## 11. 1. Lua 语法介绍

Lua是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放，其设计目的是
为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。
官网:http://www.lua.org/

## 11. 1. 1. CentOS 下安装 Lua

**tarzxflua- 5. 3. 4 .tar.gz
cdlua- 5. 3. 4
makelinuxtest
makeinstall**


## 11. 1. 2. helloWorld

### 1. 交互模式

### 2. 脚本式

创建一个以lua结尾的文件，例如hello.lua,文件内容
print(“helloworld”)

## 11. 1. 3. Lua 数据类型与变量

Lua中有 8 个基本类型分别为：nil、boolean、number、string、userdata、function、thread
和table。

Lua变量有三种类型：全局变量、局部变量、表中的域。


Lua中的变量全是全局变量，那怕是语句块或是函数里，除非用 local 显式声明为局部变量。
局部变量的作用域为从声明位置开始到所在语句块结束。
变量的默认值均为 nil。

- -单行注释
- -[[
    多行注释
- --]]
functionfun()
    a= 10 - -全局
    localb= 20 - -局部
    print(b)
end
fun()
print(a,b)
print(type( 10 ))
print(type(true))
print(type(fun))
print(type("helloworld"))

## 11. 1. 4. Lua 运算符

### 1. 赋值运算符

###### - - 赋值

**str="helllo".."world"
print(str)**

**a,b= 10 , 20
print(a,b)**

**c,d,e= 1 , 2
print(c,d,e)**

**x,y= 4 , 5 , 6
print(x,y)**


### 2. 算术运算符

###### - - 算术运算

a,b= 10 , 20

- - 加法
c=a+b
print(a.."+"..b.."="..c)
- -减法
c=a-b
print(a.."-"..b.."="..c)
- - 乘法
c=a*b
print(a.."*"..b.."="..c)
- - 除法
c=a/b
print(a.."/"..b.."="..c)
- - 求余
c=a%b
print(a.."%"..b.."="..c)
- - 乘方
c=a^ 2
print(a.."^ 2 ".."="..c)


### 3. 关系运算符

a,b= 10 , 20
print(a>b)
print(a<b)
print(a==b)
print(a~=b)

### 4. 逻辑运算符

###### - - 逻辑运算符

print(trueandfalse)
print(trueorfalse)
print(nottrue)


### 5. 其它运算符

###### - - 其它运算符

str="helloworld"
print(#str)

## 11. 1. 5. Lua 流程控制

### 1. 条件

###### - - 条件

**- -if
if(true)
then**
    **print("ok")
end
- -ifelse
a,b= 10 , 20
if(a>b)
then**
    **print(a)
else**
    **print(b)
end**
- - if 嵌套
**c= 20**


**if(c> 10 )
then
if(c< 30 )
then**
print(c.."的值在 **10** 到 **30** 之间 **")
end
end**

### 2. 循环

- -while循环
a= 10
while(a> 0 )
do
    print(a)
    a=a- 1
end
- -repeatuntil
b= 10
repeat
    print(b)
    b=b- 1
until(b< 1 )

for循环分为数值for循环与泛型for循环
1. 数值for循环


2. 泛型for循环

- -for
fora= 1 , 10 do
    print(a)
end

## 11. 1. 6. Lua 数组

Lua数组大小不固定，下标是从 1 开始。

- - 数组
**arr={"aaa","bbb","ccc"}**
- - 使用数值 **for** 通过下标来遍历数组
**fori= 1 ,#arr
do**
    **print(arr[i])
end**
- -使用泛型 **for** 遍历数组
**fori,vinipairs(arr)doprint(i,v)end**


## 11. 1. 7. Lua 数据类型转换

## 11. 1. 8. Lua 函数

### 1. 函数定义

ua编程语言函数定义格式如下：
**optional_function_scope function function_name( argument 1 , argument 2 , argument 3 ...,
argumentn)
function_body
returnresult_params_comma_separated
end**

 **optional_function_scope:** 该参数是可选的制定函数是全局函数还是局部函数，未设置该

```
参数默认为全局函数，如果你需要设置函数为局部函数需要使用关键字 local 。
```

 **function_name:** 指定函数名称。

 **argument 1 ,argument 2 ,argument 3 ...,argumentn:** 函数参数，多个参数以逗号隔开，

```
函数也可以不带参数。
```
 **function_body:** 函数体，函数中需要执行的代码语句块。

 **result_params_comma_separated:** 函数返回值，Lua语言函数可以返回多个值，每个值

```
以逗号隔开。
```
### 2. 示例

###### - - 函数

**functionsum(a,b)
returna+b
end
c=sum( 1 , 2 )
print(c)**

**- -----------------------------
functionfun(a,b,c,d)**
    **returna+ 1 ,b+ 1 ,c+ 1
end
r 1 ,r 2 =fun( 1 , 2 , 3 )
print(r 1 ,r 2 )**

## 11. 1. 9. Luatable

table 是 Lua的一种数据结构用来帮助我们创建不同的数据类型，如：数字、字典等。
Luatable使用关联型数组，你可以用任意类型的值来作数组的索引，但这个值不能是 nil。
Luatable是不固定大小的，你可以根据自己需要进行扩容。
Lua也是通过table来解决模块（module）、包（package）和对象（Object）的。例如string.format
表示使用"format"来索引tablestring。

**- -table
mytable={}
mytable.first="tom"
mytable.second="james"**

**print(mytable[ 1 ])
print(mytable.first)
print(mytable["second"])**


## 11. 1. 10. Lua 模块与包

模块类似于一个封装库，从Lua 5. 1 开始，Lua加入了标准的模块管理机制，可以把一些公
用的代码放在一个文件里，以 API接口的形式在其他地方调用，有利于代码的重用和降低
代码耦合度。
Lua的模块是由变量、函数等已知元素组成的 table，因此创建一个模块很简单，就是创建
一个 table，然后把需要导出的常量、函数放入其中，最后返回这个 table就行
module.lua文件
**module={}
module.index= 1
functionmodule.sum(a,b)
returna+b
end**
test 12 .lua

**require"module"
print(module.index)
print(module.sum( 10 , 20 ))**

## 11. 2. OpenResty 入门案例

## 11. 2. 1. helloWorld

openresty 1. 9. 3. 1 及以下版本，请使用content_by_lua命令；在openresty 1. 9. 3. 2 以
上，content_by_lua 改成了content_by_lua_block。可使用 nginx-V命令查看版本号

在nginx的配置文件nginx.conf中添加下列信息，然后执行nginx-sreload

**location/{
#root html;
#index index.htmlindex.htm;
default_typetext/html;
content_by_lua_block{
ngx.say("helloworld");
}
}**
我们也可以使用content_by_lua_file来引入一个lua文件

**location/{
#root html;
#index index.htmlindex.htm;
default_typetext/html;**


```
content_by_lua_file/usr/local/openresty/nginx/conf/my.lua;
}
```
## 11. 2. 2. 获取 http 请求信息

### 1. 获取 uri 参数

获取一个 uri有两个方法：ngx.req.get_uri_args、ngx.req.get_post_args，二者主要的区别是
参数来源有区别

- - 获取 **get** 请求参数
**localarg=ngx.req.get_uri_args()
fork,vinpairs(arg)do**
    **ngx.say("[GET]key:",k,"v:",v)**
    **ngx.say("<br>")
end**
- - 获取 **post** 请求时 请求参数
ngx.req.read_body() -- 解析 **body** 参数之前一定要先读取 **body
localarg=ngx.req.get_post_args()
fork,vinpairs(arg)do**
    **ngx.say("[POST]key:",k,"v:",v)**
       **ngx.say("<br>")
end**

### 2. 获取 header

- - 获取 **header
localheaders=ngx.req.get_headers()
fork,vinpairs(headers)do**
    **ngx.say("[header]name:",k,"v:",v)**
       **ngx.say("<br>")
end**

### 3. 获取 body

- - 获取 **body** 信息
**localdata=ngx.req.get_body_data()
ngx.say(data)**


ngx.req.get_body_data()读请求体，会偶尔出现读取不到直接返回 nil的情况。如果请

#### 求体尚未被读取，请先调用 ngx.req.read_body (或打开 lua_need_request_body

###### 选项强制本模块读取请求体，此方法不推荐）。

## 11. 2. 3. 操作 redis

**localredis=require"resty.redis"
localred=redis:new()**

**red:set_timeout( 1000 )-- 1 sec**

**localok,err=red:connect(" 192. 168. 93. 134 ", 6379 )
ifnotokthen
ngx.say("failedtoconnect:",err)
return
end**

- - 请注意这里 **auth** 的调用过程

**localcount
count,err=red:get_reused_times()
if 0 ==countthen
ok,err=red:auth("admin")
ifnotokthen
ngx.say("failedtoauth:",err)
return
end
elseiferrthen
ngx.say("failedtogetreusedtimes:",err)
return
end**

**ok,err=red:set("itcast","goodschool")
ifnotokthen
ngx.say("failedtosetitcast:",err)
return
end**

**ngx.say("setresult:",ok)**

- - 连接池大小是 **100** 个，并且设置最大的空闲时间是 **10** 秒


**localok,err=red:set_keepalive( 10000 , 100 )
ifnotokthen
ngx.say("failedtosetkeepalive:",err)
return
end**

# 12. 课程计划

1 、KafkaAPI回顾
2 、lua集成kafka
3 、爬虫测试
4 、介绍hadoop、spark测试环境
5 、SparkStreaming消费kafka数据的两种方式
6 、搭建streaming项目
7 、实现kafka的数据读取
8 、实现链路统计模块

# 13. KafkaAPI 回顾

## 13. 1. Pom

<?xmlversion=" 1. 0 "encoding="UTF- 8 "?>
<projectxmlns="http://maven.apache.org/POM/ 4. 0. 0 "
xmlns:xsi="http://www.w 3 .org/ 2001 /XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/ 4. 0. 0
[http://maven.apache.org/xsd/maven-](http://maven.apache.org/xsd/maven-) 4. 0. 0 .xsd">
<modelVersion> 4. 0. 0 </modelVersion>

```
<groupId>cn.itcast</groupId>
<artifactId>ispider</artifactId>
<version> 1. 0 - SNAPSHOT</version>
```
```
<properties>
<project.build.sourceEncoding>UTF- 8 </project.build.sourceEncoding>
<jdk.version> 1. 7 </jdk.version>
<spark.version> 1. 6. 2 </spark.version>
<scala.version> 2. 10 </scala.version>
```

<hadoop.version> 2. 6. 2 </hadoop.version>
<mysql.driver.version> 5. 1. 35 </mysql.driver.version>
<jedis.version> 2. 9. 0 </jedis.version>
<fastjson.version> 1. 2. 4 </fastjson.version>
<junit.version> 4. 12 </junit.version>
<c 3 p 0 .version> 0. 9. 1. 2 </c 3 p 0 .version>
</properties>

<dependencies>
<dependency>
<groupId>org.scala-lang</groupId>
<artifactId>scala-library</artifactId>
<version> 2. 10. 2 </version>
</dependency>
<!--注意：依赖不在Hadoop环境中，集群运行需要上传依赖包并指定-->
<dependency>
<groupId>com.jayway.jsonpath</groupId>
<artifactId>json-path</artifactId>
<version> 2. 3. 0 </version>
</dependency>

```
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-core_${scala.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-sql_${scala.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-hive_${scala.version}</artifactId>
<version>${spark.version}</version>
<!--<exclusions>
<exclusion>
<groupId>joda-time</groupId>
<artifactId>joda-time</artifactId>
</exclusion>
</exclusions>-->
</dependency>
```

<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming_${scala.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-client</artifactId>
<version>${hadoop.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming-kafka_${scala.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-mllib_${scala.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<!--mysqldriver-->
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>${mysql.driver.version}</version>
</dependency>
<!--c 3 p 0 数据库连接池-->
<dependency>
<groupId>c 3 p 0 </groupId>
<artifactId>c 3 p 0 </artifactId>
<version>${c 3 p 0 .version}</version>
</dependency>
<!--redis缓存-->
<dependency>
<groupId>redis.clients</groupId>
<artifactId>jedis</artifactId>
<version>${jedis.version}</version>
</dependency>
<dependency>
<groupId>com.alibaba</groupId>
<artifactId>fastjson</artifactId>
<version>${fastjson.version}</version>
</dependency>
<dependency>


```
<groupId>junit</groupId>
<artifactId>junit</artifactId>
<version>${junit.version}</version>
</dependency>
</dependencies>
```
<!--maven官方 [http://repo](http://repo) 1 .maven.org/maven 2 / 或 [http://repo](http://repo) 2 .maven.org/maven 2 /
（延迟低一些） -->
<repositories>

```
<repository>
<id>central</id>
<name>MavenRepositorySwitchboard</name>
<layout>default</layout>
<url>http://repo 2 .maven.org/maven 2 </url>
<snapshots>
<enabled>false</enabled>
</snapshots>
</repository>
</repositories>
```
```
<build>
<sourceDirectory>src/main/scala</sourceDirectory>
<testSourceDirectory>src/test/scala</testSourceDirectory>
<plugins>
<plugin>
<!--MAVEN编译使用的JDK版本-->
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-compiler-plugin</artifactId>
<version> 3. 3 </version>
<configuration>
<source>${jdk.version}</source>
<target>${jdk.version}</target>
<encoding>${project.build.sourceEncoding}</encoding>
</configuration>
</plugin>
<plugin>
<groupId>net.alchim 31 .maven</groupId>
<artifactId>scala-maven-plugin</artifactId>
<version> 3. 2. 0 </version>
<executions>
<execution>
<goals>
<goal>compile</goal>
```

```
<goal>testCompile</goal>
</goals>
<configuration>
<args>
<arg>-dependencyfile</arg>
```
<arg>${project.build.directory}/.scala_dependencies</arg>
</args>
</configuration>
</execution>
</executions>
</plugin>
</plugins>
</build>

</project>

## 13. 2. 生产者

packageorg.apache_ 01 _kafka.producer;

importjava.util.Properties;

importorg.apache.kafka.clients.producer.KafkaProducer;
importorg.apache.kafka.clients.producer.ProducerRecord;

/**
* 生产者
*/
publicclassTestProducer{
publicstaticvoidmain(String[]args)throwsException{
Propertiesprops=newProperties();
props.put("bootstrap.servers","node 4 : 9092 ,node 2 : 9092 ,node 3 : 9092 ");
props.put("key.serializer",
"org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer",
"org.apache.kafka.common.serialization.StringSerializer");
KafkaProducer<String,String>producer=newKafkaProducer<String,String>(
props);
for(inti= 0 ;i< 100000 ;i++){
ProducerRecord<String,String>km=newProducerRecord<String,String>(
"itheima 01 ","thisisamsg====="+i);


producer.send(km);
producer.flush();
if(i% 100 == 0 &&i!= 0 ){
Thread.sleep( 10000 );
}
}
}
}

## 13. 3. 消费者

packageorg.apache_ 01 _kafka.consumer;

importjava.util.Arrays;
importjava.util.Properties;
importorg.apache.kafka.clients.consumer.ConsumerRecord;
importorg.apache.kafka.clients.consumer.ConsumerRecords;
importorg.apache.kafka.clients.consumer.KafkaConsumer;

publicclassTestConsumer{
publicstaticvoidmain(String[]args)throwsException{
Propertiesprops=newProperties();
//指定kafka集群地址
props.put("bootstrap.servers","node 2 : 9092 ,node 3 : 9092 ,node 4 : 9092 ");
//指定消费者组id
props.put("group.id","test 1 ");
//key的序列化类
props.put("key.deserializer",
"org.apache.kafka.common.serialization.StringDeserializer");
//value的序列化类
props.put("value.deserializer",
"org.apache.kafka.common.serialization.StringDeserializer");
//通过配置，创建消费者
KafkaConsumer<String,String>kafkaConsumer=newKafkaConsumer<String,String>(
props);
//订阅消费
kafkaConsumer.subscribe(Arrays.asList("itheima 01 "));
while(true){
//消费数据
ConsumerRecords<String,String>records=kafkaConsumer.poll( 1000 );
for(ConsumerRecord<String,String>record:records){
if(record.partition()== 0 ){


Stringtopic=record.topic();
intpartition=record.partition();
longoffset=record.offset();
Stringkey=record.key();
Stringvalue=record.value();
System.out.println("topic："+topic);
System.out.println("partition："+partition);
System.out.println("offset："+offset);
System.out.println("key："+key);
System.out.println("value："+value);
System.out.println("------------------------------");
}
}
}
}
}

# 14. Lua 集成 kafka

## 14. 1. 场景描述

对于线上大流量服务或者需要上报日志的nginx服务，每天会产生大量的日志，这些日
志非常有价值。可用于计数上报、用户行为分析、接口质量、性能监控等需求。但传统nginx
记录日志的方式数据会散落在各自nginx上，而且大流量日志本身对磁盘也是一种冲击。
我们需要把这部分nginx日志统一收集汇总起来,收集过程和结果需要满足如下需求:
1 、支持不同业务获取数据,如监控业务，数据分析统计业务等。
2 、数据实时性
3 、高性能保证
openresty:http://openresty.org
kafka:http://kafka.apache.org
lua-resty-kafka:https://github.com/doujiang 24 /lua-resty-kafka

## 14. 2. 技术方案

得益于openresty和kafka的高性能，我们可以非常轻量高效的实现当前需求，架构如下:


## 14. 3. 安装配置

1 、安装openresty，记得安装nginx的监控模块
2 、安装kafka
3 、下载lua+kafka插件：https://github.com/doujiang 24 /lua-resty-kafka/archive/master.zip
4 、解压插件，将lua-resty-kafka-master\lib\resty\kafka文件夹放到openresty/lualib/resty/下

## 14. 4. 代码书写

 Nginx文件:nginx.conf
worker_processes 1 ;
events{
worker_connections 1024 ;
}
http{
include mime.types;


```
default_type application/octet-stream;
sendfile on;
keepalive_timeout 65 ;
#开启共享字典，设置内存大小为 10 M，供每个nginx的线程消费
lua_shared_dictshared_data 10 m;
#配置本地域名解析
resolver 127. 0. 0. 1 ;
server{
listen 80 ;
server_name localhost;
#charsetkoi 8 - r;
#access_log logs/host.access.log main;
location/{
#root html;
#index index.htmlindex.htm;
#开启nginx监控
stub_statuson;
#加载lua文件
default_typetext/html;
content_by_lua_file/usr/local/openresty/nginx/conf/controller.lua;
}
error_page 500502503504 / 50 x.html;
location=/ 50 x.html{
root html;
}
}
```
}
 Lua文件:controller.lua

- -数据采集阈值限制，如果lua采集超过阈值，则不采集
localDEFAULT_THRESHOLD= 100000
- -kafka分区数
localPARTITION_NUM= 6
- -kafka主题名称
localTOPIC='B 2 CDATA_COLLECTION 3 '
- - 轮询器共享变量KEY值
localPOLLING_KEY="POLLING_KEY"
- -kafka集群(定义kafkabroker地址，ip需要和kafka的host.name配置一致)
localfunctionpartitioner(key,num,correlation_id)
    returntonumber(key)
end
- -kafkabroker列表
local BROKER_LIST =
{{host=" 192. 168. 56. 112 ",port= 9092 },{host=" 192. 168. 56. 113 ",port= 9092 },{host=" 192. 168. 56. 154


",port= 9092 }}

- -kafka参数，
localCONNECT_PARAMS={producer_type="async",socket_timeout= 30000 ,flush_time=
10000 ,request_timeout= 20000 ,partitioner=partitioner}
- - 共享内存计数器，用于kafka轮询使用
localshared_data=ngx.shared.shared_data
localpollingVal=shared_data:get(POLLING_KEY)
ifnotpollingValthen
    pollingVal= 1
    shared_data:set(POLLING_KEY,pollingVal)
end
- -获取每一条消息的计数器，对PARTITION_NUM取余数，均衡分区
localpartitions=''..(tonumber(pollingVal)%PARTITION_NUM)
shared_data:incr(POLLING_KEY, 1 )
- - 并发控制
localisGone=true
- -获取ngx.var.connections_active进行过载保护，即如果当前活跃连接数超过阈值进行限流保
护
iftonumber(ngx.var.connections_active)>tonumber(DEFAULT_THRESHOLD)then
    isGone=false
end
- - 数据采集
ifisGonethen

```
localtime_local=ngx.var.time_local
iftime_local==nilthen
time_local=""
end
```
```
localrequest=ngx.var.request
ifrequest==nilthen
request=""
end
```
```
localrequest_method=ngx.var.request_method
ifrequest_method==nilthen
request_method=""
end
```
```
localcontent_type=ngx.var.content_type
ifcontent_type==nilthen
content_type=""
end
```

```
ngx.req.read_body()
localrequest_body=ngx.var.request_body
ifrequest_body==nilthen
request_body=""
end
```
```
localhttp_referer=ngx.var.http_referer
ifhttp_referer==nilthen
http_referer=""
end
```
```
localremote_addr=ngx.var.remote_addr
ifremote_addr==nilthen
remote_addr=""
end
```
```
localhttp_user_agent=ngx.var.http_user_agent
ifhttp_user_agent==nilthen
http_user_agent=""
end
```
```
localtime_iso 8601 =ngx.var.time_iso 8601
iftime_iso 8601 ==nilthen
time_iso 8601 =""
end
```
```
localserver_addr=ngx.var.server_addr
ifserver_addr==nilthen
server_addr=""
end
```
```
localhttp_cookie=ngx.var.http_cookie
ifhttp_cookie==nilthen
http_cookie=""
end
```
- -封装数据
    local message = time_local .."#CS#".. request .."#CS#".. request_method .."#CS#"..
    content_type.."#CS#"..request_body.."#CS#"..http_referer.."#CS#"..remote_addr.."#CS#"..
    http_user_agent.."#CS#"..time_iso 8601 .."#CS#"..server_addr.."#CS#"..http_cookie;
- -引入kafka的producer
    localproducer=require"resty.kafka.producer"
    - -创建producer
    localbp=producer:new(BROKER_LIST,CONNECT_PARAMS)
    - -发送数据


```
localok,err=bp:send(TOPIC,partitions,message)
```
- -打印错误日志
ifnotokthen
    ngx.log(ngx.ERR,"kafkasenderr:",err)
    return
end
end

# 15. 测试

## 15. 1. 使用浏览器测试

1 、直接在浏览器中输入nginx的ip地址
2 、用测试程序消费kafka中数据
3 、查看有无数据

## 15. 2. 爬虫测试

## 15. 2. 1. 请求数据

#### Requst 请求的连接

#### Request Method 请求的方法

#### Remote Address 客户端地址

#### Requestparameter 请求参数（包括Form 表单）

#### Content-Type "Content-Type" 请求头字段

#### Cookie 请求cookie

#### Server Address 服务器地址

#### Referer 跳转来源

#### User-Agent 用户终端浏览器信息

#### Time-Iso 8601 访问时间ISO格式

#### Time_local 访问时间


## 15. 2. 2. 单程国内查询爬虫

packagecn.itcast.spider.csair;

importjava.text.DateFormat;
importjava.text.SimpleDateFormat;
importjava.util.ArrayList;
importjava.util.Calendar;
importjava.util.Date;
importjava.util.GregorianCalendar;
importjava.util.Locale;

importorg.apache.http.client.entity.UrlEncodedFormEntity;
importorg.apache.http.client.methods.CloseableHttpResponse;
importorg.apache.http.client.methods.HttpPost;
importorg.apache.http.entity.StringEntity;
importorg.apache.http.impl.client.CloseableHttpClient;
importorg.apache.http.impl.client.HttpClients;
importorg.apache.http.message.BasicNameValuePair;

/**
* 查询国内单程航班
*/
publicclassSpiderGoAirCN{

```
publicstaticvoidmain(String[]args)throwsException{
for(inti= 0 ;i< 50 ;i++){
//请求查询信息
spiderQueryao();
//请求html
spiderHtml();
//请求js
spiderJs();
//请求css
spiderCss();
//请求png
spiderPng();
//请求jpg
spiderJpg();
Thread.sleep( 1000 );
}
}
```

```
publicstaticvoidspiderQueryao()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /query/jaxb/direct/query.ao";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 ="
+getGoTime()+"&at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 243. 45. 78. 132 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D"
+getGoTime()
+
"% 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 83 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ;
JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ("
+getGoTime()+")");


###### // 4 .设置请求参数

ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderHtml()throwsException{
```
// 1 .指定目标网站
String url =
"http:// 192. 168. 56. 161 /B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =CAN
&c 2 =CTU&d 1 = 2018 - 01 - 17 &at= 1 &ct= 0 &it= 0 ";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",

"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");


httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
//httpPost.setEntity(newStringEntity(
//
"depcity=CAN&arrcity=WUH&flightdate= 20180220 &adultnum= 1 &childnum= 0 &infantnum= 0 &ca
binorder= 0 &airline= 1 &flytype= 0 &international= 0 &action= 0 &segtype= 1 &cache= 0 &preUrl=&isM
ember="));
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderJs()throwsException{
```
// 1 .指定目标网站
String url =
"http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/modules/common/requireConfig.js";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数


```
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();


```
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}
```
```
publicstaticvoidspiderCss()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/css/flight.css";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader("Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();


parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderPng()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/images/common.png";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;


identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderJpg()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/images/loadingimg.jpg";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
```

AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticStringgetLocalDateTime(){
DateFormatdf=newSimpleDateFormat("dd/MMM/yyyy'T'HH:mm:ss+ 08 : 00 ",
Locale.ENGLISH);
StringnowAsISO=df.format(newDate());
returnnowAsISO;
```

###### }

```
publicstaticStringgetISO 8601 Timestamp(){
DateFormatdf=newSimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss+ 08 : 00 ");
StringnowAsISO=df.format(newDate());
returnnowAsISO;
}
```
```
publicstaticStringgetGoTime(){
DateFormatdf=newSimpleDateFormat("yyyy-MM-dd");
StringnowAsISO=df.format(newDate());
returnnowAsISO;
}
```
publicstaticStringgetBackTime(){
Datedate=newDate();// 取时间
Calendarcalendar=newGregorianCalendar();
calendar.setTime(date);
calendar.add(calendar.DATE,+ 1 );// 把日期往前减少一天，若想把日期向后推一天则
将负数改为正数
date=calendar.getTime();
SimpleDateFormatformatter=newSimpleDateFormat("yyyy-MM-dd");
StringdateString=formatter.format(date);
returndateString;
}
}

## 15. 2. 3. 往返国内查询

packagecn.itcast.spider.csair;

importjava.io.IOException;
importjava.io.UnsupportedEncodingException;
importjava.text.DateFormat;
importjava.text.SimpleDateFormat;
importjava.util.ArrayList;
importjava.util.Calendar;
importjava.util.Date;
importjava.util.GregorianCalendar;
importjava.util.Locale;

importorg.apache.http.client.entity.UrlEncodedFormEntity;


importorg.apache.http.client.methods.CloseableHttpResponse;
importorg.apache.http.client.methods.HttpPost;
importorg.apache.http.entity.StringEntity;
importorg.apache.http.impl.client.CloseableHttpClient;
importorg.apache.http.impl.client.HttpClients;
importorg.apache.http.message.BasicNameValuePair;
importorg.apache.http.util.EntityUtils;

/**
* 查询国内往返航班（httprefer和单程不同）
*
*/
publicclassSpiderGoAndBackAirCN{

```
publicstaticvoidmain(String[]args)throwsException{
//请求查询信息
spiderQueryaoGo();
spiderQueryaoBack();
//请求html
spiderHtml();
//请求js
spiderJs();
//请求css
spiderCss();
//请求png
spiderPng();
//请求jpg
spiderJpg();
```
```
}
```
```
publicstaticvoidspiderQueryaoGo()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /query/jaxb/direct/query.ao";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
```

```
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=R&c 1 =C
AN&c 2 =CTU&d 1 ="
+getGoTime()
+"&d 2 ="
+getBackTime()
+"&at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D"
+getGoTime()
+
"% 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 83 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 2 % 3 B
segt% 3 D%E 6 % 9 D%A 5 %E 5 % 9 B% 9 E%E 7 %A 8 % 8 B% 3 Btime% 3 D"
+getGoTime()
+"% 3 Bbtime% 3 D"
+getBackTime()
+
"% 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 83 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516172267004 :ss= 1516170709449 :fs= 15
13243317440 :pn= 6 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(R)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnu
m( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ("
+getGoTime()
+")WT.al_orgdate 2 ("
+getBackTime()
+")");
// 4 .设置请求参数


ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderQueryaoBack()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /query/jaxb/direct/query.ao";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=R&c 1 =C
AN&c 2 =CTU&d 1 ="
+getGoTime()
+"&d 2 ="
+getBackTime()
+"&at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());


httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D"
+getGoTime()
+
"% 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 83 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 2 % 3 B
segt% 3 D%E 6 % 9 D%A 5 %E 5 % 9 B% 9 E%E 7 %A 8 % 8 B% 3 Btime% 3 D"
+getGoTime()
+"% 3 Bbtime% 3 D"
+getBackTime()
+
"% 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 83 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516171911263 :ss= 1516170709449 :fs= 15
13243317440 :pn= 4 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(R)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnu
m( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ("
+getGoTime()
+")WT.al_orgdate 2 ("
+getBackTime()
+")");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}


```
publicstaticvoidspiderHtml()throwsException{
```
// 1 .指定目标网站
String url =
"http:// 192. 168. 56. 161 /B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =CAN
&c 2 =CTU&d 1 = 2018 - 01 - 17 &at= 1 &ct= 0 &it= 0 ";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",

"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();


parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderJs()throwsException{
```
// 1 .指定目标网站
String url =
"http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/modules/common/requireConfig.js";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",

"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;


sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderCss()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/css/flight.css";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader("Referer",
```
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html");
```

httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticvoidspiderPng()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/images/common.png";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
```

###### // 3 .设置请求参数

```
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum
( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求


```
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}
```
```
publicstaticvoidspiderJpg()throwsException{
```
```
// 1 .指定目标网站
Stringurl="http:// 192. 168. 56. 161 /B 2 C 40 /dist/main/images/loadingimg.jpg";
// 2 .发起请求
HttpPosthttpPost=newHttpPost(url);
// 3 .设置请求参数
httpPost.setHeader("Time-Local",getLocalDateTime());
httpPost.setHeader("Requst",
"POST/B 2 C 40 /query/jaxb/direct/query.aoHTTP/ 1. 1 ");
httpPost.setHeader("RequestMethod","POST");
httpPost.setHeader("Content-Type",
"application/x-www-form-urlencoded;charset=UTF- 8 ");
httpPost.setHeader(
"Referer",
```
"http://b 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =C
AN&c 2 =WUH&d 1 = 2018 - 02 - 20 &at= 1 &ct= 0 &it= 0 ");
httpPost.setHeader("RemoteAddress"," 192. 168. 56. 1 ");
httpPost.setHeader(
"User-Agent",
"Mozilla/ 5. 0 (WindowsNT 10. 0 ;Win 64 ;x 64 )AppleWebKit/ 537. 36 (KHTML,
likeGecko)Chrome/ 63. 0. 3239. 132 Safari/ 537. 36 ");
httpPost.setHeader("Time-Iso 8601 ",getISO 8601 Timestamp());
httpPost.setHeader("ServerAddress"," 192. 168. 56. 161 ");
httpPost.setHeader(
"Cookie",
"JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
sid=b 5 cc 11 e 02 e 154 ac 5 b 0 f 3609332 f 86803 ; aid= 8 ae 8768760927 e 280160 bb 348 bef 3 e 12 ;
identifyStatus=N; userType 4 logCookie=M; userId 4 logCookie= 13818791413 ;
useridCookie= 13818791413 ; userCodeCookie= 13818791413 ;
temp_zh=cou% 3 D 0 % 3 Bsegt% 3 D%E 5 % 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 13 % 3 B%E 5 %B 9
%BF%E 5 %B 7 % 9 E-%E 5 % 8 C% 97 %E 4 %BA%AC% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 cou% 3 D 1 % 3 Bsegt% 3 D%E 5
% 8 D% 95 %E 7 %A 8 % 8 B% 3 Btime% 3 D 2018 - 01 - 17 % 3 B%E 5 %B 9 %BF%E 5 %B 7 % 9 E-%E 6 % 88 % 90 %E 9 % 8
3 %BD% 3 B 1 % 2 C 0 % 2 C 0 % 3 B% 26 ; JSESSIONID= 782121159357 B 98 CA 6112554 CF 44321 E;
WT-FPC=id= 211. 103. 142. 26 - 608782688. 30635197 :lv= 1516170718655 :ss= 1516170709449 :fs= 15
13243317440 :pn= 2 :vn= 10 ; language=zh_CN;
WT.al_flight=WT.al_hctype(S)% 3 AWT.al_adultnum( 1 )% 3 AWT.al_childnum( 0 )% 3 AWT.al_infantnum


( 0 )% 3 AWT.al_orgcity 1 (CAN)% 3 AWT.al_dstcity 1 (CTU)% 3 AWT.al_orgdate 1 ( 2018 - 01 - 17 )");
// 4 .设置请求参数
ArrayList<BasicNameValuePair>parameters=newArrayList<BasicNameValuePair>();
parameters
.add(newBasicNameValuePair(
"json",
"{\"depcity\":\"CAN\", \"arrcity\":\"WUH\",
\"flightdate\":\" 20180220 \", \"adultnum\":\" 1 \", \"childnum\":\" 0 \", \"infantnum\":\" 0 \",
\"cabinorder\":\" 0 \",\"airline\":\" 1 \",\"flytype\":\" 0 \",\"international\":\" 0 \",\"action\":\" 0 \",
\"segtype\":\" 1 \",\"cache\":\" 0 \",\"preUrl\":\"\",\"isMember\":\"\"}"));
httpPost.setEntity(newUrlEncodedFormEntity(parameters));
// 5 .发起请求
CloseableHttpClienthttpClient=HttpClients.createDefault();
CloseableHttpResponseresponse=httpClient.execute(httpPost);
// 6 .获取返回值
System.out.println(response!=null);
}

```
publicstaticStringgetLocalDateTime(){
DateFormatdf=newSimpleDateFormat("dd/MMM/yyyy'T'HH:mm:ss+ 08 : 00 ",
Locale.ENGLISH);
StringnowAsISO=df.format(newDate());
returnnowAsISO;
```
```
}
```
```
publicstaticStringgetISO 8601 Timestamp(){
DateFormatdf=newSimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss+ 08 : 00 ");
StringnowAsISO=df.format(newDate());
returnnowAsISO;
}
```
```
publicstaticStringgetGoTime(){
DateFormatdf=newSimpleDateFormat("yyyy-MM-dd");
StringnowAsISO=df.format(newDate());
returnnowAsISO;
}
```
publicstaticStringgetBackTime(){
Datedate=newDate();// 取时间
Calendarcalendar=newGregorianCalendar();
calendar.setTime(date);
calendar.add(calendar.DATE,+ 1 );// 把日期往前减少一天，若想把日期向后推一天则
将负数改为正数


date=calendar.getTime();
SimpleDateFormatformatter=newSimpleDateFormat("yyyy-MM-dd");
StringdateString=formatter.format(date);
returndateString;
}
}

# 16. Dataprocess 功能概述

## 16. 1. 模块用例图

###### 图数据处理用例图


## 16. 2. 模块分类处理流


# 17. 构建 streaming 项目

## 17. 1. 创建项目

## 17. 1. 1. 创建项目



## 17. 1. 2. POM 文件

<?xmlversion=" 1. 0 "encoding="UTF- 8 "?>
<projectxmlns="http://maven.apache.org/POM/ 4. 0. 0 "
xmlns:xsi="http://www.w 3 .org/ 2001 /XMLSchema-instance"
xsi:schemaLocation="http://maven.apache.org/POM/ 4. 0. 0
[http://maven.apache.org/xsd/maven-](http://maven.apache.org/xsd/maven-) 4. 0. 0 .xsd">
<modelVersion> 4. 0. 0 </modelVersion>

```
<groupId>cn.itcast</groupId>
```

<artifactId>ispider</artifactId>
<version> 1. 0 - SNAPSHOT</version>

<properties>
<project.build.sourceEncoding>UTF- 8 </project.build.sourceEncoding>
<jdk.version> 1. 8 </jdk.version>
<scala.binary.version> 2. 11 </scala.binary.version>
<scala.version> 2. 11. 8 </scala.version>
<spark.version> 2. 1. 3 </spark.version>
<kafka.version> 1. 1. 0 </kafka.version>
<hadoop.version> 2. 6. 2 </hadoop.version>
<mysql.driver.version> 5. 1. 35 </mysql.driver.version>
<jedis.version> 2. 9. 0 </jedis.version>
<fastjson.version> 1. 2. 4 </fastjson.version>
<junit.version> 4. 12 </junit.version>
<c 3 p 0 .version> 0. 9. 1. 2 </c 3 p 0 .version>
<commons-lang.version> 2. 6 </commons-lang.version>
</properties>
<dependencies>
<dependency>
<groupId>org.scala-lang</groupId>
<artifactId>scala-library</artifactId>
<version>${scala.version}</version>
</dependency>
<!--spark-->
<!--spark-core-->
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-core_${scala.binary.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<!--spark-streaming-->
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming_${scala.binary.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<!--spark-sql-->
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-sql_${scala.binary.version}</artifactId>
<version>${spark.version}</version>
</dependency>


<!--spark-streaming-kafka- 0 - 10 - ->
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming-kafka- 0 - 8 _${scala.binary.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<!--spark-streaming-flume-->
<dependency>
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>
<version>${spark.version}</version>
</dependency>
<!--Spark中的RPC是使用Akka实现的， -->
<dependency>
<groupId>com.typesafe.akka</groupId>
<artifactId>akka-actor_ 2. 11 </artifactId>
<version> 2. 5. 4 </version>
</dependency>

<!--commons-lang-->
<dependency>
<groupId>commons-lang</groupId>
<artifactId>commons-lang</artifactId>
<version>${commons-lang.version}</version>
</dependency>

<!--logging-->
<dependency>
<groupId>org.apache.logging.log 4 j</groupId>
<artifactId>log 4 j-api</artifactId>
<version> 2. 11. 0 </version>
<!--<scope>runtime</scope>-->
</dependency>
<dependency>
<groupId>org.apache.logging.log 4 j</groupId>
<artifactId>log 4 j-core</artifactId>
<version> 2. 11. 0 </version>
<!--<scope>runtime</scope>-->
</dependency>
<dependency>
<groupId>org.apache.logging.log 4 j</groupId>
<artifactId>log 4 j-slf 4 j-impl</artifactId>


<version> 2. 11. 0 </version>
<!--<scope>runtime</scope>-->
</dependency>

<!--testing-->
<dependency>
<groupId>junit</groupId>
<artifactId>junit</artifactId>
<version> 4. 4 </version>
<scope>test</scope>
</dependency>
<!--注意：依赖不在Hadoop环境中，集群运行需要上传依赖包并指定-->
<dependency>
<groupId>com.jayway.jsonpath</groupId>
<artifactId>json-path</artifactId>
<version> 2. 3. 0 </version>
</dependency>

<dependency>
<groupId>org.apache.hadoop</groupId>
<artifactId>hadoop-client</artifactId>
<version>${hadoop.version}</version>
</dependency>
<!--mysqldriver-->
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>${mysql.driver.version}</version>
</dependency>
<!--c 3 p 0 数据库连接池-->
<dependency>
<groupId>c 3 p 0 </groupId>
<artifactId>c 3 p 0 </artifactId>
<version>${c 3 p 0 .version}</version>
</dependency>
<!--redis缓存-->
<dependency>
<groupId>redis.clients</groupId>
<artifactId>jedis</artifactId>
<version>${jedis.version}</version>
</dependency>
<dependency>
<groupId>com.alibaba</groupId>


```
<artifactId>fastjson</artifactId>
<version>${fastjson.version}</version>
</dependency>
<dependency>
<groupId>junit</groupId>
<artifactId>junit</artifactId>
<version>${junit.version}</version>
</dependency>
</dependencies>
```
<!--maven官方 [http://repo](http://repo) 1 .maven.org/maven 2 / 或 [http://repo](http://repo) 2 .maven.org/maven 2 /
（延迟低一些） -->
<repositories>

```
<repository>
<id>central</id>
<name>MavenRepositorySwitchboard</name>
<layout>default</layout>
<url>http://repo 2 .maven.org/maven 2 </url>
<snapshots>
<enabled>false</enabled>
</snapshots>
</repository>
<repository>
<id>scala-tools.org</id>
<name>Scala-ToolsMaven 2 Repository</name>
<url>http://scala-tools.org/repo-releases</url>
</repository>
</repositories>
```
```
<build>
<sourceDirectory>src/main/scala</sourceDirectory>
<testSourceDirectory>src/test/scala</testSourceDirectory>
<plugins>
<plugin>
<!--MAVEN编译使用的JDK版本-->
<groupId>org.apache.maven.plugins</groupId>
<artifactId>maven-compiler-plugin</artifactId>
<version> 3. 3 </version>
<configuration>
<source>${jdk.version}</source>
<target>${jdk.version}</target>
<encoding>${project.build.sourceEncoding}</encoding>
</configuration>
```

```
</plugin>
<plugin>
<groupId>net.alchim 31 .maven</groupId>
<artifactId>scala-maven-plugin</artifactId>
<version> 3. 2. 0 </version>
<executions>
<execution>
<goals>
<goal>compile</goal>
<goal>testCompile</goal>
</goals>
<configuration>
<args>
<arg>-dependencyfile</arg>
```
<arg>${project.build.directory}/.scala_dependencies</arg>
</args>
</configuration>
</execution>
</executions>
</plugin>
</plugins>
</build>

</project>

## 17. 1. 3. 引入配置文件


## 17. 1. 4. 创建包

## 17. 1. 5. 引入 bean


## 17. 1. 6. 引入 util

# 18. Streaming 消费 kafka 的两种方式

## 18. 1. KafkaUtils.createStream

## 18. 1. 1. 特点

构造函数为KafkaUtils.createStream(ssc,[zk],[consumergroupid],[per-topic,partitions])
使用了receivers来接收数据，利用的是Kafka高层次的消费者api，对于所有的receivers接
收到的数据将会保存在sparkexecutors中，然后通过SparkStreaming启动job来处理这些数
据，默认会丢失，可启用WAL日志，该日志存储在HDFS上
 创建一个receiver来对kafka进行定时拉取数据，ssc的rdd分区和kafka的topic分区不
是一个概念，故如果增加特定主体分区数仅仅是增加一个receiver中消费topic的线程
数，并不增加spark的并行处理数据数量
 对于不同的group和topic可以使用多个receivers创建不同的DStream
 如果启用了WAL，需要设置存储级别，即
KafkaUtils.createStream(....,StorageLevel.MEMORY_AND_DISK_SER)

## 18. 1. 2. 代码实现

packageorg.apache.spark.stream


importorg.apache.spark.streaming.kafka.KafkaUtils
importorg.apache.spark.streaming.{Seconds,StreamingContext}
importorg.apache.spark.{HashPartitioner,SparkConf}

objectKafkaWordCount{
valupdateFunction=(iter:Iterator[(String,Seq[Int],Option[Int])])=>{
iter.flatMap{case(x,y,z)=>Some(y.sum+z.getOrElse( 0 )).map(v=>(x,v))}
}
defmain(args:Array[String]){
valconf=newSparkConf().setMaster("local[ 2 ]").setAppName("KafkaWordCount")
valssc=newStreamingContext(conf,Seconds( 5 ))
//回滚点设置在本地
//ssc.checkpoint("./")
//将回滚点写到hdfs
ssc.checkpoint("hdfs://node 1 : 9000 /streamcheckpoint")
val Array(zkQuorum, groupId, topics, numThreads) =
Array[String]("node 2 : 2181 ,node 3 : 2181 ,node 4 : 2181 ","g 1 ","wangsf-test"," 2 ")
valtopicMap=topics.split(",").map((_,numThreads.toInt)).toMap
vallines=KafkaUtils.createStream(ssc,zkQuorum,groupId,topicMap).map(_._ 2 )
valresults=lines.flatMap(_.split("")).map((_, 1 )).updateStateByKey(updateFunction,new
HashPartitioner(ssc.sparkContext.defaultParallelism),true)
results.foreachRDD(x=>x.foreach(println))
ssc.start()
ssc.awaitTermination()
}
}

## 18. 2. KafkaUtils.createDirectStream

## 18. 2. 1. 特点

区别Receiver接收数据，这种方式定期地从kafka的topic+partition中查询最新的偏移
量，再根据偏移量范围在每个batch里面处理数据，使用的是kafka的简单消费者api
优点:
 简化并行，不需要多个kafka输入流，该方法将会创建和kafka分区一样的rdd个数，
而且会从kafka并行读取。
 高效，这种方式并不需要WAL，WAL模式需要对数据复制两次，第一次是被kafka复制，
另一次是写到wal中
 恰好一次语义(Exactly-once-semantics)，传统的读取kafka数据是通过kafka高层次api
把偏移量写入zookeeper中，存在数据丢失的可能性是zookeeper中和ssc的偏移量不
一致。EOS通过实现kafka低层次api，偏移量仅仅被ssc保存在checkpoint中，消除了
zk和ssc偏移量不一致的问题。


## 18. 2. 2. 代码实现

packageorg.apache.spark.stream

importkafka.serializer.StringDecoder
importorg.apache.spark.streaming.kafka.KafkaUtils
importorg.apache.spark.streaming.{Seconds,StreamingContext}
importorg.apache.spark.{HashPartitioner,SparkConf}

objectKafkaWordCountDrect{
valupdateFunction=(iter:Iterator[(String,Seq[Int],Option[Int])])=>{
iter.flatMap{case(x,y,z)=>Some(y.sum+z.getOrElse( 0 )).map(v=>(x,v))}
}
defmain(args:Array[String]){
valconf=newSparkConf().setMaster("local[ 2 ]").setAppName("KafkaWordCountDrect")
valssc=newStreamingContext(conf,Seconds( 5 ))
//回滚点设置在本地
ssc.checkpoint("./")
//将回滚点写到hdfs
//ssc.checkpoint("hdfs://master 1 : 9000 /streamcheckpoint")
valbrokerList:String="node 2 : 9092 ,node 3 : 9092 ,node 4 : 9092 "
valsourceTopic:Set[String]=Set("wangsf-test")
valkafkaParams=Map("metadata.broker.list"->brokerList)
vallines =KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,
kafkaParams,sourceTopic).map(_._ 2 )
valresults=lines.flatMap(_.split("")).map((_, 1 )).updateStateByKey(updateFunction,new
HashPartitioner(ssc.sparkContext.defaultParallelism),true)
results.foreachRDD(x=>x.foreach(println))
ssc.start()
ssc.awaitTermination()
}
}

# 19. 消费 kafka 数据

## 19. 1. 需求

使用KafkaUtils.createDirectStream消费lua生产到kafka的数据


## 19. 2. 代码

packagecom.air.antispider.stream.dataprocess
importcom.air.antispider.stream.common.util.jedis.PropertiesUtil
importcom.air.antispider.stream.common.util.log 4 j.LoggerLevels
importkafka.serializer.StringDecoder
importorg.apache.spark.sql.SQLContext
importorg.apache.spark.streaming.kafka.KafkaUtils
importorg.apache.spark.streaming.{Seconds,StreamingContext}
importorg.apache.spark.{SparkConf,SparkContext}

/**
*Createdbywangsenfengon 2018 / 11 / 18.
* 运行数据处理的主类
*/
objectLauncher{

defmain(args:Array[String]){
LoggerLevels.setStreamingLogLevels()
//当应用被停止的时候，进行如下设置可以保证当前批次执行完之后再停止应用。
System.setProperty("spark.streaming.stopGracefullyOnShutdown","true")
//初始化sparkcontext
valconf=newSparkConf()
.setAppName("streaming-data-peocess")
.setMaster("local[*]")
.set("spark.metrics.conf.executor.source.jvm.class",
"org.apache.spark.metrics.source.JvmSource")
valsc=newSparkContext(conf)
//创建sql环境
valsqlcc=newSQLContext(sc)
//加载配置文件中kafka配置
val brokerList: String = PropertiesUtil.getStringByKey("default.brokers",
"kafkaConfig.properties")
val sourceTopic: Set[String] = Set(PropertiesUtil.getStringByKey("source.nginx.topic",
"kafkaConfig.properties"))
valkafkaParams=Map("metadata.broker.list"->brokerList)
//创建流式处理类
valscc:StreamingContext=setupSsc(sc,sqlcc,kafkaParams,sourceTopic)
//开启streaming
scc.start()
scc.awaitTermination()
}


###### /**

###### *业务处理流程

###### *

*@paramsc
*@paramsqlcc
*@paramkafkaParams
*@paramsourceTopic
*@return
*/
def setupSsc(sc: SparkContext, sqlcc: SQLContext, kafkaParams: Map[String, String],
sourceTopic:Set[String]):StreamingContext={
//创建stream
valscc:StreamingContext=newStreamingContext(sc,Seconds( 1 ))
//消费kafka数据
val messages = KafkaUtils.createDirectStream[String, String, StringDecoder,
StringDecoder](scc,kafkaParams,sourceTopic)
//打印数据
valmap=messages.map(_._ 2 )
map.foreachRDD(x=>{
x.foreach(println)
})
//返回scc
scc
}
}

## 19. 3. 测试

###### 1 、使用爬虫生产数据

2 、消费kafka数据并打印

# 20. 链路统计

## 20. 1. 需求

```
统计lua链路数据
```

## 20. 2. 设计

```
1 、部署服务器：server_ip，可以从kafka的数据中获取
2 、 当前服务器的活跃连接数：
需要增加lua采集指标，对接到kafka中
```
在streaming程序中获取数据，数据采集到redis中，写到数据库
3 、最近三天采集数据量
昨天：每天的采集数据在stream中统计，存储到redis中（过期时间 24 小时），web
端定时任务每天 24 点统计数据到数据库
前天： 24 点定时任务把昨天的数据更新到前天，重新计算昨天的数据
前三天：累加上面两个数据和当天redis中的数据


## 20. 3. 代码

## 20. 3. 1. 数据收集代码

packagecom.air.antispider.stream.dataprocess.businessprocess

importcom.air.antispider.stream.common.util.jedis.{JedisConnectionUtil,PropertiesUtil}
importorg.apache.spark.rdd.RDD
importorg.json 4 s.DefaultFormats
importorg.json 4 s.jackson.Json

/**
*Createdbywangsenfengon 2018 / 11 / 22.
*/
objectBusinessProcess{
deflinkCount(messageRDD:RDD[String]):Unit={
//链路统计
messageRDD.foreach(println)
valserversCount=messageRDD.map(
//对rdd数据进行map
record=>{
//将数据进行分割，分隔符#CS#
valvalues=record.split("#CS#",- 1 )
//获取到serveraddress，封装成（serveraddr， 1 ）
if(values.length> 9 )(values( 9 ), 1 )else("", 1 )
}
//按照serveraddr进行汇总
).reduceByKey((x,y)=>x+y)
//记录活跃数
valactiveNum=messageRDD.map(
//对rdd数据进行map
record=>{


###### //将数据进行分割，分隔符#CS#

valvalues=record.split("#CS#",- 1 )
//获取到serveraddress，封装成（serveraddr， 1 ）
if(values.length> 10 )(values( 9 ),values( 11 ))else("", 1 )
}
//按照serveraddr进行汇总
).reduceByKey((x,y)=>y)
if(!activeNum.isEmpty()&&!serversCount.isEmpty()){
//链路统计结果转换为map
valserversCountMap=serversCount.collectAsMap()
//活跃数转换为map
valavtiveNumMap=activeNum.collectAsMap()
//将结果输出到redis
valfieldMap=scala.collection.mutable.Map(
"serversCountMap"->serversCountMap,
"avtiveNumMap"->avtiveNumMap)
//连接redis
valjedis=JedisConnectionUtil.getJedisCluster
try{
//redis有效期，单位秒
valmonitorDataExpTime:Int=PropertiesUtil.getStringByKey("cluster.exptime.monitor",
"jedisConfig.properties").toInt
//链路数据保存的key，CSANTI_MONITOR_LP+时间戳
val keyName = PropertiesUtil.getStringByKey("cluster.key.monitor.linkProcess",
"jedisConfig.properties")+System.currentTimeMillis.toString
//更新最新的监控数据
jedis.setex(keyName,monitorDataExpTime,Json(DefaultFormats).write(fieldMap))
}catch{
casee:Exception=>
e.printStackTrace()
jedis.close
}
}
}
}


## 20. 3. 2. 数据同步 mysql 代码

@Override
publicvoidsaveDataCollectData(){

```
JedisClusterjedisCluster=JedisConnectionUtil.getJedisCluster();
//记录当天的链路总数
Map<String,Integer>linkCount=TrafficUtil
.trafficLinkInfo(Constants.CSANTI_MONITOR_LP);
//循环存储
Set<String>keySet=linkCount.keySet();
for(StringserverAddr:keySet){
//根据serveraddr查询链路历史数据
Stringhql="fromDatacollectwhereserver_name=:serverName";
Map<String,Object>params=newHashMap<String,Object>();
params.put("serverName",serverAddr);
Datacollectdatacollect=dataCollectDao.get(hql,params);
//历史数据不存在，新存入数据
if(datacollect==null){
datacollect=newDatacollect();
datacollect.setId(UUID.randomUUID().toString());
datacollect.setServerName(serverAddr);
datacollect.setLastThreeDaysNum(linkCount.get(serverAddr));
datacollect.setBeforeYesterdayNum( 0 );
datacollect.setYesterdayNum(linkCount.get(serverAddr));
dataCollectDao.save(datacollect);
}else{
//历史数据存在，累加
datacollect.setLastThreeDaysNum(linkCount.get(serverAddr)
+datacollect.getYesterdayNum()
+datacollect.getBeforeYesterdayNum());
datacollect
```

```
.setBeforeYesterdayNum(datacollect.getYesterdayNum());
datacollect.setYesterdayNum(linkCount.get(serverAddr));
dataCollectDao.update(datacollect);
}
}
}
```
## 20. 3. 3. 数据展示代码

 Js


###### /**

###### *

###### * 获取所有服务器信息

*@return
*/
@RequestMapping(value="/getDataCollect",method=RequestMethod.GET)
@ResponseBody
publicList<DatacollectView>getDataCollect(HttpServletRequestrequest){
List<DatacollectView>dataCollectViewList=newArrayList<DatacollectView>();
try{
JedisClusterjedisCluster=JedisConnectionUtil.getJedisCluster();
//从redis中拿出所有数据
List<Datacollect>dataCollectList=dataCollectService.getDataCollect();
for(Datacollectdatacollect:dataCollectList){
String last = JedisConnectionUtil.keys(jedisCluster,
Constants.CSANTI_MONITOR_LP+"*").last();
Stringvalue=jedisCluster.get(last);
LinkJsonVOresolveLinkJson=JsonResolveUtil.resolveLinkJson(value);
Map<String,Integer>map 2 =resolveLinkJson.getAvtiveNumMap();
Set<String>keySet=map 2 .keySet();
for(Stringkey:keySet){
if(key.equals(datacollect.getServerName())){
DatacollectViewdatacollectView=newDatacollectView();
datacollectView.setActiveNum(map 2 .get(key));

```
datacollectView.setBeforeYesterdayNum(datacollect.getBeforeYesterdayNum());
datacollectView.setId(datacollect.getId());
```
```
datacollectView.setLastThreeDaysNum(datacollect.getLastThreeDaysNum());
datacollectView.setServerName(datacollect.getServerName());
```

```
datacollectView.setYesterdayNum(datacollect.getYesterdayNum());
dataCollectViewList.add(datacollectView);
}
}
}
}catch(Exceptione){
logger.info(e.getMessage());
}
returndataCollectViewList;
}
```
# 21. 课程目标

###### 1 、实现数据清洗模块

###### 2 、实现数据脱敏模块

###### 3 、实现数据分类打标签模块

###### 4 、解析“查询数据”

###### 5 、解析“预定数据”

# 22. 数据清洗模块

## 22. 1. 需求

1 、kafka读取过来的数据格式为：

其中夹杂着很多的css、js、jpg、png等静态文件，这些静态文件并非业务需求文件，所
以，我们需要将这些文件过滤掉，实现数据清洗的功能
2 、数据库中存储了过滤规则：

我们需要根据过滤规则进行数据过滤，后期需要设计到界面控制，实现数据库和redis
标记同步。


## 22. 2. 设计

1 、在sc初始化后读取kafka数据前，我们需要将数据库过滤规则读取，并且设置到广播变
量中，供所有的executor使用。
2 、在读取kafka数据进行处理时，业务上线后会一直进行业务计算，在这期间可能过滤规
则会发生改变，所以需要根据redis中的标记来决定是否更新过滤规则，所以每处理一
批数据，需要查看redis标识，决定是否更新规则。
3 、读取和更新规则后，根据规则过滤kafka中的数据，只保留query相关的url，js等信息
过滤掉。

## 22. 3. 代码

## 22. 3. 1. 查询过滤规则

###### //初始化的时候读取规则，更新到广播变量

```
vallist=AnalyzeRuleDB.queryFilterRule()
@volatilevarbroadcastFilterList=sc.broadcast(list)
```
## 22. 3. 2. AnalyzeRuleDB.queryFilterRule()

objectAnalyzeRuleDB{
/**
*查询过滤规则信息，添加到广播变量中
*
*@return
*/
defqueryFilterRule():ArrayBuffer[String]={
//从数据库中读取过滤规则
valnfrsql="selectvaluefromnh_filter_rule"
valnfrField="value"
vallist=QueryDB.queryData(nfrsql,nfrField)
list
}
}


## 22. 3. 3. QueryDB.queryData

packagecom.air.antispider.stream.common.util.database

importscala.collection.mutable.ArrayBuffer

/*通过数据库连接池查询数据*/
objectQueryDB{
/**
*数据查询
*
*@paramsqlSQL语句String类型
*@paramfield 查询字段
*@return 返回list列表
*/
defqueryData(sql:String,field:String):ArrayBuffer[String]={
valarr:ArrayBuffer[String]=newArrayBuffer[String]()
valconn=c 3 p 0 Util.getConnection
valps=conn.prepareStatement(sql)
valrs=ps.executeQuery()
while(rs.next()){
arr.+=(rs.getString(field))
}
c 3 p 0 Util.close(conn,ps,rs)
arr
}
}

## 22. 3. 4. 根据 redis 标识更新规则

###### //更新所有规则信息到广播变量

```
valjedis=JedisConnectionUtil.getJedisCluster
//从redis查询“过滤规则变更标识”
valneedUpdateFilterList=jedis.get("FilterChangeFlag")
//是否需要更新过滤规则
if(!needUpdateFilterList.isEmpty&&needUpdateFilterList.toBoolean){
//需要更新则查询更新信息
valfilterUpdateList=AnalyzeRuleDB.queryFilterRule()
//删除此广播变量
broadcastFilterList.unpersist()
//重新更新广播变量
broadcastFilterList=sc.broadcast(filterUpdateList)
```

###### //重新设置标识

```
jedis.set("FilterChangeFlag","false")
}
jedis.close()
```
## 22. 3. 5. 过滤数据

//过滤数据：只保留查询数据，过滤掉css、js等文件
valfiltered=messageRDD.filter{x=>URLFilter.filterURL(x,broadcastFilterList.value)}

## 22. 3. 6. URLFilter.filterURL

packagecom.air.antispider.stream.dataprocess.filters

importjava.sql.{Connection,ResultSet}

importcom.air.antispider.stream.common.util.database.c 3 p 0 Util
importorg.apache.spark.rdd.RDD

importscala.collection.mutable.ArrayBuffer

/**
*Createdbywangsenfeng.
*/
classURLFilter{
}
objectURLFilter{
/**
*请求URL过滤
*过滤数据，过滤掉css、js、png、jpg、html等静态文件
*/
deffilterURL(json:String,list:ArrayBuffer[String]):Boolean={
//匹配标记
varisMatch=true
//将请求数据用分隔符分割
valvalues=json.split("#CS#")
//取出request字段
valresult=if(values.length> 1 )values( 1 )else""
//循环广播变量中的过滤规则
for(rule<-list){
//匹配url
if(result.matches(rule)){
//匹配上，设置为false，意味着过滤掉此条数据


isMatch=false
}
}
isMatch
}
}

# 23. 数据脱敏模块

## 23. 1. 需求

由于cookie信息中可能带有敏感信息：用户手机号、用户身份证号（ID），对于这些敏
感信息，我们在做数据处理的时候需要进行脱敏，以免暴露用户信息。
使用MD 5 算法对用户的敏感信息进行加密

## 23. 2. 设计

###### 1 、引入MD 5 算法

###### 2 、对手机号进行MD 5 加密

###### 3 、对身份证号进行MD 5 加密

## 23. 3. 代码

## 23. 3. 1. 加密

## 23. 3. 2. EncryptedData.encryptedPhone

objectEncryptedData{
defencryptedPhone(mes:String):String={
valmd 5 =newMD 5 ()


###### //复制给局部变量

varencrypted=mes
//手机号正则
val phonePattern =
Pattern.compile("(( 13 [ 0 - 9 ])|( 14 [ 5 | 7 ])|( 15 ([ 0 - 3 ]|[ 5 - 9 ]))|( 17 [ 0 - 9 ])|( 18 [ 0 , 5 - 9 ]))\\d{ 8 }")
// 匹 配 规 则 ：
java.util.regex.Matcher[pattern=(( 13 [ 0 - 9 ])|( 14 [ 5 | 7 ])|( 15 ([ 0 - 3 ]|[ 5 - 9 ]))|( 17 [ 0 - 9 ])|( 18 [ 0 , 5 - 9 ]))\d{ 8 }
region= 0 , 1440 lastmatch=]
valphoneMatcher=phonePattern.matcher(mes)
//查找手机号
while(phoneMatcher.find()){
//手机号的前一个index，注：phoneMatcher.group()找到的手机号
vallowIndex=mes.indexOf(phoneMatcher.group())- 1
//手机号的后一个index
valhighIndex=lowIndex+phoneMatcher.group().length()+ 1
//手机号的前一个字符
vallowLetter=mes.charAt(lowIndex).toString()

//如果前一位字符不是数字，那就要看后一位是否是数字
if(!lowLetter.matches("^[ 0 - 9 ]$")){
//如果字符串的最后是手机号，直接替换即可
if(highIndex<mes.length()){
//手机号的后一个字符
valhighLetter=mes.charAt(highIndex).toString()
//后一位也不是数字，那说明这个字符串就是一个电话号码
if(!highLetter.matches("^[ 0 - 9 ]$")){
encrypted = encrypted.replace(phoneMatcher.group(),
md 5 .getMD 5 ofStr(phoneMatcher.group()))
}
}else{
encrypted = encrypted.replace(phoneMatcher.group(),
md 5 .getMD 5 ofStr(phoneMatcher.group()))
}
}
}
encrypted
}

## 23. 3. 3. EncryptedData.encryptedId

defencryptedId(mes:String):String={
valmd 5 =newMD 5 ()
//复制给局部变量
varencrypted=mes


```
validPattern=Pattern.compile("(\\d{ 18 })|(\\d{ 17 }(\\d|X|x))|(\\d{ 15 })")
validMatcher=idPattern.matcher(mes)
while(idMatcher.find()){
encrypted=encrypted.replace(idMatcher.group(),md 5 .getMD 5 ofStr(idMatcher.group()))
}
encrypted
}
```
# 24. 数据分类

## 24. 1. 需求

无论是“查询操作”还是“预定操作”，我们除了需要从请求中解析出常规的http信息
和用户信息外，还需要根据不同的操作（查询、预定）将数据分为查询数据和预定数据，然
后根据数据库中的解析规则，将requestbody解析出来，封装到对应的bean中。
所以，我们需要从原始数据中解析出航线类型（国际、国内）、操作类型（查询、预定），
从而将数据分为国内查询、国际查询、国内预定、国际预定等不同的维度进行打标记，然后
根据不同的解析规则进行解析body数据。
在查询操作的时候，我们已经提供了一个bean用来数据封装（QueryRequestData），其
中有两个字段（航线类型和是否往返）需要我们通过数据的解析出来，并打上标签，然后根
据标签去数据库匹配出解析规则，解析body。

在预定的时候，我们也提供了BookRequestData进行数据封装，同样，有两个字段
（flightType和traveType）需要我们从原始数据中解析出来，并打上标签，，然后根据标签去
数据库匹配出解析规则，解析body。


## 24. 2. 设计

1 、此时，我们处理的已经是通过map分出来的一条数据，而非rdd，并且对每条数据都进
行了脱敏处理

###### 2 、我们需要将脱敏后的数据进行分割，分割出一个一个的字段，供后面解析使用

3 、Request字段中存储着查询的url，我们通过加载数据库的url正则表达式去匹配出request
中的url，判断数据是哪种操作哪种航线，并封装到对应的RequestType中

4 、在http_refer存储着查询或者预定的日期，好处在于如果日期是一个就是单程，日期是
两个就是往返，我们通过上面封装的RequestType来进行判断，看看是属于往返查询还
是往返预定，当然http_refer只是在查询的时候封装了往返日期，如果是预定，往返信
息封装在requestbody中，我们并不能获取到预定信息，单位了保持程序的健壮性，我
们在这里需要进行判断区别查询和预定。

###### 5 、此时，我们已经将每条数据分类出了对应的标签，稍后，我们可以根据对应的标签，去


```
数据库读取所有解析规则，匹配出属于某一条数据的解析规则，根据规则解析出body
数据封装到对应的bean中
```
## 24. 3. 数据分割

## 24. 3. 1. 分割

###### /**

###### *对原始的数据分割

###### */

val (request, requestMethod, contentType, requestBody, httpReferrer, remoteAddr,
httpUserAgent,timeIso 8601 ,serverAddr,cookiesStr,
cookieValue_JSESSIONID,cookieValue_USERID)=DataSplit.dataSplit(encrypted)

## 24. 3. 2. DataSplit.dataSplit

packagecom.air.antispider.stream.dataprocess.businessprocess

importjava.util.regex.Pattern

importcom.air.antispider.stream.common.util.decode.{RequestDecoder,EscapeToolBox}
importcom.air.antispider.stream.common.util.jedis.PropertiesUtil
importcom.air.antispider.stream.common.util.string.CsairStringUtils

/**
*Createdbywangsenfengon 2018 / 11 / 28.
*/
objectDataSplit{
defdataSplit(mes:String):()={
//用#CS#分割数据
valvalues=mes.split("#CS#",- 1 )
//记录数据长度
valvaluesLength=values.length
//request原始数据
valregionalRequest=if(valuesLength> 1 )values( 1 )else""
//分割出request中的url
valrequest=if(regionalRequest.split("").length> 1 ){
regionalRequest.split("")( 1 )
}else{
""


###### }

###### //请求方式GET/POST

valrequestMethod=if(valuesLength> 2 )values( 2 )else""
//content_type
valcontentType=if(valuesLength> 3 )values( 3 )else""
//Post提交的数据体
valrequestBody=if(valuesLength> 4 )values( 4 )else""
//http_referrer
valhttpReferrer=if(valuesLength> 5 )values( 5 )else""
//客户端IP
valremoteAddr=if(valuesLength> 6 )values( 6 )else""
//客户端UA
valhttpUserAgent=if(valuesLength> 7 )values( 7 )else""
//服务器时间的ISO 8610 格式
valtimeIso 8601 =if(valuesLength> 8 )values( 8 )else""
//服务器地址
valserverAddr=if(valuesLength> 9 )values( 9 )else""
//Cookie信息
//原始信息中获取Cookie字符串，去掉空格，制表符
valcookiesStr=CsairStringUtils.trimSpacesChars(if(valuesLength> 10 )values( 10 )else"")
//提取Cookie信息并保存为K-V形式
valcookieMap={
vartempMap=newscala.collection.mutable.HashMap[String,String]
if(!cookiesStr.equals("")){
cookiesStr.split(";").foreach{s=>
valkv=s.split("=")
//UTF 8 解码
if(kv.length> 1 ){
try{
valchPattern=Pattern.compile("u([ 0 - 9 a-fA-F]{ 4 })")
valchMatcher=chPattern.matcher(kv( 1 ))
varisUnicode=false
while(chMatcher.find()){
isUnicode=true
}
if(isUnicode){
tempMap+=(kv( 0 )->EscapeToolBox.unescape(kv( 1 )))
}else{
tempMap+=(kv( 0 )->RequestDecoder.decodePostRequest(kv( 1 )))
}
}catch{
casee:Exception=>e.printStackTrace()
}
}


###### }

###### }

tempMap
}
//Cookie关键信息解析
//从配置文件读取Cookie配置信息
val cookieKey_JSESSIONID = PropertiesUtil.getStringByKey("cookie.JSESSIONID.key",
"cookieConfig.properties")
val cookieKey_userId 4 logCookie = PropertiesUtil.getStringByKey("cookie.userId.key",
"cookieConfig.properties")
//Cookie-JSESSIONID
valcookieValue_JSESSIONID=cookieMap.getOrElse(cookieKey_JSESSIONID,"NULL")
//Cookie-USERID-用户ID
valcookieValue_USERID=cookieMap.getOrElse(cookieKey_userId 4 logCookie,"NULL")

(request,requestMethod,contentType,requestBody,httpReferrer,remoteAddr,httpUserAgent,timeI
so 8601 ,serverAddr,cookiesStr,cookieValue_JSESSIONID,cookieValue_USERID)
}
}

## 24. 4. 加载更新数据库规则

## 24. 4. 1. 加载数据库正则

###### //分类规则

```
valruleMapTemp=AnalyzeRuleDB.queryRuleMap()
@volatilevarbroadcastRuleMap=sc.broadcast(ruleMapTemp)
```
## 24. 4. 2. AnalyzeRuleDB.queryRuleMap

###### /**

###### *查询分类规则正则表达式，添加到广播变量

###### *

*@return
*/
defqueryRuleMap():java.util.Map[String,ArrayBuffer[String]]={
//从数据库中查找航班分类规则-国内查询
val nqsql = "select expression from nh_classify_rule where flight_type = " +
FlightTypeEnum.National.id+"andoperation_type="+BehaviorTypeEnum.Query.id
//从数据库中查找航班分类规则-国际查询
val iqsql = "select expression from nh_classify_rule where flight_type = " +
FlightTypeEnum.International.id+"andoperation_type="+BehaviorTypeEnum.Query.id


###### //从数据库中查找航班分类规则-国内预定

val nbsql = "select expression from nh_classify_rule where flight_type = " +
FlightTypeEnum.National.id+"andoperation_type="+BehaviorTypeEnum.Book.id
//从数据库中查找航班分类规则-国际预定
val ibsql = "select expression from nh_classify_rule where flight_type = " +
FlightTypeEnum.International.id+"andoperation_type="+BehaviorTypeEnum.Book.id
valncrField="expression"
valruleMapTemp:java.util.Map[String,ArrayBuffer[String]]=newjava.util.HashMap[String,
ArrayBuffer[String]]
valnationalQueryList=QueryDB.queryData(nqsql,ncrField)
valinternationalQueryList=QueryDB.queryData(iqsql,ncrField)
valnationalBookList=QueryDB.queryData(nbsql,ncrField)
valinternationalBookList=QueryDB.queryData(ibsql,ncrField)
ruleMapTemp.put("nationalQuery",nationalQueryList)
ruleMapTemp.put("internationalQuery",internationalQueryList)
ruleMapTemp.put("nationalBook",nationalBookList)
ruleMapTemp.put("internationalBook",internationalBookList)
ruleMapTemp
}

## 24. 4. 3. 规则更新

###### //请求分类规则变更标识

```
valneedUpdateClassifyRule=jedis.get("ClassifyRuleChangeFlag")
//Mysql-规则是否改变标识
if(!needUpdateClassifyRule.isEmpty()&&needUpdateClassifyRule.toBoolean){
valruleMapTemp=AnalyzeRuleDB.queryRuleMap()
broadcastRuleMap.unpersist()
broadcastRuleMap=sc.broadcast(ruleMapTemp)
jedis.set("ClassifyRuleChangeFlag","false")
}
```
## 24. 5. 分类打标签

## 24. 5. 1. 分类

###### /**

###### * 分类：根据请求类型打标记

###### * 请求的类型，查询、预定、国内、国际

###### */

val requestTypeLabel = RequestTypeClassifier.classifyByRequest(request,
broadcastRuleMap.value)


## 24. 5. 2. RequestTypeClassifier.classifyByRequest

packagecom.air.antispider.stream.dataprocess.businessprocess

importcom.air.antispider.stream.common.bean.RequestType
importcom.air.antispider.stream.dataprocess.constants.{BehaviorTypeEnum,FlightTypeEnum}

importscala.collection.mutable.ArrayBuffer

classRequestTypeClassifier{

}

objectRequestTypeClassifier{

###### /**

*根据RequestURL匹配分类规则，分类打标签
*( 0 , 0 )-查询国内，( 0 , 1 )-查询国际，( 1 , 0 )-预定国内，( 1 , 1 )-预定国际，(null,null)-既不是查
询也不是预定
*/
def classifyByRequest(request: String, map: java.util.Map[String, ArrayBuffer[String]]):
RequestType={

```
//国内查询URL-正则表达式
valnationalQueryList=map.get("nationalQuery")
//国际查询URL-正则表达式
valinternationalQueryList=map.get("internationalQuery")
//国内预定URL-正则表达式
valnationalBookList=map.get("nationalBook")
//国际预定URL-正则表达式
valinternationalBookList=map.get("internationalBook")
//标记，如果哪个都没匹配上，另外打标记
varflag=true
//请求类型
varrequestType:RequestType=null
//国内查询循环匹配
for(nqTemp<- 0 untilnationalQueryList.sizeifflag){
//匹配上
if(request.matches(nationalQueryList(nqTemp))){
//匹配上，设置为false，不用另外打标记
flag=false
```

```
//将请求类型封装到RequestType中返回
requestType=RequestType(FlightTypeEnum.National,BehaviorTypeEnum.Query)
}
}
```
```
for(iqTemp<- 0 untilinternationalQueryList.sizeifflag){
if(request.matches(internationalQueryList(iqTemp))){
flag=false
requestType=RequestType(FlightTypeEnum.International,BehaviorTypeEnum.Query)
}
}
```
```
for(nbTemp<- 0 untilnationalBookList.sizeifflag){
if(request.matches(nationalBookList(nbTemp))){
flag=false
requestType=RequestType(FlightTypeEnum.National,BehaviorTypeEnum.Book)
}
}
```
for(ibTemp<- 0 untilinternationalBookList.sizeifflag){
if(request.matches(internationalBookList(ibTemp))){
flag=false
requestType=RequestType(FlightTypeEnum.International,BehaviorTypeEnum.Book)
}
}
//没匹配上，打个other的标记
if(flag){
requestType=RequestType(FlightTypeEnum.Other,BehaviorTypeEnum.Other)
}
//返回标记
requestType
}
}

## 24. 6. 往返信息打标签

## 24. 6. 1. 往返信息

###### /**

```
* 解析：根据http_refer，增加标记（单程或往返）
*/
valtravelType=TravelTypeClassifier.classifyByRefererAndRequestBody(
requestTypeLabel,httpReferrer,requestBody)
```

## 24. 6. 2. TravelTypeClassifier.classifyByRefererAndRequestBo

## dy

packagecom.air.antispider.stream.dataprocess.businessprocess

importcom.air.antispider.stream.common.bean.RequestType
importcom.air.antispider.stream.common.util.xml.xmlUtil
importcom.air.antispider.stream.dataprocess.constants.TravelTypeEnum.TravelTypeEnum
importcom.air.antispider.stream.dataprocess.constants.{BehaviorTypeEnum,TravelTypeEnum}

###### /**

###### * 单程或往返分类器

###### */

objectTravelTypeClassifier{
/**
*根据请求类型（http_refer）进行分类并统计
* http_refer ：
[http://b](http://b) 2 c.csair.com/B 2 C 40 /modules/bookingnew/main/flightSelectDirect.html?t=S&c 1 =CAN&c
2 =WUH&d 1 = 2018 - 01 - 28 &at= 1 &ct= 0 &it= 0
*
*@paramrequestTypeLabel
*@paramhttpReferrer
*@paramrequestBody
*@return
*/
def classifyByRefererAndRequestBody(requestTypeLabel: RequestType, httpReferrer: String,
requestBody:String):TravelTypeEnum={
//创建往返类型标记类
varresult=TravelTypeEnum(- 1 )
//操作类型为查询
if(BehaviorTypeEnum.Query==requestTypeLabel.behaviorType){
vardateCounts= 0
//日期正则
valregex="^(\\d{ 4 })-( 0 \\d{ 1 }| 1 [ 0 - 2 ])-( 0 \\d{ 1 }|[ 12 ]\\d{ 1 }| 3 [ 01 ])$"
//用？分割
if(httpReferrer.contains("?")&&httpReferrer.split("\\?").length> 1 ){
//先用？分割，再用&分割
valparams=httpReferrer.split("\\?")( 1 ).split("&")
//取到所有的参数循环
for(param<-params){
//用“=”分割


```
valkeyAndValue=param.split("=")
//正则匹配日期
if(keyAndValue.length> 1 &&keyAndValue( 1 ).matches(regex)){
dateCounts=dateCounts+ 1
}
}
```
```
}
```
if(dateCounts== 1 ){
//一个日期为单程
result=TravelTypeEnum.OneWay
}elseif(dateCounts== 2 ){
//两个日期为往返
result=TravelTypeEnum.RoundTrip
}else{
//否则啥都不是
result=TravelTypeEnum.Unknown
}
}
//操作类型为预定：无法拿到订单数据，暂时不做
if(BehaviorTypeEnum.Book==requestTypeLabel.behaviorType){
result=xmlUtil.getTravelType(requestBody)
}
result
}

/**
*对URL请求进行分类
*
*@paramhttpReferrer
*@return
*/
defclassifyByReferer(httpReferrer:String):TravelTypeEnum={
vardateCounts= 0
valregex="^(\\d{ 4 })-( 0 \\d{ 1 }| 1 [ 0 - 2 ])-( 0 \\d{ 1 }|[ 12 ]\\d{ 1 }| 3 [ 01 ])$"
if(httpReferrer.contains("?")&&httpReferrer.split("\\?").length> 1 ){
valparams=httpReferrer.split("\\?")( 1 ).split("&")
for(param<-params){
valkeyAndValue=param.split("=")
if(keyAndValue.length> 1 &&keyAndValue( 1 ).matches(regex)){
dateCounts=dateCounts+ 1
}
}


###### }

```
if(dateCounts== 1 ){
TravelTypeEnum.OneWay
}elseif(dateCounts== 2 ){
TravelTypeEnum.RoundTrip
}else{
TravelTypeEnum.Unknown
}
}
```
###### }

# 25. 解析查询数据

## 25. 1. 需求

###### 之前，我们已经对每条数据分类出了一个属于它的标签，知道此条数据是属于国内查询、

###### 国际查询、国内预定、国际预定中的某一类。

###### 每一个不同的分类的请求数据格式都是不同的，在数据库中对应了每一中请求方式的解

析规则，这个规则在web工程中是可以编辑的。
我们需要从数据库中读取所有的解析规则，根据数据打上的标签，匹配出属于这条数据
的解析规则。
根据解析规则，我们需要解析出对应的body数据，封装到对应的查询bean或者预定
bean中。

## 25. 2. 设计

```
1 、在初始化scc之后，读取kafka数据之前，从数据库中读取“查询解析规则”，存储
到广播变量中
2 、在kafka消费数据过程中，从redis中读取更新规则标识，决定是否需要更新规则
3 、用数据库的规则进行filter，过滤出对应的这条数据的解析规则
4 、通过解析规则对数据进行解析，解析的时候有很多种格式，如get提交、post提交、
提交的是form表单、提交的是json数据、提交的是xml数据等，我们需要针对不
同的数据格式制定解析规则，但在此业务中，我们只需要着重关注解析json格式的
数据。
```

## 25. 3. 代码

## 25. 3. 1. 解析数据

###### /**

* 去数据库匹配出解析规则，用解析规则解析查询中的body数据
*/
val queryRequestData = AnalyzeRequest.analyzeQueryRequest( requestTypeLabel,
requestMethod,contentType,request,requestBody,travelType,broadcastQueryRules.value)

## 25. 3. 2. 匹配规则： AnalyzeRequest.analyzeQueryRequest

objectAnalyzeRequest{
vallogger=Logger.getLogger("AnalyzeRequest")
/**
*不同类型数据，根据不同解析规则进行解析
*/
defanalyzeQueryRequest(requestTypeLabel:RequestType,
requestMethod:String,
contentType:String,
request:String,
requestBody:String,
travelType:TravelTypeEnum,
analyzeRules:List[AnalyzeRule]):Option[QueryRequestData]={
//创建用来封装query数据的类QueryRequestData
valqueryRequestData:QueryRequestData=newQueryRequestData
//数据库中有四条解析规则，我们需要通过传过来的这一条数据，确定数据匹配上的解
析规则，然后用这个解析规则解析数据
valmatchedRules=analyzeRules.filter{rule=>
//先根据请求方式和请求类型过滤出是属于查询的规则
if (rule.requestMethod.equalsIgnoreCase(requestMethod) && rule.BehaviorType ==
requestTypeLabel.behaviorType.id)
true
else
false
}.filter{rule=>
//然后根据url正则表达式，过滤出是属于国内查询还是国际查询
request.matches(rule.requestMatchExpression)
}
//如果过滤结果是大于 0 个，证明此条数据已经找到匹配的解析规则,不同类型数据根据
规则解析数据
if(matchedRules.size> 0 ){


###### //如果匹配到多个获取最后一个匹配成功的解析规则

valmatchedRule=matchedRules.last
//常规GET请求传参
if(matchedRule.requestMethod.equalsIgnoreCase("GET")&&matchedRule.isNormalGet)
{
analyseNormalGetParams(queryRequestData,request,matchedRule)
} else if (matchedRule.requestMethod.equalsIgnoreCase("POST") &&
matchedRule.isNormalForm){
analyseNormalFormParams(queryRequestData,requestBody,matchedRule)
} else if (matchedRule.requestMethod.equalsIgnoreCase("POST")
&&!matchedRule.isNormalForm
&&matchedRule.isJson&&matchedRule.formDataField.nonEmpty){
//解析post提交的、非form表单、json类型的数据
analyseUnNormalFormParams(queryRequestData,requestBody,matchedRule)
} else if (matchedRule.requestMethod.equalsIgnoreCase("POST") &&
contentType.equalsIgnoreCase("application/json")&&
matchedRule.isApplicationJson&&matchedRule.isJson&&requestBody.nonEmpty){
analyseApplicationJsonParams(queryRequestData,requestBody,matchedRule)
} else if (matchedRule.requestMethod.equalsIgnoreCase("POST") &&
contentType.equalsIgnoreCase("text/xml")&&
matchedRule.isTextXml&&matchedRule.isXML&&requestBody.nonEmpty){
analyseTextXmlParams(queryRequestData,requestBody,matchedRule)
}
}
//封装操作类型和航线类型
if(queryRequestData!=null){
queryRequestData.flightType=requestTypeLabel.behaviorType.id
queryRequestData.travelType=travelType.id
Some(queryRequestData)
}else{
None
}
}

## 25. 3. 3. 参数切割方法： analyseUnNormalFormParams

###### /**

```
*解析通过json方式提交的其他形式参数
*
*@paramqueryRequestData
*@paramrequestBody
*@parammatchedRule
*@return
*/
```

defanalyseUnNormalFormParams(queryRequestData:QueryRequestData,
requestBody:String,
matchedRule:AnalyzeRule):QueryRequestData={
try{
valparamMap=scala.collection.mutable.Map[String,String]()
//先用&分割出所有的kv对
valparams=requestBody.split("&")
//将kv对用=分割，封装到map中
for(param<-params){
valkeyAndValue=param.split("=")
if(keyAndValue.length> 1 ){
paramMap += (keyAndValue( 0 ) ->
RequestDecoder.decodePostRequest(keyAndValue( 1 )))
}
}
//获取Json字符串
valjson=paramMap.getOrElse(matchedRule.formDataField,"")
//根据解析规则提取Json字符串信息
extractGeneralJsonData(json,queryRequestData,matchedRule)
}catch{
casee:Exception=>e.printStackTrace()
logger.error("解析Form表单数据出错，请求为："+requestBody)
}
//返回数据
queryRequestData
}

## 25. 3. 4. 解析 json 数据方法： extractGeneralJsonData

###### /**

*通用查询参数Json字符串数据提取方法
*
*@paramjson
*@paramqueryRequestData
*@parammatchedRule
*/
defextractGeneralJsonData(json:String,queryRequestData:QueryRequestData,matchedRule:
AnalyzeRule):Unit={
//json串不为空
if(!json.trim.isEmpty){
//解析json串
valjsonPathParser=JsonPathParser(json)
//成人乘机人数
queryRequestData.adultNum=jsonPathParser.getValueByPath[String](


matchedRule.query_adultNum)match{
caseSome(result)=>result
caseNone=>""
}
//目的地
queryRequestData.arrCity=jsonPathParser.getValueByPath[String](
matchedRule.query_arrCity)match{
caseSome(result)=>result
caseNone=>""
}
//儿童乘机人数
queryRequestData.childNum=jsonPathParser.getValueByPath[String](
matchedRule.query_childNum)match{
caseSome(result)=>result
caseNone=>""
}
//国家
queryRequestData.country=jsonPathParser.getValueByPath[String](
matchedRule.query_country)match{
caseSome(result)=>result
caseNone=>""
}
//始发地
queryRequestData.depCity=jsonPathParser.getValueByPath[String](
matchedRule.query_depCity)match{
caseSome(result)=>result
caseNone=>""
}
//起飞时间
queryRequestData.flightDate=jsonPathParser.getValueByPath[String](
matchedRule.query_flightDate)match{
caseSome(result)=>result
caseNone=>""
}
//婴儿乘机人数
queryRequestData.infantNum=jsonPathParser.getValueByPath[String](
matchedRule.query_infantNum)match{
caseSome(result)=>result
caseNone=>""
}
//标识解析数据成功
queryRequestData.isEmpty=false
}


###### }

## 25. 3. 5. 解析 json 工具类

packagecom.air.antispider.stream.common.util.json

importcom.jayway.jsonpath.JsonPath
importcom.jayway.jsonpath.Configuration
importscala.reflect.ClassTag

/**
*@authorwangsenfeng
*
*/
classJsonPathParser(document:Object){
/**
* 根 据 json 节 点 路 径 来 获 取 对 应 的 值 ， 路 径 写 法 参 考 ：
https://github.com/json-path/JsonPath
*因为所返回的值类型根据传入的节点不同而不同，所以调用者必须知道json结构以及
明确指定返回的对象的类型
*@parampathFormat 模式
* @returnoption[TYPE]对象
*/
defgetValueByPath[TYPE:ClassTag](pathFormat:String):Option[TYPE]={
try{
Some(JsonPath.read(document,pathFormat).asInstanceOf[TYPE])
}catch{
casee:Exception=>
// e.printStackTrace()
None
}
}
}

/**
* 解析json数据
*/
objectJsonPathParser{
defapply(json:String):JsonPathParser={
try{
valdocument=Configuration.defaultConfiguration().jsonProvider().parse(json)
newJsonPathParser(document)
}catch{
casee:Exception=>


```
e.printStackTrace()
null
}
}
```
}

## 25. 3. 6. 解析其他类型数据的方法

 Get请求
/**
*解析普通GET方式提交的参数
*
*@paramqueryRequestData
*@paramrequest
*@parammatchedRule
*@return
*/
defanalyseNormalGetParams(queryRequestData:QueryRequestData,
request:String,
matchedRule:AnalyzeRule):QueryRequestData={

```
//get请求参数集合
valparamMap=scala.collection.mutable.Map[String,String]()
//获取请求中包含的所有参数
if(request.contains("?")&&request.split("\\?").length> 1 ){
valparams=request.split("\\?")( 1 ).split("&")
for(param<-params){
valkeyAndValue=param.split("=")
if(keyAndValue.length> 1 ){
paramMap+=(keyAndValue( 0 )->keyAndValue( 1 ))
}
}
}
```
getQueryRequestFromMap(queryRequestData,paramMap,matchedRule)
}
/**
*对国际航班特有单程往返进行标识
*
*@paramqueryRequestData
*@paramparamMap
*@parammatchedRule
*@return


###### */

```
defgetQueryRequestFromMap(queryRequestData:QueryRequestData,
paramMap:scala.collection.mutable.Map[String,String],
matchedRule:AnalyzeRule):QueryRequestData={
if(paramMap.nonEmpty){
//成人乘机人数
queryRequestData.adultNum=paramMap.getOrElse(matchedRule.query_adultNum,"")
//目的地
queryRequestData.arrCity=paramMap.getOrElse(matchedRule.query_arrCity,"")
//儿童乘机人数
queryRequestData.childNum=paramMap.getOrElse(matchedRule.query_childNum,"")
//国家
queryRequestData.country=paramMap.getOrElse(matchedRule.query_country,"")
//始发地
queryRequestData.depCity=paramMap.getOrElse(matchedRule.query_depCity,"")
//起飞时间
queryRequestData.flightDate=paramMap.getOrElse(matchedRule.query_flightDate,"")
//婴儿乘机人数
queryRequestData.infantNum=paramMap.getOrElse(matchedRule.query_infantNum,"")
```
//国际航班特有单程往返标识
if(!matchedRule.query_travelType.trim.isEmpty){
paramMap.get(matchedRule.query_travelType)match{
caseSome(param)=>{
if(!param.trim.isEmpty){
queryRequestData.travelType=param.toInt
}else{
queryRequestData.travelType=- 1
}
}
caseNone=>queryRequestData.travelType=- 1
}
}
//标识解析数据成功
queryRequestData.isEmpty=false
}
queryRequestData
}
 普通form表单
/**
*解析普通Form表单方式提交的参数
*(application/x-www-form-urlencoded)
*
*@paramqueryRequestData


*@paramrequestBody
*@parammatchedRule
*@return
*/
defanalyseNormalFormParams(queryRequestData:QueryRequestData,
requestBody:String,
matchedRule:AnalyzeRule):QueryRequestData={
//解析requestBody，其中中文汉字用utf- 8 编码，所以这里需要对其进行解码
valparamMap=scala.collection.mutable.Map[String,String]()
valparams=requestBody.split("&")
for(param<-params){
valkeyAndValue=param.split("=")
if(keyAndValue.length> 1 ){
paramMap += (keyAndValue( 0 ) ->
RequestDecoder.decodePostRequest(keyAndValue( 1 )))
}
}

getQueryRequestFromMap(queryRequestData,paramMap,matchedRule)
}
 Xml格式数据
/**
*对xml数据进行解析并进行标识
*
*@paramqueryRequestData
*@paramrequestBody
*@parammatchedRule
*/
def analyseTextXmlParams(queryRequestData: QueryRequestData, requestBody: String,
matchedRule:AnalyzeRule)={

//xml字符串
valxmlSubString=requestBody.substring(requestBody.indexOf("<page>"))
val xmlBuilder: DocumentBuilder =
DocumentBuilderFactory.newInstance().newDocumentBuilder()
valis=newInputSource(newStringReader(xmlSubString))
//xml对象模型
valxmlDocument:Document=xmlBuilder.parse(is)
valxpath:XPath=XPathFactory.newInstance().newXPath()

//成人乘机人数
if(!matchedRule.query_adultNum.trim.isEmpty){
queryRequestData.adultNum = xpath.evaluate(matchedRule.query_adultNum,
xmlDocument)match{


casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}
//目的地
if(!matchedRule.query_arrCity.trim.isEmpty){
queryRequestData.arrCity = xpath.evaluate(matchedRule.query_arrCity, xmlDocument)
match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}
//儿童乘机人数
if(!matchedRule.query_childNum.trim.isEmpty){
queryRequestData.childNum = xpath.evaluate(matchedRule.query_childNum,
xmlDocument)match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}
//国家
if(!matchedRule.query_country.trim.isEmpty){
queryRequestData.country=xpath.evaluate(matchedRule.query_country,xmlDocument)
match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}
//始发地
if(!matchedRule.query_depCity.trim.isEmpty){
queryRequestData.depCity=xpath.evaluate(matchedRule.query_depCity,xmlDocument)
match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}
//起飞时间
if(!matchedRule.query_flightDate.trim.isEmpty){
queryRequestData.flightDate = xpath.evaluate(matchedRule.query_flightDate,
xmlDocument)match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}


###### //婴儿乘机人数

if(!matchedRule.query_infantNum.trim.isEmpty){
queryRequestData.infantNum = xpath.evaluate(matchedRule.query_infantNum,
xmlDocument)match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value
case_=>""
}
}

//国际航班特有单程往返标识
if(!matchedRule.query_travelType.trim.isEmpty){
queryRequestData.travelType = xpath.evaluate(matchedRule.query_travelType,
xmlDocument)match{
casevalueifvalue!=null&&(!value.trim.isEmpty)=>value.toInt
case_=>- 1
}
}
//标识解析数据成功
queryRequestData.isEmpty=false

```
}
```
# 26. 解析预定数据

###### 由于无法获取订单数据，所以我们只看代码逻辑，不做测试

## 26. 1. 需求

###### 我们之前已经提取了数据的标签，只需要按照“解析查询数据”一样解析预定数据即可，

###### 同样，预定数据的格式也可能存在很多种，我们需要针对不同的数据格式匹配出对应的解析

规则，将数据解析出来，封装到bean里

## 26. 2. 设计

1 、 在初始化scc之后，读取kafka数据之前，从数据库中读取“预定解析规则”，存储到广
播变量中
2 、 在kafka消费数据过程中，从redis中读取更新规则标识，决定是否需要更新规则
3 、 用数据库的规则进行filter，过滤出对应的这条数据的解析规则
4 、 通过解析规则对数据进行解析，解析的时候有很多种格式，如提交的是json数据、提
交的是xml数据等，我们需要针对不同的数据格式制定解析规则。


## 26. 3. 代码

## 26. 3. 1. 解析数据

###### /**

* 去数据库匹配出解析规则，用解析规则解析预定中的body数据
*/
val bookRequestData = AnalyzeBookRequest.analyzeBookRequest( requestTypeLabel,
requestMethod,contentType,request,requestBody,travelType,broadcastBookRules.value)

## 26. 3. 2. 匹 配 规 则 ：

## AnalyzeBookRequest.analyzeBookRequest

packagecom.air.antispider.stream.dataprocess.businessprocess

importcom.air.antispider.stream.common.bean.{AnalyzeRule,BookRequestData,RequestType}
importcom.air.antispider.stream.common.util.decode.{MD 5 ,RequestDecoder}
importcom.air.antispider.stream.common.util.json.JsonParser
importcom.air.antispider.stream.common.util.xml.xmlUtil
importcom.air.antispider.stream.dataprocess.constants.FlightTypeEnum
importcom.air.antispider.stream.dataprocess.constants.TravelTypeEnum.TravelTypeEnum

objectAnalyzeBookRequest{
/**
*将数据进行分析，得出规则
*
*@paramrequestTypeLabel
*@paramrequestMethod
*@paramcontentType
*@paramrequest
*@paramrequestBody
*@paramtravelType
*@return
*/
defanalyzeBookRequest(requestTypeLabel:RequestType,requestMethod:String,contentType:
String, request: String, requestBody: String, travelType: TravelTypeEnum, analyzeRules:
List[AnalyzeRule]):Option[BookRequestData]={
varresultData:BookRequestData=newBookRequestData
//循环规则，匹配出符合数据的规则
valmatchedRules=analyzeRules.filter{rule=>
//匹配请求方式和请求类型- 预定


if (rule.requestMethod.equalsIgnoreCase(requestMethod) && rule.BehaviorType ==
requestTypeLabel.behaviorType.id)
true
else
false
}.filter{rule=>
request.matches(rule.requestMatchExpression)
}
//匹配出规则，解析
if(matchedRules.size> 0 ){
valmatchedRule=matchedRules.last
//国内
valnationalId=FlightTypeEnum.National.id
//国际
valinternationalId=FlightTypeEnum.International.id
//xml类型数据
if (contentType.equalsIgnoreCase("text/xml") && matchedRule.isTextXml &&
matchedRule.isXML&&requestBody.nonEmpty){
resultData=xmlUtil.parseXML(requestBody,matchedRule)
}
//json类型数据
else if (contentType.equalsIgnoreCase("application/json")
&&!matchedRule.isNormalForm&&matchedRule.isApplicationJson&&matchedRule.isJson&&
requestBody.nonEmpty){
resultData=JsonParser.parseJsonToBean(requestBody,matchedRule)
}
//json类型数据
else if (!matchedRule.isNormalForm && matchedRule.isJson
&&!matchedRule.isApplicationJson&&matchedRule.formDataField.nonEmpty){
valparamMap=scala.collection.mutable.Map[String,String]()
valparams=requestBody.split("&")
for(param<-params){
valkeyAndValue=param.split("=")
if(keyAndValue.length> 1 ){
paramMap += (keyAndValue( 0 ) ->
RequestDecoder.decodePostRequest(keyAndValue( 1 )))

}
}
varjson=paramMap.getOrElse(matchedRule.formDataField,"")
if ("[".equals(json.charAt( 0 ).toString()) && "]".equals(json.charAt(json.length() -
1 ).toString())){
json=json.substring( 1 ,json.length()- 1 )
}


```
if(json.trim.nonEmpty){
resultData=JsonParser.parseJsonToBean(json,matchedRule)
}
}
```
```
matchedRule.flightTypematch{
//国内
case`nationalId`=>{
resultData.flightType=FlightTypeEnum.National.id
resultData.travelType=travelType.id
}
//国际
case`internationalId`=>{
resultData.flightType=FlightTypeEnum.International.id
resultData.travelType=travelType.id
}
case_=>None
}
}
//resultData
if(resultData!=null){
//加密手机号，证件号。
valmd 5 =newMD 5 ()
resultData.contractPhone=md 5 .getMD 5 ofStr(resultData.contractPhone)
validCardEncrypted=scala.collection.mutable.ListBuffer[String]()
```
```
for(record<-resultData.idCard){
idCardEncrypted.append(md 5 .getMD 5 ofStr(record))
}
```
```
resultData.idCard=idCardEncrypted
```
Some(resultData)
}else{
None
}
}
}

## 26. 3. 3. 解析 xml 格式数据

###### /**

```
*解析xml文件
*
```

*@paramxmlString xml数据
*@paramanalyzeRule 规则对象
*@return 规则数据
*/
defparseXML(xmlString:String,analyzeRule:AnalyzeRule):BookRequestData={
//返回值
valanalyzedData=newBookRequestData
//xml字符串
valxmlSubString=xmlString.substring(xmlString.indexOf("<page>"))
val xmlBuilder: DocumentBuilder =
DocumentBuilderFactory.newInstance().newDocumentBuilder()
valis=newInputSource(newStringReader(xmlSubString))
//xml对象模型
valxmlDocument:Document=xmlBuilder.parse(is)
valxpath:XPath=XPathFactory.newInstance().newXPath()
//从对象模型中解析出bookUserId
if(analyzeRule.book_bookUserId!=null&&!analyzeRule.book_bookUserId.equals("")){
valbookUserId=xpath.evaluate(analyzeRule.book_bookUserId,xmlDocument)
analyzedData.bookUserId=bookUserId
}
//从对象模型中解析出bookUnUserId
if(analyzeRule.book_bookUnUserId!=null&&!analyzeRule.book_bookUnUserId.equals(""))
{
valbookUnUserId=xpath.evaluate(analyzeRule.book_bookUnUserId,xmlDocument)
analyzedData.bookUnUserId=bookUnUserId
}
//从对象模型中解析出psgName,可以存在多个值
valpsgName=parseForXpath(analyzeRule.book_psgName,xpath,xmlDocument)
if(psgName!=null&&psgName.size> 0 ){
psgName.foreach{temp=>
analyzedData.psgName+=temp
}
}
//从对象模型中解析出psgType,可以存在多个值
valpsgType=parseForXpath(analyzeRule.book_psgType,xpath,xmlDocument)
if(psgType!=null&&psgType.size> 0 ){
psgType.foreach{temp=>
analyzedData.psgType+=temp
}
}

```
//从对象模型中解析出idType,可以存在多个值
validType=parseForXpath(analyzeRule.book_idType,xpath,xmlDocument)
if(idType!=null&&idType.size> 0 ){
```

```
idType.foreach{temp=>
analyzedData.idType+=temp
}
}
```
```
//从对象模型中解析出idCard,可以存在多个值
validCard=parseForXpath(analyzeRule.book_idCard,xpath,xmlDocument)
if(idCard!=null&&idCard.size> 0 ){
idCard.foreach{temp=>
analyzedData.idCard+=temp
}
}
```
//从对象模型中解析出contractName
if(analyzeRule.book_contractName!=null&&!analyzeRule.book_contractName.equals(""))
{
valcontractName=xpath.evaluate(analyzeRule.book_contractName,xmlDocument)
analyzedData.contractName=contractName.substring( 0 ,contractName.indexOf("|"))
}
//从对象模型中解析出contractPhone
if(analyzeRule.book_contractPhone!=null&&!analyzeRule.book_contractPhone.equals(""))
{
valcontractPhone=xpath.evaluate(analyzeRule.book_contractPhone,xmlDocument)
analyzedData.contractPhone=contractPhone.substring( 0 ,contractPhone.indexOf("/"))
}

```
//从对象模型中解析出depCity,可以存在一个或两个值
valdepCity=parseForXpath(analyzeRule.book_depCity,xpath,xmlDocument)
if(depCity!=null&&depCity.size> 0 ){
depCity.foreach{temp=>
analyzedData.depCity+=temp
}
}
```
```
//从对象模型中解析出arrCity,可以存在一个或两个值
valarrCity=parseForXpath(analyzeRule.book_arrCity,xpath,xmlDocument)
if(arrCity!=null&&arrCity.size> 0 ){
arrCity.foreach{temp=>
analyzedData.arrCity+=temp
}
}
```
```
//从对象模型中解析出flightDate,可以存在一个或两个值
valflightDate=parseForXpath(analyzeRule.book_flightDate,xpath,xmlDocument)
```

```
if(flightDate!=null&&flightDate.size> 0 ){
flightDate.foreach{temp=>
analyzedData.flightDate+=temp
}
}
```
```
//从对象模型中解析出cabin,可以存在一个或两个值
valcabin=parseForXpath(analyzeRule.book_cabin,xpath,xmlDocument)
if(cabin!=null&&cabin.size> 0 ){
cabin.foreach{temp=>
analyzedData.cabin+=temp
}
}
```
```
analyzedData
}
```
## 26. 3. 4. 解析 json 格式数据

###### /**

*根据json数据解析出来的结果构建Bean对象
*
*@paramrequestBody报文
*@paramanalyzeRule 规则
*@returnAnalyzedData数据
*/
defparseJsonToBean(requestBody:String,analyzeRule:AnalyzeRule):BookRequestData={
varresultData:BookRequestData=newBookRequestData
//去掉json字符串的前后缀
valjsonStringTemp=requestBody.replace("\\x 22 ","").trim
valjsonString=JsonPathParser(jsonStringTemp)
//解析购票人ID
resultData.bookUserId = jsonString.getValueByPath[String](analyzeRule.book_bookUserId)
match{
caseSome(id)=>id
caseNone=>""
}
//解析乘机人姓
val firstName =
jsonString.getValueByPath[java.util.ArrayList[String]](analyzeRule.book_psgFirName)match{
caseSome(fn)=>fn
caseNone=>null
}
//解析乘机人名


val lastName =
jsonString.getValueByPath[java.util.ArrayList[String]](analyzeRule.book_psgName)match{
caseSome(ln)=>ln
caseNone=>null
}
//构建乘机人姓名
if(firstName!=null){
for(i<- 0 untilfirstName.size()){
valpsgName=firstName.get(i)+lastName.get(i)
resultData.psgName+=psgName
}
}
//解析证件类型
validTypes=jsonString.getValueByPath[java.util.ArrayList[String]](analyzeRule.book_idType)
match{
caseSome(temp)=>temp
caseNone=>null
}
if(idTypes!=null){
for(i<- 0 untilidTypes.size()){
resultData.idType+=idTypes.get(i)
}
}
//解析乘机人证件号
val numbers =
jsonString.getValueByPath[java.util.ArrayList[String]](analyzeRule.book_idCard)match{
caseSome(temp)=>temp
caseNone=>null
}
if(numbers!=null){
for(i<- 0 untilnumbers.size()){
resultData.idCard+=numbers.get(i)
}
}
//解析联系人姓名
jsonString.getValueByPath[String](analyzeRule.book_contractName)match{
caseSome(name)=>resultData.contractName=name
caseNone=>""
}
//解析联系人手机号
resultData.contractPhone =
jsonString.getValueByPath[String](analyzeRule.book_contractPhone)match{
caseSome(mobile)=>mobile
caseNone=>""


###### }

###### //始发地

valdepCities=jsonString.getValueByPath[String](analyzeRule.book_depCity)match{
caseSome(depCity)=>depCity
caseNone=>null
}
if(depCities!=null){
resultData.depCity+=depCities
}

//到达地
valarrCities=jsonString.getValueByPath[String](analyzeRule.book_arrCity)match{
caseSome(arrCity)=>arrCity
caseNone=>null
}
if(arrCities!=null){
resultData.arrCity+=arrCities
}

//起飞时间
valflightDates=jsonString.getValueByPath[String](analyzeRule.book_flightDate)match{
caseSome(flightDate)=>flightDate
caseNone=>null
}
if(flightDates!=null){
resultData.flightDate+=flightDates
}

//航班号
valflightNos=jsonString.getValueByPath[String](analyzeRule.book_flightNo)match{
caseSome(flightNo)=>flightNo
caseNone=>null
}
if(flightNos!=null){
resultData.flightNo+=flightNos
}

//仓位级别
valcabins=jsonString.getValueByPath[String](analyzeRule.book_cabin)match{
caseSome(cabin)=>cabin
caseNone=>null
}
if(cabins!=null){


```
resultData.cabin+=cabins
}
```
```
resultData
}
```
# 27. 课程目标

1 、数据加工：高频ip
2 、数据封装
3 、实现数据推送kafka模块
4 、实现spark任务监控模块
5 、Web端查看监控信息
6 、Web端定时任务备份redis数据

# 28. 数据加工

## 28. 1. 需求

在后续的业务中，我们会根据ip的出现排名，统计出ip的黑名单，所以，在这个业务
阶段，我们需要针对此条数据的ip做一个判断，判断它是否是高频ip（黑名单ip），并做个
标记，供后续业务使用。

## 28. 2. 设计

```
1 、在scc初始化后，加载kafka数据前，初始化加载数据库黑名单数据到广播变量
2 、在加载kafka数据后，通过redis判断是否需要更新黑名单广播变量数据
3 、通过remoteaddr和ip黑名单数据进行高频IP判断，并返回标记
```
## 28. 3. 代码

## 28. 3. 1. 判断高频 ip

###### /**

```
* 加工:判断该IP是否属于高频IP段，返回boolean标记
*/
varhighFrqIPGroup=IpOperation.isFreIP(remoteAddr,broadcastIPList.value)
```

## 28. 3. 2. IpOperation.isFreIP

packagecom.air.antispider.stream.dataprocess.businessprocess

importscala.collection.mutable.ArrayBuffer

objectIpOperation{
/**
*判断是否属于高频IP段
*redis中存储结构为Map，如：（blacklist,ip_key,ip_value）,（whitelist,ip_key,ip_value）
*@paramip ip字段
*@paramipFiledsIp黑名单
*@returnboolean值
*/
defisFreIP(ip:String,ipFileds:ArrayBuffer[String]):Boolean={
//初始化标记
varresultFlag=false
//mysql的ip黑名单已经存储在广播变量中，循环广播变量的黑名单数据
valit=ipFileds.iterator
while(it.hasNext&&!resultFlag){
valmysqlTemp=it.next()
//匹配
if(mysqlTemp.equals(ip)){
//做标记
resultFlag=true
}
}
//返回标记
resultFlag
}

}

# 29. 数据结构化

## 29. 1. 需求

将数据按照ProcessedData格式封装，其中有日期格式、集合格式数据需要转换为字符
串，后期会将数据处理后的最终结果ProcessedData发送kafka供后续rulecompute业务进行
处理


## 29. 2. 设计

###### 1 、转换日期为字符串、转换集合格式为字符串

###### 2 、封装主要参数

3 、封装ProcessedData

## 29. 3. 代码

## 29. 3. 1. 封装数据

###### /**

###### * 数据结构化

###### */

val processedData = DataPackage.dataPackage("", requestMethod, request,
remoteAddr,httpUserAgent,
timeIso 8601 ,serverAddr,highFrqIPGroup,
requestTypeLabel,travelType,cookieValue_JSESSIONID,
cookieValue_USERID,queryRequestData,bookRequestData,httpReferrer)
//返回封装好的数据
processedData

## 29. 3. 2. 数据封装方法： DataPackage.dataPackage

packagecom.air.antispider.stream.dataprocess.businessprocess

importcom.air.antispider.stream.common.bean._
importcom.air.antispider.stream.dataprocess.constants.TravelTypeEnum._

/**
*sourceData:原始kafka数据，这里不需要了，预留字段
*highFrqIPGroup：高频ip标记
*requestType：航线类型和操作类型
*travelType：航线往返类型
*queryRequestData：解析后的查询数据封装
*bookRequestData：解析后的预定数据封装
*requestTypeLabel：请求的类型，查询、预定、国内、国际
*/
objectDataPackage{
defdataPackage(sourceData:String,requestMethod:String,request:String,
remoteAddr:String,httpUserAgent:String,timeIso 8601 :String,
serverAddr:String, highFrqIPGroup:Boolean,


requestTypeLabel:RequestType,travelType:TravelTypeEnum,
cookieValue_JSESSIONID:String,cookieValue_USERID:String,
queryRequestData: Option[QueryRequestData], bookRequestData:
Option[BookRequestData],
httpReferrer:String):ProcessedData={
varflightDate=""
bookRequestDatamatch{
caseSome(book)=>flightDate=book.flightDate.mkString
caseNone=>
}
queryRequestDatamatch{
caseSome(value)=>flightDate=value.flightDate
caseNone=>
}

```
vardepCity=""
bookRequestDatamatch{
caseSome(book)=>depCity=book.depCity.mkString
caseNone=>
}
queryRequestDatamatch{
caseSome(value)=>depCity=value.depCity
caseNone=>
}
```
vararrCity=""
bookRequestDatamatch{
caseSome(book)=>arrCity=book.arrCity.mkString
caseNone=>
}
queryRequestDatamatch{
caseSome(value)=>arrCity=value.arrCity
caseNone=>
}
//主要请求参数
valrequestParams=CoreRequestParams(flightDate,depCity,arrCity)
ProcessedData("",requestMethod,request,remoteAddr,httpUserAgent,
timeIso 8601 ,serverAddr, highFrqIPGroup,
requestTypeLabel,travelType,requestParams,cookieValue_JSESSIONID,
cookieValue_USERID,queryRequestData,bookRequestData,httpReferrer)
}
}


## 29. 3. 3. 数据封装类： ProcessedData

packagecom.air.antispider.stream.common.bean

importcom.air.antispider.stream.dataprocess.constants.BehaviorTypeEnum.BehaviorTypeEnum
importcom.air.antispider.stream.dataprocess.constants.FlightTypeEnum.FlightTypeEnum
importcom.air.antispider.stream.dataprocess.constants.TravelTypeEnum.TravelTypeEnum
importcom.fasterxml.jackson.databind.ObjectMapper
importcom.fasterxml.jackson.module.scala.DefaultScalaModule

/**
* 保存请求参数的结构化数据
*sourceData：请求原始数据
*requestMethod：请求方法
*request：请求路径
*remoteAddr：客户端ip
*httpUserAgent：代理
*timeIso 8601 ：时间
*serverAddr：请求的服务器地址
*criticalCookie：cookie信息
*highFrqIPGroup：此次请求中的ip地址是否命中高频ip
*requestType：请求类型
*travelType：往返类型
*requestParams：核心请求参数，飞行时间、目的地、出发地
*cookieValue_JSESSIONID：cookie中的jessionid
*cookieValue_USERID：cookie中的userid
*queryRequestData：查询请求的form数据
*bookRequestData：预定请求的body数据
*httpReferrer：refer
*/
//保存结构化数据
caseclassProcessedData(sourceData:String,requestMethod:String,request:String,
remoteAddr:String,httpUserAgent:String,timeIso 8601 :String,
serverAddr:String,highFrqIPGroup:Boolean,
requestType:RequestType,travelType:TravelTypeEnum,
requestParams: CoreRequestParams, cookieValue_JSESSIONID:
String,cookieValue_USERID:String,
queryRequestData:Option[QueryRequestData],bookRequestData:
Option[BookRequestData],
httpReferrer:String){

```
//用null替换空数据
implicitclassStringUtils(s:String){
```

defrepEmptyStr(replacement:String="NULL"):String={
if(s.isEmpty)replacementelses
}
}

//推送到kafka的数据格式，使用#CS#分隔数据
deftoKafkaString(fieldSeparator:String="#CS#"):String={
//转换查询参数和预订参数对象为JSON
valmapper=newObjectMapper();
mapper.registerModule(DefaultScalaModule)
valqueryRequestDataStr=queryRequestDatamatch{
caseSome(value)=>
try{
mapper.writeValueAsString(value)
}catch{
case_:Throwable=>""
}
case_=>""
}
valbookRequestDataStr=bookRequestDatamatch{
caseSome(value)=>
try{
mapper.writeValueAsString(value)
}catch{
case_:Throwable=>""
}
case_=>""
}
//_ 0 - 原始数据
sourceData.repEmptyStr()+fieldSeparator+
//_ 1 - 请求类型 GET/POST
requestMethod.repEmptyStr()+fieldSeparator+
//_ 2 - 请求 [http://xxxxx](http://xxxxx)
request.repEmptyStr()+fieldSeparator+
//_ 3 - 客户端地址(IP)
remoteAddr.repEmptyStr()+fieldSeparator+
//_ 4 - 客户端浏览器(UA)
httpUserAgent.repEmptyStr()+fieldSeparator+
//_ 5 - 服务器时间的ISO 8610 格式
timeIso 8601 .repEmptyStr()+fieldSeparator+
//_ 6 - 服务器端地址
serverAddr.repEmptyStr()+fieldSeparator+
//_ 8 - 是否属于高频IP段
highFrqIPGroup+fieldSeparator+


```
//_ 9 - 航班类型-National/International/Other
requestType.flightType+fieldSeparator+
//_ 10 - 请求行为-Query/Book/Other
requestType.behaviorType+fieldSeparator+
//_ 11 - 行程类型-OneWay/RoundTrip/Unknown
travelType+fieldSeparator+
//_ 12 - 航班日期 -
requestParams.flightDate.repEmptyStr()+fieldSeparator+
//_ 13 - 始发地 -
requestParams.depcity.repEmptyStr()+fieldSeparator+
//_ 14 - 目的地 -
requestParams.arrcity.repEmptyStr()+fieldSeparator+
//_ 15 - 关键Cookie-JSESSIONID
cookieValue_JSESSIONID.repEmptyStr()+fieldSeparator+
//_ 16 - 关键Cookie- 用户ID
cookieValue_USERID.repEmptyStr()+fieldSeparator+
//_ 17 - 解析的查询参数对象JSON
queryRequestDataStr.repEmptyStr()+fieldSeparator+
//_ 18 - 解析的购票参数对象JSON
bookRequestDataStr.repEmptyStr()+fieldSeparator+
//_ 19 - 当前请求是从哪个请求跳转过来的
httpReferrer.repEmptyStr()
```
}
}

//封装请求类型：航线类别（ 0 - 国内， 1 - 国际，- 1 - 其他） 和 操作类别（ 0 - 查询， 1 - 预定，

- 1 - 其他）
caseclassRequestType(flightType:FlightTypeEnum,behaviorType:BehaviorTypeEnum)

//用于封装核心请求信息：飞行时间、目的地、出发地
caseclassCoreRequestParams(flightDate:String,depcity:String,arrcity:String)

# 30. 数据推送

## 30. 1. 需求

前面我们已经将数据进行了分类、解析、加工，并封装到了ProcessedData中，现在我
们需要将加工完的数据推送到kafka中，供后续的反爬虫计算或者其他业务使用。
由于ProcessedData中封装着两种数据类型，query数据或者book数据，我们需要在推
送的时候判断是属于query还是book，分别推送到不同的topic中。


## 30. 2. 设计

```
1 、通过操作类型behaviorType= 0 过滤ProcessedData数据中的query数据
2 、如果query数据不为空，推送到kafka的processedQuery这个topic中
3 、通过操作类型behaviorType= 1 过滤ProcessedData数据中的book数据
4 、如果book数据不为空，推送到kafka的processedBook这个topic中
```
## 30. 3. 代码

## 30. 3. 1. 推送数据

###### /**

```
*数据推送kafka
*/
//查询行为数据 forKafka
DataSend.sendQueryDataToKafka(processed)
//预订行为数据 forKafka
DataSend.sendBookDataToKafka(processed)
```
## 30. 3. 2. 推送 query 数据： DataSend.sendQueryDataToKafka

###### /**

*推送query数据
*
*@paramprocessed
*/
defsendQueryDataToKafka(processed:RDD[ProcessedData]):Unit={
//过滤出query的数据
val queryDataToKafka = processed.filter(y => y.requestType.behaviorType ==
BehaviorTypeEnum.Query).map(x=>x.toKafkaString())
//如果有查询数据，推送查询数据到Kafka
if(!queryDataToKafka.isEmpty()){
//查询数据的topic：target.query.topic=processedQuery
val queryTopic = PropertiesUtil.getStringByKey("target.query.topic",
"kafkaConfig.properties")
//创建map封装kafka参数
valprops=newjava.util.HashMap[String,Object]()
//设置brokers
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
PropertiesUtil.getStringByKey("default.brokers","kafkaConfig.properties"))
//key序列化方法


props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
PropertiesUtil.getStringByKey("default.key_serializer_class_config","kafkaConfig.properties"))
//value序列化方法
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
PropertiesUtil.getStringByKey("default.value_serializer_class_config","kafkaConfig.properties"))
//批发送设置： 32 KB作为一批次或 10 ms作为一批次
props.put(ProducerConfig.BATCH_SIZE_CONFIG,
PropertiesUtil.getStringByKey("default.batch_size_config","kafkaConfig.properties"))
props.put(ProducerConfig.LINGER_MS_CONFIG,
PropertiesUtil.getStringByKey("default.linger_ms_config","kafkaConfig.properties"))

```
//按照分区发送数据
queryDataToKafka.foreachPartition{records=>
//每个分区创建一个kafkaproducer
valproducer=newKafkaProducer[String,String](props)
//循环rdd
records.foreach{record=>
//发送数据
valpush=newProducerRecord[String,String](queryTopic,null,record)
producer.send(push)
}
//关闭流
producer.close()
}
}
}
```
## 30. 3. 3. 推送 book 数据： DataSend.sendBookDataToKafka

###### /**

*推送book数据
*
*@paramprocessed
*/
defsendBookDataToKafka(processed:RDD[ProcessedData]):Unit={
//过滤出book的数据
val bookDataToKafka = processed.filter(y => y.requestType.behaviorType ==
BehaviorTypeEnum.Book).map(x=>x.toKafkaString())
//book的topic
valbookTopic=PropertiesUtil.getStringByKey("target.book.topic","kafkaConfig.properties")
//推送预订数据到Kafka
if(!bookDataToKafka.isEmpty()){
//创建map，封装kafka参数
valprops=newjava.util.HashMap[String,Object]()


//设置brokers
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
PropertiesUtil.getStringByKey("default.brokers","kafkaConfig.properties"))
//key序列化方法
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
PropertiesUtil.getStringByKey("default.key_serializer_class_config","kafkaConfig.properties"))
//value序列化方法
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
PropertiesUtil.getStringByKey("default.value_serializer_class_config","kafkaConfig.properties"))
//批发送设置： 32 KB作为一批次或 10 ms作为一批次
props.put(ProducerConfig.BATCH_SIZE_CONFIG,
PropertiesUtil.getStringByKey("default.batch_size_config","kafkaConfig.properties"))
props.put(ProducerConfig.LINGER_MS_CONFIG,
PropertiesUtil.getStringByKey("default.linger_ms_config","kafkaConfig.properties"))
//按照分区发送数据
bookDataToKafka.foreachPartition{records=>
//每个分区创建一个kafkaproducer
valproducer=newKafkaProducer[String,String](props)
records.foreach{record=>
//发送数据
valpush=newProducerRecord[String,String](bookTopic,null,record)
producer.send(push)
}
//关闭流
producer.close()
}
}
}

# 31. 任务实时监控

## 31. 1. 需求

###### 在任务运行的过程中，我们需要对任务进行实时的监控，没处理一批数据，我们要看下

###### 处理的速度和时间，还有状态，从而能显著的跟踪任务的进行状态。

在spark中提供了一个监控功能，路径为http://localhost: 4040 /metrics/json/，数据格式
为json，作为监控数据最为合适了，样本如下


###### 数据中存储了很多的任务指标，其中有批处理的开始时间、批处理的结束时间，再加上

###### 我们计算出批处理的数据量，我们就可以得知批处理的平均计算速度，从而对任务的速度进

###### 行监控。

###### 当然，除了时间，还有很多其他的指标可供统计，请自行使用

## 31. 2. 设计

1 、解析监控json数据
2 、获取gauges节点下的数据
3 、拿到批处理的开始时间和结束时间，计算批处理花费的时间
4 、计算rdd的count
5 、计算批处理的平均计算速度
6 、将指标appid，appname、endtime、rddcount、花费时间、速度、serversCountMap封装
到map中
7 、将map存储到redis，key设计为包含时间戳的，设置超时时间
8 、将map记录到最新数据key中，设置超时时间


## 31. 3. 代码

## 31. 3. 1. 性能监控

###### /**

```
*Spark性能实时监控
*/
SparkStreamingMonitor.streamMonitor(sc,messageRDD,serversCountMap)
```
## 31. 3. 2. SparkStreamingMonitor.streamMonitor

packagecom.air.antispider.stream.dataprocess.monitor

importjava.text.SimpleDateFormat
importjava.util.Date

importcom.air.antispider.stream.common.util.jedis.{JedisConnectionUtil,PropertiesUtil}
importcom.air.antispider.stream.common.util.spark.SparkMetricsUtils
importorg.apache.spark.SparkContext
importorg.apache.spark.rdd.RDD
importorg.json 4 s.DefaultFormats
importorg.json 4 s.jackson.Json

/**
*SparkStreaming实时监控，对接数据到redis，web端会有定时任务对接数据到mysql，
供前台展示使用
*/
objectSparkStreamingMonitor{
/**
*性能监控
*
*@paramsc
*@parammessageRDD
*/
def streamMonitor(sc: SparkContext, messageRDD:
RDD[String],serversCountMap:collection.Map[String,Int]):Unit={
//记录applicationId用于监控和前端通过Yarn终止该Spark任务


valapplicationId=sc.applicationId
//应用的名字，和sc中的是一个
valapplicationUniqueName="streaming-data-peocess"
/*
//监控数据获取
val sparkDriverHost =
sc.getConf.get("spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY
_URI_BASES")
//在yarn上运行的监控数据json路径
valurl=s"${sparkDriverHost}/metrics/json"
*/
//local模式的路径
valurl="http://localhost: 4040 /metrics/json/"
//获取监控json数据
valjsonObj=SparkMetricsUtils.getMetricsJson(url)
//应用的一些监控指标在节点gauges下
valresult=jsonObj.getJSONObject("gauges")
/*
*监控信息的json路径：应用id+.driver.+应用名称+具体的监控指标名称
*/
//最近完成批次的处理"开始时间"-Unix时间戳（Unixtimestamp）-单位：毫秒
val startTimePath = applicationId + ".driver." + applicationUniqueName +
".StreamingMetrics.streaming.lastCompletedBatch_processingStartTime"
valstartValue=result.getJSONObject(startTimePath)
varprocessingStartTime:Long= 0
if(startValue!=null){
processingStartTime=startValue.getLong("value")
}

```
//最近完成批次的处理"结束时间"-Unix时间戳（Unixtimestamp）-单位：毫秒
```
//local- 1517307598019 .driver.streaming-data-peocess.StreamingMetrics.streaming.lastComplete
dBatch_processingEndTime":{"value": 1517307625168 }
val endTimePath = applicationId + ".driver." + applicationUniqueName +
".StreamingMetrics.streaming.lastCompletedBatch_processingEndTime"
valendValue=result.getJSONObject(endTimePath)
varprocessingEndTime:Long= 0
if(endValue!=null){
processingEndTime=endValue.getLong("value")
}
//最近批处理的数据行数
valsourceCount=messageRDD.count()
//最近批处理所花费的时间
valcostTime=processingEndTime-processingStartTime


//监控指标（平均计算时间）:实时处理的速度监控指标-monitorIndex需要写入Redis，
由web端读取Redis并持久化到Mysql
valcountPerMillis=sourceCount.toFloat/costTime.toFloat
//将数据封装到map中
valformat=newSimpleDateFormat("yyyy-MM-ddHH:mm:ss")
valprocessingEndTimeString=format.format(newDate(processingEndTime))
valfieldMap=scala.collection.mutable.Map(
"endTime"->processingEndTimeString,
"applicationUniqueName"->applicationUniqueName.toString,
"applicationId"->applicationId.toString,
"sourceCount"->sourceCount.toString,
"costTime"->costTime.toString,
"countPerMillis"->countPerMillis.toString,
"serversCountMap"->serversCountMap)
/*
*存储监控数据到redis
*/
try{
valjedis=JedisConnectionUtil.getJedisCluster
//监控记录有效期，单位秒
val monitorDataExpTime: Int= PropertiesUtil.getStringByKey("cluster.exptime.monitor",
"jedisConfig.properties").toInt
//产生不重复的key值
val keyName = PropertiesUtil.getStringByKey("cluster.key.monitor.dataProcess",
"jedisConfig.properties")+System.currentTimeMillis.toString
val keyNameLast = PropertiesUtil.getStringByKey("cluster.key.monitor.dataProcess",
"jedisConfig.properties")+"_LAST"
//保存监控数据
jedis.setex(keyName,monitorDataExpTime,Json(DefaultFormats).write(fieldMap))
//更新最新的监控数据
jedis.setex(keyNameLast,monitorDataExpTime,Json(DefaultFormats).write(fieldMap))
//JedisConnectionUtil.returnRes(jedis)
}catch{
casee:Exception=>
e.printStackTrace()
}
}
}


# 32. Web 页面数据对接

## 32. 1. 代码

## 32. 1. 1. Index.jsp

## 32. 1. 2. Index.js


## 32. 1. 3. commonIndex.js

## 32. 1. 4. systemMonitorIndex.jsp


## 32. 1. 5. systemMonitorIndex.js

## 32. 1. 6. IndexSystemMonitorController


## 32. 1. 7. TrafficUtil

## 32. 2. 效果

## 32. 2. 1. 实时流量转发情况

X轴为一段时间的sourceCount
Y轴为endtime

## 32. 2. 2. 各链路流量转发情况

数据为serversCountMap
X轴为某一链路的ip


Y轴为这一链路一段时间的转发数据count

## 32. 2. 3. 功能状况监控

# 33. 定时任务

```
在web端，会有定时任务将redis数据同步到mysql中，用作备份
```

## 33. 1. 流量数据

## 33. 2. 累加数据

# 34. 数据处理阶段总结

## 34. 1. 模块用例图


## 34. 2. 模块分类处理流

###### 图数据处理用例图

###### 图数据分类处理流程图


## 34. 3. 模块分析

本模块主要基于SparkStreaming进行实时流的计算和分析，其中规则配置信息以及统计
分析结果主要与Redis进行交互，因为Redis主要是基于内存操作的存储系统，同时可以将
内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。从而避免了直接访问
Mysql数据库时不断建立和销毁数据连接而产生的系统损耗。

###### 图模块分析图


## 34. 4. 模块数据分类处理

```
1 、首先将需要过滤的规则录入Mysql数据库中，过滤规则如：^.*html.*$，^.*css.*$等，
详细信息请到数据库中查询；
2 、对源数据中的敏感信息进行脱敏，使用MD 5 进行加密；
2 、其次通过正则表达式的匹配规则过滤掉无用信息；
3 、解析有用信息，获取需要统计的字段
```
#### RequestURL 请求的连接

#### RequestMethod 请求的方法

#### RemoteAddress 客户端地址

#### Requestparameter 请求参数（包括Form表单）

#### Cookie 无

#### Host 服务器地址

#### Referer 跳转来源

#### User-Agent 用户终端浏览器信息

#### Time_local 访问时间

###### 图数据分类图


## 34. 5. 数据处理：

```
1 、 按get或post的请求方式解析出请求参数；
2 、 根据请求参数中日期个数标记单程或往返；
3 、 构造格式化数据即将原始数据和统计字段使用#CS#作为相邻数据间的分隔符
合并为一条数据推送到kafka，以便后续统计分析。
4 、 模块的应用逻辑
```
###### 图数据处理图


## 34. 6. 模块时序图

## 34. 7. 实现该模块主要用到的类 / 对象及方法

```
类/对象名称 queryDB
方法 1 queryData(sql:String,field:String):java.util.List[String]
功能描述 通过c 3 p 0 线程池在mysql中查找对应的数据
```
```
类/对象名称 URLFilter
方法 1 filterURL(json:String,list:java.util.List[String]):Boolean
功能描述 过滤出待使用的URL
```
```
类/对象名称 JsonParser
方法 1 parseJson(json:String,fieldName:String):String
功能描述 解析json数据，获取key对应的value值
```
```
类/对象名称 RequestTypeClassifier
方法 1 classifyByRequest(request:String,map:java.util.Map[String,
java.util.List[String]]):RequestType
```
###### 图时序图


###### 功能描述 根据URL匹配分类规则，并打标签

```
类/对象名称 TravelTypeClassifier
方法 1 classifyByReferer(httpReferrer:String):TravelTypeEnum
功能描述 根据httpReferrer传入的参数标记为单程或往返
```
```
类/对象名称 ProcessedData
方法 1 toKafkaString(fieldSeparator:String="#CS#"):String
功能描述 将数据拼接成一个字符串推送到kafka中，以便后续处理
```
# 35. 课程目标

###### 1 、熟悉反爬虫指标

2 、从kafka对接dataprocess数据
3 、数据分割封装
4 、加载数据库规则
5 、单位时间内的IP段访问量（前两位）
6 、某个IP，单位时间内总访问量
7 、某个IP，单位时间内的关键页面访问总量
8 、某个IP，单位时间内的UA种类数统计
9 、某个IP，单位时间内的关键页面最短访问间隔
10 、 某个IP，单位时间内小于最短访问间隔（自设）的关键页面查询次数
11 、 某个IP，单位时间内查询不同行程的次数

##### 12 、 某个IP，单位时间内关键页面的访问次数的Cookie数

##### 13 、 某个IP，某个session，只对关键页面进行了访问，而没有访问其他页面和资源，

##### 也就是说它直接通过URL进行特定页面的访问

# 36. 反爬虫 Rulecompute 功能描述

## 36. 1. 模块功能

###### 反爬虫统计字段规则：

###### 总体统计:

###### 192. 168. 56. 112


###### 244. 12. 134. 56

###### 单位时间内的IP段访问量（前两位）

###### 基于IP的统计

###### 单位时间内的访问总量

###### 单位时间内的关键页面访问总量

###### 单位时间内的UA出现次数统计

###### 单位时间内的关键页面最短访问间隔

###### 单位时间内小于最短访问间隔（自设）的关键页面查询次数

单位时间内关键页面的访问次数的Cookie数少于X（自设）
单位时间内查询不同行程的次数

## 36. 2. 模块用例图


## 36. 3. 模块逻辑分析

# 37. 读取 kafka 中 queryTopic 数据

## 37. 1. 需求

在之前的dataprocess数据处理中，我们已经将数据分为query和book数据分别存到了
不同的topic中，分别为：processedQuery和processedBook。


其中processedQuery是用来计算反爬虫指标的，而processedBook是用来计算反占座指
标的，由于我们无法模拟订单数据，所以我们的重点是处理processedQuery从而计算防爬
指标。
所以，我们需要读取kafka中的processedQuery数据，读取数据的方式和之前的
dataprocess类似，只是增加了自定义存储topic的offset功能，避免数据丢失。

## 37. 2. 设计

```
1 、创建SparkContext和SparkStreamingContext
2 、加载kafka和zk的配置信息
3 、通过DirectStream的方式读取数据，并判断是否存在offset，打印测试
4 、记录offset到zookeeper
```
## 37. 3. 代码

## 37. 3. 1. 主类

packagecom.air.antispider.stream.rulecompute.antispider

importcom.air.antispider.stream.common.util.jedis.PropertiesUtil
importcom.air.antispider.stream.common.util.kafka.KafkaOffsetUtil
importcom.air.antispider.stream.common.util.log 4 j.LoggerLevels
importkafka.message.MessageAndMetadata
importkafka.serializer.StringDecoder
importorg.I 0 Itec.zkclient.ZkClient
importorg.apache.spark.sql.SQLContext
importorg.apache.spark.streaming.dstream.InputDStream
importorg.apache.spark.streaming.kafka.KafkaUtils
importorg.apache.spark.streaming.{Seconds,StreamingContext}
importorg.apache.spark.{SparkConf,SparkContext}

/**
*业务规则计算主类
*/
objectQueryLauncher{
defmain(args:Array[String]){
//设置日志级别
LoggerLevels.setStreamingLogLevels()
//当应用被停止的时候，进行如下设置可以保证当前批次执行完之后再停止应用。
System.setProperty("spark.streaming.stopGracefullyOnShutdown","true")
//创建sparkConf
val conf = new


SparkConf().setAppName("csair_streaming_rulecompute_antispider").setMaster("local[*]")
//创建sparkContext
valsc=newSparkContext(conf)
//创建sqlcontext
valsqlContext=newSQLContext(sc)
/*
加载配置文件中kafka配置
*/
val brokerList: String = PropertiesUtil.getStringByKey("default.brokers",
"kafkaConfig.properties")
//dataprocess生产的数据topic
valsourceQueryTopic:Set[String] =Set(PropertiesUtil.getStringByKey("source.query.topic",
"kafkaConfig.properties"))
valkafkaParams=Map("metadata.broker.list"->brokerList)
/*
加载zk配置
*/
valzkHosts=PropertiesUtil.getStringByKey("zkHosts","zookeeperConfig.properties")
val zkPath = PropertiesUtil.getStringByKey("rulecompute.antispider.zkPath",
"zookeeperConfig.properties")
valzkClient=newZkClient(zkHosts, 30000 , 30000 )
//调用业务处理方法
valssc=setupSsc(sc,sqlContext,sourceQueryTopic,kafkaParams,zkClient,zkHosts,zkPath)
//启动streaming程序
ssc.start()
ssc.awaitTermination()
}
/**
*对kafka中拿过来的数据进行处理，最终将黑名单数据保存到HDFS中和redis中
*
*@paramsc ：SparkContext
*@paramsqlContext：SQLContext
*@paramtopicsSet：kafka中的touple设置
*@paramkafkaParams ：kafka中metadata.broker.list设置
*@paramzkClient：zookeeperClient设置
*@paramzkHosts：zookeeper节点
*@paramzkPath：zookeeper路径
*@return
*/
defsetupSsc(sc:SparkContext,sqlContext:SQLContext,
topicsSet:Set[String],
kafkaParams:Map[String,String],
zkClient:ZkClient,zkHosts:String,
zkPath:String


):StreamingContext={
//创建streaming，每 30 秒接收数据
valssc=newStreamingContext(sc,Seconds( 5 ))
//从Kafka中读取数据
/*createdirectkafkastream*/
valmessages=createCustomDirectKafkaStream(ssc,kafkaParams,zkClient,zkHosts,zkPath,
topicsSet)
//取每条元素touple中的第二条数据
vallines=messages.map(_._ 2 )
//打印数据
lines.foreachRDD(line=>line.foreach(println))
//保存offset到zookeeper
messages.foreachRDD(rdd=>KafkaOffsetUtil.saveOffsets(zkClient,zkHosts,zkPath,rdd))
ssc
}

/**
*创建Streaming连接kafka的流
*
*@paramsscStreamingContext
*@paramkafkaParamskafkaParams
*@paramzkClientzookeeperClient
*@paramzkHostszookeeperHosts
*@paramzkPathzookeeperPath
*@paramtopicstopics
*@returnInputDStream[(String,String)]
*/
defcreateCustomDirectKafkaStream(ssc:StreamingContext,
kafkaParams:Map[String,String],
zkClient:ZkClient,
zkHosts:String,
zkPath:String,
topics:Set[String]):InputDStream[(String,String)]={
//拿出topic
valtopic=topics.last
//读取保存的offser
valstoredOffsets=KafkaOffsetUtil.readOffsets(zkClient,zkHosts,zkPath,topic)
//拿到offset去匹配
valkafkaStream=storedOffsetsmatch{
caseNone=>//如果没有数据，就从这个topic保存在zk上的offset开始读取数据
KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,
kafkaParams,topics)
caseSome(fromOffsets)=>// 如果有数据，就从记录的offset读取数据，避免数据丢
失


val messageHandler = (mmd: MessageAndMetadata[String, String]) => (mmd.key,
mmd.message)
KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, (String,
String)](ssc,kafkaParams,fromOffsets,messageHandler)
}
//返回kafka数据流
kafkaStream
}
}

## 37. 3. 2. KafkaOffsetUtil

packagecom.air.antispider.stream.common.util.kafka

importkafka.common.TopicAndPartition
importorg.I 0 Itec.zkclient.ZkClient
importorg.I 0 Itec.zkclient.exception.{ZkNoNodeException,ZkNodeExistsException}
importorg.apache.log 4 j.Logger
importorg.apache.spark.rdd.RDD
importorg.apache.spark.streaming.kafka.HasOffsetRanges
importorg.apache.zookeeper.data.Stat

/**
*/
objectKafkaOffsetUtil{
vallogger=Logger.getLogger("KafkaOffsetUtil")

```
/**
*从zookeeper指定的path路径中去读数据
*@paramclientzookeeperClient
*@parampath 路径
*@returnoption对象
*/
defreadDataMaybeNull(client:ZkClient,path:String):(Option[String],Stat)={
valstat:Stat=newStat()
valdataAndStat=try{
(Some(client.readData(path,stat)),stat)
}catch{
casee:ZkNoNodeException=>
(None,stat)
casee 2 :Throwable=>throwe 2
}
dataAndStat
}
```

###### /**

*将数据写入zookeeper中指定的路径中，如果该路径存在父目录且父目录不存在则先
创建父目录
*@paramclientzookeeperClient
*@parampath 路径
*@paramdata数据
*/
defupdatePersistentPath(client:ZkClient,path:String,data:String)={
try{
client.writeData(path,data)
}catch{
casee:ZkNoNodeException=>{
createParentPath(client,path)
try{
client.createPersistent(path,data)
}catch{
casee:ZkNodeExistsException=>
client.writeData(path,data)
casee 2 :Throwable=>throwe 2
}
}
casee 2 :Throwable=>throwe 2
}
}

```
/**
*创建父目录
*@paramclientzookeeperClient
*@parampath 路径
*/
privatedefcreateParentPath(client:ZkClient,path:String):Unit={
valparentDir=path.substring( 0 ,path.lastIndexOf('/'))
if(parentDir.length!= 0 )
client.createPersistent(parentDir,true)
}
```
```
/*
ReadthepreviouslysavedoffsetsfromZookeeper
*/
/**
*取kafka中的偏移量
*@paramzkClientzookeeperClient
*@paramzkHostszookeeperhosts节点
```

*@paramzkPath zookeeper路径
*@paramtopic kafka中的topic
*@return Option对象
*/
defreadOffsets(zkClient:ZkClient,
zkHosts:String,
zkPath:String,
topic:String):Option[Map[TopicAndPartition,Long]]={
logger.info("ReadingoffsetsfromZookeeper")
valstopwatch=newStopwatch()
val(offsetsRangesStrOpt,_)=readDataMaybeNull(zkClient,zkPath)
offsetsRangesStrOptmatch{
caseSome(offsetsRangesStr)=>
logger.info(s"Readoffsetranges:${offsetsRangesStr}")
valoffsets=offsetsRangesStr.split(",")
.map(s=>s.split(":"))
.map { case Array(partitionStr, offsetStr) => (TopicAndPartition(topic,
partitionStr.toInt)->offsetStr.toLong)}
.toMap
logger.info("DonereadingoffsetsfromZookeeper.Took"+stopwatch)
Some(offsets)
caseNone=>
logger.info("NooffsetsfoundinZookeeper.Took"+stopwatch)
None
}
}

/**
*将offest偏移量保存到zookeeper中，并打上日志
*@paramzkClientzookeeperClient
*@paramzkHosts zookeeperhosts节点
*@paramzkPath zookeeper路径
*@paramrdd 偏移量的rdd
*/
defsaveOffsets(zkClient:ZkClient,
zkHosts:String,
zkPath:String,
rdd:RDD[_]):Unit={
logger.info("SavingoffsetstoZookeeper")
valstopwatch=newStopwatch()
valoffsetsRanges=rdd.asInstanceOf[HasOffsetRanges].offsetRanges
offsetsRanges.foreach(offsetRange=>logger.debug(s"Using${offsetRange}"))
val offsetsRangesStr = offsetsRanges.map(offsetRange =>
s"${offsetRange.partition}:${offsetRange.fromOffset}")


.mkString(",")
logger.info("WritingoffsetstoZookeeperzkClient="+zkClient+" zkHosts="+zkHosts+
"zkPath="+zkPath+" offsetsRangesStr:"+offsetsRangesStr)
updatePersistentPath(zkClient,zkPath,offsetsRangesStr)
logger.info("DoneupdatingoffsetsinZookeeper.Took"+stopwatch)
}

/**
*过程时间
*/
classStopwatch{
privatevalstart=System.currentTimeMillis()
overridedeftoString()=(System.currentTimeMillis()-start)+"ms"
}
}

## 37. 3. 3. 测试

###### 1 、打开爬虫生产数据

2 、打开dataprocess进行数据处理
3 、查看rulecompute打印数据

## 37. 3. 4. Zk 上的信息

# 38. 封装加载数据

## 38. 1. 需求

从kafka上读取processedQuery数据使用#CS#分割的字符串，我们需要将其重新解封到
对应的ProcessedData中，后期使用方便


## 38. 2. 设计

1 、拿到lines进行mapPartitions，减少创建类的开销
2 、对RDD进行map，循环数据
3 、对一行数据用#CS#分割，取出所有的值封装到ProcessedData中
4 、返回数据

## 38. 3. 代码

## 38. 3. 1. 数据拆分封装

###### /**

###### * 数据加载封装

###### */

valstructuredDataLines=QueryDataPackage.queryDataLoadAndPackage(lines)

## 38. 3. 2. QueryDataPackage.queryDataLoadAndPackage

packagecom.air.antispider.stream.rulecompute.antispider.businessprocess

importcom.air.antispider.stream.common.bean._
importcom.air.antispider.stream.dataprocess.constants.TravelTypeEnum.TravelTypeEnum
import com.air.antispider.stream.dataprocess.constants.{BehaviorTypeEnum, FlightTypeEnum,
TravelTypeEnum}
importcom.fasterxml.jackson.databind.ObjectMapper
importcom.fasterxml.jackson.module.scala.DefaultScalaModule
importorg.apache.spark.streaming.dstream.DStream

/**
* 数据分割封装
*/
objectQueryDataPackage{
/**
*数据分割封装
*@paramlines
*@return
*/
defqueryDataLoadAndPackage(lines:DStream[String]):DStream[ProcessedData]={
//使用mapPartitions减少包装类的创建开销
lines.mapPartitions{partitionsIterator=>
//创建json解析


valmapper=newObjectMapper
mapper.registerModule(DefaultScalaModule)
//将数据进行map，一条条处理
partitionsIterator.map{sourceLine=>
//分割数据
valdataArray=sourceLine.split("#CS#",- 1 )
//原始数据，站位，并无数据
valsourceData=dataArray( 0 )
valrequestMethod=dataArray( 1 )
valrequest=dataArray( 2 )
valremoteAddr=dataArray( 3 )
valhttpUserAgent=dataArray( 4 )
valtimeIso 8601 =dataArray( 5 )
valserverAddr=dataArray( 6 )
valhighFrqIPGroup:Boolean=dataArray( 7 ).equalsIgnoreCase("true")
valrequestType:RequestType=RequestType(FlightTypeEnum.withName(dataArray( 8 )),
BehaviorTypeEnum.withName(dataArray( 9 )))
valtravelType:TravelTypeEnum=TravelTypeEnum.withName(dataArray( 10 ))
val requestParams: CoreRequestParams = CoreRequestParams(dataArray( 11 ),
dataArray( 12 ),dataArray( 13 ))
valcookieValue_JSESSIONID:String=dataArray( 14 )
valcookieValue_USERID:String=dataArray( 15 )
//分析查询请求的时候不需要book数据
valbookRequestData:Option[BookRequestData]=None
//封装query数据
valqueryRequestData=if(!dataArray( 16 ).equalsIgnoreCase("NULL")){
mapper.readValue(dataArray( 16 ),classOf[QueryRequestData])match{
casevalueifvalue!=null=>Some(value)
case_=>None
}
}else{
None
}
valhttpReferrer=dataArray( 18 )
//封装流程数据，返回
ProcessedData("",requestMethod,request,
remoteAddr,httpUserAgent,timeIso 8601 ,
serverAddr, highFrqIPGroup,
requestType,travelType,requestParams,
cookieValue_JSESSIONID,cookieValue_USERID,
queryRequestData,bookRequestData,httpReferrer)
}
}
}


###### }

# 39. 加载规则

## 39. 1. 需求

在之前dataprocess中，我们加载和很多的数据库规则，在rulecompute时，我们也需
要加载数据库规则，供计算指标使用，在这里我们提前加载进来
规则 1 ：关键页面；后期我们要分析关键页面的访问量，这里加载进来，并广播
规则 2 ：ip黑名单
规则 3 ：流程规则；后期我们需要根据流程规则对数据打分

## 39. 2. 设计

1 、在sc初始化后，kafka数据加载前初始化mysql数据，并广播
2 、在加载kafka数据后，我们需要根据数据库标识，决定时候需要更新广播变量

## 39. 3. 代码：关键页面

## 39. 3. 1. 广播

###### //获取查询关键页面

```
valqueryCriticalPages=AnalyzeRuleDB.queryCriticalPages()
@volatilevarbroadcastQueryCriticalPages=sc.broadcast(queryCriticalPages)
```
## 39. 3. 2. AnalyzeRuleDB.queryCriticalPages

###### /**

###### *查询关键页面

###### *

*@return
*/
defqueryCriticalPages():ArrayBuffer[String]={
val queryCriticalPagesSql = "select criticalPageMatchExpression from
nh_query_critical_pages"
valqueryCriticalPagesField="criticalPageMatchExpression"
valqueryCriticalPages=QueryDB.queryData(queryCriticalPagesSql,queryCriticalPagesField)
queryCriticalPages
}


## 39. 4. 代码：黑名单

## 39. 4. 1. 广播

//mysql中ip黑名单数据
valipInitList=AnalyzeRuleDB.queryIpListToBrocast()
@volatilevarbroadcastIPList=sc.broadcast(ipInitList)

## 39. 4. 2. AnalyzeRuleDB.queryIpListToBrocast

###### /**

```
*查询ip黑名单，添加到广播变量
*
*@return
*/
defqueryIpListToBrocast():ArrayBuffer[String]={
//mysql中ip黑名单数据
valnibsql="selectip_namefromnh_ip_blacklist"
valnibField="ip_name"
valipInitList=QueryDB.queryData(nibsql,nibField)
ipInitList
}
```

## 39. 5. 代码：流程规则

## 39. 5. 1. Web 页面设置

###### 注：这些规则是我们后期判断爬虫信息的核心指标

## 39. 5. 2. 库表

 Nh_rule

 nh_rules_maintenance_table


## 39. 5. 3. 广播

###### //获取流程规则策略配置

valflowList=AnalyzeRuleDB.createFlow( 0 )
@volatilevarbroadcastFlowList=sc.broadcast(flowList)

## 39. 5. 4. AnalyzeRuleDB.createFlow

###### /**

###### *获取流程列表

*参数n为 0 为反爬虫流程
*参数n为 1 为防占座流程
*
*@returnArrayBuffer[FlowCollocation]
*/
defcreateFlow(n:Int):ArrayBuffer[FlowCollocation]={
vararray=newArrayBuffer[FlowCollocation]
varsql:String=""
if(n == 0 ){ sql = "select
nh_process_info.id,nh_process_info.process_name,nh_strategy.crawler_blacklist_thresholds
fromnh_process_info,nh_strategywherenh_process_info.id=nh_strategy.idandstatus= 0 "}
else if(n == 1 ){sql = "select
nh_process_info.id,nh_process_info.process_name,nh_strategy.occ_blacklist_thresholds from
nh_process_info,nh_strategywherenh_process_info.id=nh_strategy.idandstatus= 1 "}

```
varconn:Connection=null
varps:PreparedStatement=null
varrs:ResultSet=null
try{
conn=c 3 p 0 Util.getConnection
ps=conn.prepareStatement(sql)
rs=ps.executeQuery()
while(rs.next()){
```

valflowId=rs.getString("id")
valflowName=rs.getString("process_name")
if(n== 0 ){
valflowLimitScore=rs.getDouble("crawler_blacklist_thresholds")
array += new FlowCollocation(flowId, flowName,createRuleList(flowId,n),
flowLimitScore,flowId)
}elseif(n== 1 ){
valflowLimitScore=rs.getDouble("occ_blacklist_thresholds")
array += new FlowCollocation(flowId, flowName,createRuleList(flowId,n),
flowLimitScore,flowId)
}

```
}
}catch{
casee:Exception=>e.printStackTrace()
}finally{
c 3 p 0 Util.close(conn,ps,rs)
}
array
}
```
## 39. 5. 5. createRuleList

###### /**

###### *获取规则列表

###### *

*@paramprocess_id 根据该ID查询规则
*@returnlist列表
*/
defcreateRuleList(process_id:String,n:Int):List[RuleCollocation]={
varlist=newListBuffer[RuleCollocation]
val sql = "select * from(select
nh_rule.id,nh_rule.process_id,nh_rules_maintenance_table.rule_real_name,nh_rule.rule_type,n
h_rule.crawler_type,"+
"nh_rule.status,nh_rule.arg 0 ,nh_rule.arg 1 ,nh_rule.score from
nh_rule,nh_rules_maintenance_tablewherenh_rules_maintenance_table."+
"rule_name=nh_rule.rule_name) as tab where process_id = '"+process_id + "'and
crawler_type="+n
//andstatus="+n
varconn:Connection=null
varps:PreparedStatement=null
varrs:ResultSet=null
try{
conn=c 3 p 0 Util.getConnection


ps=conn.prepareStatement(sql)
rs=ps.executeQuery()
while(rs.next()){
valruleId=rs.getString("id")
valflowId=rs.getString("process_id")
valruleName=rs.getString("rule_real_name")
valruleType=rs.getString("rule_type")
valruleStatus=rs.getInt("status")
valruleCrawlerType=rs.getInt("crawler_type")
valruleValue 0 =rs.getDouble("arg 0 ")
valruleValue 1 =rs.getDouble("arg 1 ")
valruleScore=rs.getInt("score")
val ruleCollocation = new
RuleCollocation(ruleId,flowId,ruleName,ruleType,ruleStatus,ruleCrawlerType,ruleValue 0 ,ruleValu
e 1 ,ruleScore)
list+=ruleCollocation
}
}catch{
casee:Exception=>e.printStackTrace()
}finally{
c 3 p 0 Util.close(conn,ps,rs)
}
list.toList
}

# 40. 5 分钟内的 IP 段（ IP 前两位）访问量

## 40. 1. 需求

###### 计算 5 分钟内IP段的访问量，IP段是指 192. 168. 56. 151 - > 192. 168 ,，计算IP段可以帮我

们监控爬虫的攻击范围，如果爬虫伪造ip，但是出口ip还是公司的网络，这样我们可以通
过ip段判断攻击ip来源于那个区域。

## 40. 2. 设计

1 、拿到kafka读取的message数据structuredDataLines，窗口长度 5 分钟，滑动长度 30 s计
算指标
2 、先判断remoteAddress是否有数据，没有数据记录（null， 1 ）
3 、有数据的话取得IP的前两位，统计为（IP段， 1 ）
4 、通过reduceByKeyAndWindow计算指标，传入窗口长度和滑动长度


## 40. 3. 代码

## 40. 3. 1. 计算指标

###### /**

###### * 计算业务指标

###### */

###### /*

###### *单位时间内的IP段访问量（前两位）

###### * 窗口长度： 5 分钟，计算 5 分钟内的数据

```
* 滑动间隔 30 s
*/
valipBlock=CoreRule.ipBlockAccessCounts(structuredDataLines,Minutes( 5 ),Seconds( 30 ))
//IP访问数据统计
varipBlockAccessCountsMap=scala.collection.Map[String,Int]()
//收集数据，供后面计算使用
ipBlock.foreachRDD{ipBlock=>
ipBlockAccessCountsMap=ipBlock.collectAsMap()
}
//测试
ipBlock.foreachRDD(x=>x.foreach(println))
```
## 40. 3. 2. CoreRule.ipBlockAccessCounts

packagecom.air.antispider.stream.rulecompute.antispider.businessprocess

importcom.air.antispider.stream.common.bean.ProcessedData
importorg.apache.spark.streaming.Duration
importorg.apache.spark.streaming.dstream.DStream

importscala.collection.mutable.ArrayBuffer

/**
* 反爬核心计算规则函数集
*/
objectCoreRule{

```
/**
*单位时间内的IP段访问量（前两位）
*
*@paramstructuredDataLines结构化的查询数据
*@paramwindowDuration 窗口长度
```

*@paramslideDuration 窗口滑动周期
*@returnDStream[(String,Int)]
*/
defipBlockAccessCounts(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration):DStream[(String,Int)]={
structuredDataLines.map{processedData=>
//remoteaddr没数据
if("NULL".equalsIgnoreCase(processedData.remoteAddr)){
(processedData.remoteAddr, 1 )
}else{
//remoteaddr有数据,如 192. 168. 56. 151 第一个“.”出现的位置
valindex=processedData.remoteAddr.indexOf(".")
try{
/*
*processedData.remoteAddr.indexOf(".",index+ 1 )：找“.”，从index+ 1 位置查
找，也就是找第二个“.”的位置
*计算结果如：( 192. 168 , 1 )
*/
(processedData.remoteAddr.substring( 0 , processedData.remoteAddr.indexOf(".",
index+ 1 )), 1 )
}catch{
casee:Exception=>
("", 1 )
}
}
//叠加 5 分钟的数据
}.reduceByKeyAndWindow((a:Int,b:Int)=>(a+b),windowDuration,slideDuration)
}
}

# 41. 某个 IP ， 5 分钟内总访问量

## 41. 1. 需求

###### 计算某个IP在 5 分钟的访问量，如果后期超过我们设置的阈值，可以定为爬虫，设置

###### 到黑名单

## 41. 2. 设计

1 、拿到kafka数据


2 、统计remoteaddress的wordcount

## 41. 3. 代码

## 41. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内总访问量

###### */

```
valip=CoreRule.ipAccessCounts(structuredDataLines,Minutes( 5 ),Seconds( 30 ))
varipAccessCountsMap=scala.collection.Map[String,Int]()
ip.foreachRDD{ipRdd=>
ipAccessCountsMap=ipRdd.collectAsMap()
}
```
## 41. 3. 2. CoreRule.ipAccessCounts

###### /**

###### *某个IP，单位时间内总访问量

###### *

```
*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
*@returnDStream[(String,Int)]
*/
defipAccessCounts(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration):DStream[(String,Int)]={
structuredDataLines.map{processedData=>
(processedData.remoteAddr, 1 )
}.reduceByKeyAndWindow((a:Int,b:Int)=>(a+b),windowDuration,slideDuration)
}
```
# 42. 某个 IP ， 5 分钟内的关键页面访问总量

## 42. 1. 需求

###### 什么是关键页面？

###### 在主网站中，我们除了可以查询、预定车票以外，还会有很多查询请求，例如：服务大


厅、实名认证 ，这些请求的后缀并不是我们过滤html、css、js等文件，

而是和查询请求一样的ao文件，这些请求对于我们的业务并没有用，但是我们并不能将他
们过滤掉，所以，在这里我们要对关键页面（查询、预定）进行统计，而通过筛选关键页面
的统计量，就能很容易的识别出爬虫信息，因为普通用户不会在短时间内大量的访问这些关
键页面的。
针对数据库关键页面规则，我们想知道那些ip对关键页面的访问量特别多，因为爬虫
是针对特殊的页面进行爬去，这样我们就可以过滤出爬虫。

## 42. 2. 设计

1 、拿到kafka消息和数据库关键页面广播变量的value
2 、从kafka数据中拿到request，匹配关键页面
3 、匹配成功记录（remoteaddr， 1 ）
4 、匹配失败记录（remoteaddr， 0 ）
5 、统计最终结果

## 42. 3. 代码

## 42. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内的关键页面访问总量

###### */

val criticalPages = CoreRule.criticalPagesCounts(structuredDataLines, Minutes( 5 ),
Seconds( 30 ),broadcastQueryCriticalPages.value)
varcriticalPagesCountsMap=scala.collection.Map[String,Int]()
criticalPages.foreachRDD{criticalPageRdd=>
criticalPagesCountsMap=criticalPageRdd.collectAsMap()
}
//测试
criticalPages.foreachRDD(x=>x.foreach(println))

## 42. 3. 2. CoreRule.criticalPagesCounts

###### /**

###### *某个IP，单位时间内的关键页面访问总量

###### *

```
*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
```

```
*@returnDStream[(String,Int)]
*/
defcriticalPagesCounts(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration,
queryCriticalPages:ArrayBuffer[String]):DStream[(String,Int)]={
structuredDataLines.map{processedData=>
//从request中拿到访问页面
valrequest=processedData.request
//判断页面是否为关键页面
varflag=false
for(page<-queryCriticalPages){
if(request.matches(page)){
flag=true
}
}
//如果是关键页面，记录这个ip为 1
if(flag){
(processedData.remoteAddr, 1 )
}else{
//如果不是关键页面，记录为 0
(processedData.remoteAddr, 0 )
}
//统计
}.reduceByKeyAndWindow((a:Int,b:Int)=>(a+b),windowDuration,slideDuration)
}
```
# 43. 某个 IP ， 5 分钟内的 UA 种类数统计

## 1. 1. 需求

什么是User-Agent？
User-Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识
别客户使用的操作系统及版本、CPU类型、浏览器及版本、浏览器渲染引擎、浏览器语言、
浏览器插件等。一些网站常常通过判断 UA来给不同的操作系统、不同的浏览器发送不同
的页面，因此可能造成某些页面无法在某个浏览器中正常显示，但通过伪装 UA可以绕过
检测。
正常用户（ip）在短时间内是不会切换操作系统和浏览器的，但是，爬虫可能通过伪装
User-Agent来逃过反爬虫系统的检测，所以我们要查看某一个ip在 5 分钟内是否频繁切换
了User-Agent，这可能是伪装的爬虫


## 1. 2. 设计

```
1 、拿到kafka数据、窗口时间、滑动时间进行统计
2 、以remoteaddr为key，User-Agent为value，统计为tuple，并将结果汇总，格式为
( 192. 168. 56. 1 ,ArrayBuffer(ua 1 ，ua 2 ，......))
3 、将上面的结果集进行map，然后将ua进行去重统计个数，以remoteaddr为key统
计ua个数，格式为：（remoteaddr，uaCount）
4 、将统计结果收集到hashmap中
```
## 1. 3. 代码

## 1. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内的UA种类数统计

###### */

```
valuserAgent=CoreRule.userAgent(structuredDataLines,Minutes( 5 ),Seconds( 30 ))
varuserAgentCountsMap=scala.collection.Map[String,Int]()
userAgent.map{userAgents=>
valagents=userAgents._ 2
(userAgents._ 1 ,RuleUtil.differentAgents(agents))
}.foreachRDD{userAgentRdd=>
userAgentCountsMap=userAgentRdd.collectAsMap()
//测试
println(userAgentCountsMap)
userAgentCountsMap
}
//测试
userAgent.foreachRDD(x=>x.foreach(println))
```
## 1. 3. 2 .CoreRule.userAgent

###### /**

###### *某个IP，单位时间内的UA种类数统计

###### *

```
*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
*@returnDStream[(String,Iterable[String])]
*/
defuserAgent(structuredDataLines:DStream[ProcessedData],
```

```
windowDuration:Duration,
slideDuration:Duration):DStream[(String,Iterable[String])]={
structuredDataLines.map{processedData=>
(processedData.remoteAddr,processedData.httpUserAgent)
}.groupByKeyAndWindow(windowDuration,slideDuration)
}
```
## 1. 3. 3 .RuleUtil.differentAgents

###### /**

###### *不同UA种类

###### *

```
*@paramagentsIterable
*@return 集合长度
*/
defdifferentAgents(agents:Iterable[String]):Int={
valset=mutable.Set[String]()
for(agent<-agents){
set.add(agent)
}
set.size
}
```
# 44. 某个 IP ， 5 分钟内的关键页面最短访问

# 间隔

## 44. 1. 需求

我们想看看某个remoteaddr在关键页面上的最小访问时间。
如果是一个普通用户，他两次访问关键页面的时间间隔应该不是很短的，因为他查询完
票的信息要看看合不合适，才会进行下一次查询；
但是，如果是爬虫，那么就会不断的爬取关键页面的数据信息，甚至攻击关键页面，这
样，爬虫对于关键页面的两次查询间隔时间会很短，甚至为 0 。
而我们需要计算每两次查询的时间间隔，并且在一段时间，这个间隔时间最小是多少，
从而衡量一个用户时候为爬虫用户。

## 44. 2. 设计

1 、拿到kafka数据，关键页面的广播变量，窗口时间，滑动时间进行计算


2 、从kafka数据中取到request，匹配关键页面
3 、匹配成功，记录（remoteaddr，timeIso 8601 ），并做groupByKeyAndWindow操作，将相
同remoteaddr的timeIso 8601 统计到ArrayBuffer中
4 、拿到上面的数据进行map，循环出每一个remoteaddr和他的timeIso 8601 们
5 、对timeIso 8601 们进行循环计算，计算临近的两个时间差，封装到ArrayBuffer中
6 、在将remoteaddr和timeIso 8601 的最小值封装到tuple中
7 、最后将（remoteaddr，timeIso 8601 最小值）收集到map中

## 44. 3. 代码

## 44. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内的关键页面最短访问间隔

###### */

valcriticalPagesAccTime=CoreRule.criticalPagesAccTime(structuredDataLines,Seconds( 30 ),
Seconds( 30 ),broadcastQueryCriticalPages.value)
varcriticalMinIntervalMap=scala.collection.Map[String,Int]()
criticalPagesAccTime.map{record=>
valaccTimes=record._ 2
//计算时间差
valintervals=RuleUtil.calculateIntervals(accTimes)
//将remoteaddr和时间差最小值封装到tuple
(record._ 1 ,RuleUtil.minInterval(intervals))
}.foreachRDD{critivalMinIntervalRdd=>
//收集数据
criticalMinIntervalMap=critivalMinIntervalRdd.collectAsMap()
//测试
println(criticalMinIntervalMap)
criticalMinIntervalMap
}
//测试
criticalPagesAccTime.foreachRDD(x=>x.foreach(println))

## 44. 3. 2. CoreRule.criticalPagesAccTime

###### /**

###### *某个IP，单位时间内的关键页面最短访问间隔

###### *

```
*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
```

*@returnDStream[(String,Iterable[String])]
*/
defcriticalPagesAccTime(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration,
queryCriticalPages: ArrayBuffer[String]): DStream[(String,
Iterable[String])]={
structuredDataLines.map{processedData=>
valaccessTime=processedData.timeIso 8601
valrequest=processedData.request

```
varflag=false
for(i<- 0 untilqueryCriticalPages.size){
if(request.matches(queryCriticalPages(i))){
flag=true
}
}
```
```
if(flag){
(processedData.remoteAddr,accessTime)
}else{
(processedData.remoteAddr," 0 ")
}
}.groupByKeyAndWindow(windowDuration,slideDuration)
}
```
## 44. 3. 3. RuleUtil.calculateIntervals

###### /**

###### *计算关键页面访问时间间隔，单位秒

###### *

```
*@paramaccTimes时间Iterable
*@returnlist集合
*/
defcalculateIntervals(accTimes:Iterable[String]):java.util.List[Int]={
valtimeList:java.util.List[String]=newjava.util.ArrayList[String]
//格式化时间
for(time<-accTimes){
if(!" 0 ".equalsIgnoreCase(time)){
try{
timeList.add(time.substring( 0 ,time.indexOf("+")).replace("T",""))
}catch{
casee:Exception=>
e.printStackTrace()
```

###### }

###### }

###### }

## 44. 3. 4. RuleUtil.minInterval

###### /**

###### *计算最短访问间隔

###### *

```
*@paramintervals集合
*@return 最小的值
*/
defminInterval(intervals:java.util.List[Int]):Int={
varminInterval= 0
if(intervals.size()>= 2 ){
minInterval=intervals.get( 0 )
for(i<- 1 untilintervals.size()){
if(minInterval>intervals.get(i)){
minInterval=intervals.get(i)
}
}
}
minInterval
}
```
# 45. 某个 IP ， 5 分钟内小于最短访问间隔（自

# 设）的关键页面查询次数

## 45. 1. 需求

###### 我们之前统计了 5 分钟内关键页面的最短访问时间，现在，我们想看看，假如我们设置

###### 了一个自定义的时间，比如说是十秒，我们想知道爬虫对关键页面访问时间小于我们预设值

###### 的次数是多少，也就是说，我们需要对每两次关键页面访问做时间差，只要是小于我们预设

###### 值的，我们就记录叠加，最终看看总和是多少。

###### 这个指标可以很灵活的区分爬虫和普通用户，因为我们可以假设我们进行了一次车票查

###### 询，但是不满意，又重新查询了车票，但是这两个时间之间是有用户思考和分析时间的，我

们可以给这个时间预设一个我们觉得合理的值，如果一个remoteaddr的访问总是大于这个
预设值的时候，或者是remoteaddr只有特殊的几次访问是小于这个预设值的，那么我们可
以认定这是一个普通用户，如果一个remoteaddr大部分访问都是大于这个预设值的，那么
他可能就疑似是爬虫了！


## 45. 2. 设计

1 、 拿到kafka数据，关键页面的广播变量，窗口时间，滑动时间进行计算
2 、 从kafka数据中取到request，匹配关键页面
3 、 匹配成功，记录（（remoteAddr,request），timeIso 8601 ），并做groupByKeyAndWindow
操作，将相同（remoteAddr,request）的timeIso 8601 统计到ArrayBuffer中
4 、 拿到上面的数据进行map，循环出每一个（remoteAddr,request）和他的timeIso 8601
们
5 、 对timeIso 8601 们进行循环计算，计算临近的两个时间差，封装到ArrayBuffer中
6 、 对上面的arraybuffer进行迭代判断，大于预设值的，加到count中，并封装成
（（remoteAddr,request），count），也就是某个用户对某个关键页面访问时间小于预设值的
访问次数
7 、 最后将（（remoteAddr,request），count）收集到map中

## 45. 3. 代码

## 45. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内小于最短访问间隔（自设）的关键页面查询次数

###### */

val aCriticalPagesAccTime = CoreRule.aCriticalPagesAccTime(structuredDataLines,
Minutes( 5 ),Seconds( 30 ),broadcastQueryCriticalPages.value)
varaccessIntervalLessThanDefaultMap=scala.collection.Map[(String,String),Int]()
aCriticalPagesAccTime.map{record=>
valaccTimes=record._ 2
valintervals=RuleUtil.calculateIntervals(accTimes)
(record._ 1 ,RuleUtil.intervalsLessThanDefault(intervals))
}.foreachRDD{rdd=>
accessIntervalLessThanDefaultMap=rdd.collectAsMap()
//测试
println(accessIntervalLessThanDefaultMap)
accessIntervalLessThanDefaultMap
}
//测试
aCriticalPagesAccTime.foreachRDD(x=>x.foreach(println))

## 45. 3. 2. CoreRule.aCriticalPagesAccTime

###### /**

###### *某个IP，单位时间内小于最短访问间隔（自设）的关键页面查询次数


###### *

*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
*@returnDStream[((String,String),Iterable[String])]
*/
defaCriticalPagesAccTime(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration,
queryCriticalPages: ArrayBuffer[String]): DStream[((String,
String),Iterable[String])]={
structuredDataLines.map{processedData=>
valaccessTime=processedData.timeIso 8601
valrequest=processedData.request

```
varflag=false
for(i<- 0 untilqueryCriticalPages.size){
if(request.matches(queryCriticalPages(i))){
flag=true
}
}
```
```
if(flag){
((processedData.remoteAddr,request),accessTime)
```
```
}else{
((processedData.remoteAddr,request)," 0 ")
}
}.groupByKeyAndWindow(windowDuration,slideDuration)
}
```
## 45. 3. 3. RuleUtil.calculateIntervals

###### /**

###### *计算关键页面访问时间间隔，单位秒

###### *

```
*@paramaccTimes时间Iterable
*@returnlist集合
*/
defcalculateIntervals(accTimes:Iterable[String]):java.util.List[Int]={
valtimeList:java.util.List[String]=newjava.util.ArrayList[String]
//格式化时间
for(time<-accTimes){
if(!" 0 ".equalsIgnoreCase(time)){
```

```
try{
timeList.add(time.substring( 0 ,time.indexOf("+")).replace("T",""))
}catch{
casee:Exception=>
e.printStackTrace()
}
}
}
//对时间排序
Collections.sort(timeList)
```
```
valintervals:java.util.List[Int]=newjava.util.ArrayList[Int]
valsdf=newSimpleDateFormat("yyyy-MM-ddHH:mm:ss")
//循环时间
if(timeList.size()> 1 ){
for(i<- 1 untiltimeList.size()){
//获取间隔时间
valtime 1 =sdf.parse(timeList.get(i- 1 )).getTime
valtime 2 =sdf.parse(timeList.get(i)).getTime
//计算时间差，毫秒值
valinterval=(time 2 - time 1 )/ 1000
//将时间差封装到list中
intervals.add(interval.toInt)
}
}
intervals
}
```
## 45. 3. 4. RuleUtil.intervalsLessThanDefault

###### /**

###### *小于最短访问间隔（自设）的关键页面查询次数

###### *

```
*@paramintervals集合
*@return 访问次数
*/
defintervalsLessThanDefault(intervals:java.util.List[Int]):Int={
//单位时间内最短访问间隔（自设）
valdefaultMinInterval= 10
varaccessCount= 0
if(intervals!=null&&intervals.size()> 0 ){
for(i<- 0 untilintervals.size()){
if(defaultMinInterval>intervals.get(i)){
accessCount=accessCount+ 1
```

###### }

###### }

###### }

```
accessCount
}
```
# 46. 某个 IP ， 5 分钟内查询不同行程的次数

## 46. 1. 需求

###### 那我们知道，如果我们是正常的用户，我们去查询车票的时候，我们可能会选择几趟不

###### 同的车次或者不同的时间进行对比参考，但是我们的行程应该是固定的，假如我想从北京去

###### 上海，那么我可能查询很多车次和时间，但是行程永远是北京到上海，当然，也可能出现特

###### 殊的情况，但肯定是少数。

###### 那对于爬虫呢？他会短时间内大量的爬取不同行程、不同车次、不同时间，这显然和普

###### 通用户是有区别的，我们通过这个维度的指标也能计算出爬虫的特征。

## 46. 2. 设计

1 、 拿到kafka数据，窗口时间，滑动时间进行计算
2 、 取 出 remoteaddr， 出 发 城 市 、 到 达 城 市 ， 进 行 封 装 (remoteAddr,
(requestParams.depcity,requestParams.arrcity))，并进行groupByKeyAndWindow操作，将
当前remoteaddr的所有查询行程封装到buffer中
3 、 将出发城市和到达城市转换成字符串“depcity—>arrcity”,然后进行去重操作、count
操作，得到 5 分钟内remoteaddr查询了多少个不同城市
4 、 将（remoteaddr，count）封装，记得到最终结果，并收集到map中

## 46. 3. 代码

## 46. 3. 1. 计算指标

###### /*

###### *某个IP，单位时间内查询不同行程的次数

###### */

```
valflightQuery=CoreRule.flightQuerys(structuredDataLines,Minutes( 5 ),Seconds( 30 ))
//统计一个IP下查询不同行程的次数
vardifferentTripQuerysMap=scala.collection.Map[String,Int]()
flightQuery.map{record=>
valquerys=record._ 2
(record._ 1 ,RuleUtil.calculateDifferentTripQuerys(querys))
```

}.foreachRDD{rdd=>
differentTripQuerysMap=rdd.collectAsMap()
//测试
println(differentTripQuerysMap)
}
//测试
flightQuery.foreachRDD(x=>x.foreach(println))

## 46. 3. 2. CoreRule.flightQuerys

###### /**

###### *某个IP，单位时间内查询不同行程的次数

###### *

*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
*@returnDStream[(String,Iterable[(String,String)])]
*/
defflightQuerys(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration):DStream[(String,Iterable[(String,String)])]={
structuredDataLines.map{processedData=>
(processedData.remoteAddr, (processedData.requestParams.depcity,
processedData.requestParams.arrcity))
}.groupByKeyAndWindow(windowDuration,slideDuration)
}

## 46. 3. 3. RuleUtil.calculateDifferentTripQuerys

###### /**

###### *统计一个IP下查询不同行程的次数

###### *

```
*@paramquerys Iterable
*@return 集合长度
*/
defcalculateDifferentTripQuerys(querys:Iterable[(String,String)]):Int={
vallist:java.util.List[String]=newjava.util.ArrayList[String]
for(query<-querys){
list.add(query._ 1 +"-->"+query._ 2 )
}
valhashSet=newHashSet(list)
list.clear()
list.addAll(hashSet)
```

```
list.size()
}
```
# 47. 某个 IP ， 5 分钟内关键页面的访问次数

# 的 Cookie 数

## 1. 1. 需求

Cookie是一个很好的衡量是否是爬虫的指标，cookie中有个JSESSIONID可以用来区别每
一个cookie，我们只要计算出 5 分钟内所有访问过关键页面的cookie，然后将每个cookie
的JSESSIONID取出来进行去重汇总计算，就可以得出最后指标。

## 1. 2. 设计

1 、 拿到kafka数据，关键页面的广播变量，窗口时间，滑动时间进行计算
2 、 从kafka数据中取到request，匹配关键页面
3 、 匹配成功，记录（remoteaddr，JSESSIONID），并做groupByKeyAndWindow操作，将相
同remoteaddr的JSESSIONID统计到ArrayBuffer中
4 、 拿到上面的数据进行去重count，最终封装数据（remoteaddr，JSESSIONIDCount）
5 、 最后将（remoteaddr，JSESSIONIDCount最小值）收集到map中

## 1. 3. 代码

## 1. 3. 1. 计算指标

###### /*

*某个IP，单位时间内关键页面的访问次数的Cookie数
*/
valcriticalCookie=CoreRule.criticalCookies(structuredDataLines,Minutes( 5 ),Seconds( 30 ),
broadcastQueryCriticalPages.value)
//统计一个IP下不同Cookie数
varcriticalCookiesMap=scala.collection.Map[String,Int]()
criticalCookie.map{record=>
valcookies=record._ 2
(record._ 1 ,RuleUtil.cookiesCounts(cookies))
}.foreachRDD{rdd=>
criticalCookiesMap=rdd.collectAsMap()
//测试


println(criticalCookiesMap)
}
//测试
criticalCookie.foreachRDD(x=>x.foreach(println))

## 1. 3. 2. CoreRule.criticalCookies

###### /**

*某个IP，关键页面访问Cookie
*
*@paramstructuredDataLinesProcessedData数据
*@paramwindowDuration 窗口长度
*@paramslideDuration 窗口滑动周期
*@returnDStream[((String,String),Iterable[String])]
*/
defcriticalCookies(structuredDataLines:DStream[ProcessedData],
windowDuration:Duration,
slideDuration:Duration,
queryCriticalPages: ArrayBuffer[String]): DStream[(String,
Iterable[String])]={
structuredDataLines.map{processedData=>
valrequest=processedData.request

```
varflag=false
for(i<- 0 untilqueryCriticalPages.size){
if(request.matches(queryCriticalPages(i))){
flag=true
}
}
```
```
if(flag){
(processedData.remoteAddr,processedData.cookieValue_JSESSIONID)
}else{
(processedData.remoteAddr,"")
}
}.groupByKeyAndWindow(windowDuration,slideDuration)
}
```
## 1. 3. 3. RuleUtil.cookiesCounts

###### /**

```
*不同种类的cookies
*
```

```
*@paramcookiescookies
*@return 集合长度
*/
defcookiesCounts(cookies:Iterable[String]):Int={
vallist:java.util.List[String]=newjava.util.ArrayList[String]
for(cookie<-cookies){
if(!"".equals(cookie)){
list.add(cookie)
}
}
valhashSet=newHashSet(list)
list.clear()
list.addAll(hashSet)
list.size()
}
```
# 48. 课程目标

##### 1 、根据各流程进行规则匹配和打分并进行阈值判断

###### 2 、剔除非黑名单数据

###### 3 、黑名单数据去重

4 、黑名单redis数据恢复
5 、黑名单数据备份到redis
6 、黑名单数据实时存储hdfs，用于redis数据恢复
7 、存储规则计算结果RDD到HDFS

# 49. Web 端创建流程

## 49. 1. 创建流程



## 49. 2. 代码

# 50. 根据各流程进行规则匹配和打分并进

# 行阈值判断

## 50. 1. 需求

之前我们已经通过streaming程序计算好了 5 分钟内的各个指标，接下来，我们需要针
对每一次请求，看看他们在指标中对应的值，例如我们统计了ip段的访问次数，我们要看
看请求过来的ip（例如是 192. 168. 56. 151 ）在 5 分钟内ip段访问量是多少。
然后，我们拿出数据库中的规定的ip段访问量不允许超过的阈值，和刚才那个ip的阈
值进行对比，如果超过了，那么我们就把数据库中的score记录，没超过就打分为 0.
针对每一个指标，我们都会打一个score，然后我们根据一个规定好的打分策略（最高


分权重 60 %，开启率权重 40 %）来为我们这条数据打分，打分区间为：score— 10 *score，然
后我们在根据数据库中配置的爬虫阈值，看看这条数据是否命中！

## 50. 2. 设计

1 、拿到封装好的kafka的数据structuredDataLines进行map操作，这样会得到每一条访问
数据
2 、然后用数据库流程策略、ip、request、和所有收集的map进行匹配打分
3 、针对一条数据，从map中取出所有匹配的streaming的计算结果，封装到一个map中
4 、拿到这条数据封装的map和数据库流程策略进行打分
5 、最后将所有打分结果封装到AntiCalculateResult中返回

## 50. 3. 代码

## 50. 3. 1. 调用方法

###### /**

###### * 根据各流程进行规则匹配和打分并进行阈值判断

###### */

valantiCalculateResults=structuredDataLines.map{processedData=>
//获取ip和request，从而可以从上面的计算结果Map中得到这条记录对应的 5 分钟
内总量，从而匹配数据库流程规则
valip=processedData.remoteAddr
valrequest=processedData.request
//反爬虫结果
val antiCalculateResult = RuleUtil.calculateAntiResult(processedData,
broadcastFlowList.value.toArray,ip,request,
ipBlockAccessCountsMap, ipAccessCountsMap, criticalPagesCountsMap,
userAgentCountsMap,
criticalMinIntervalMap,accessIntervalLessThanDefaultMap,differentTripQuerysMap,
criticalCookiesMap)

```
antiCalculateResult
}
```
## 50. 3. 2. 计算发爬虫结果 RuleUtil.calculateAntiResult

###### /**

###### *取到每条记录对应的统计结果

###### *

```
*@paramprocessedDataprocessedData
```

*@paramFlowCollocations 流程Collocations
*@paramipip
*@paramrequest请求
*@paramipBlockAccessCountsMapIP段访问量
*@paramipAccessCountsMap 单位时间内某个IP访问量
*@paramcriticalPagesCountsMap 单位时间内的关键页面访问总量
*@paramuserAgentCountsMap 单位时间内的UA种类数统计
*@paramcriticalMinIntervalMap 单位时间内的关键页面最短访问间隔
*@paramaccessIntervalLessThanDefaultMap单位时间内小于最短访问间隔（自设）的关
键页面查询次数
*@paramdifferentTripQuerysMap 单位时间内查询不同行程的次数
*@paramcriticalCookiesMap 单位时间内关键页面的Cookie数
*@returnAntiCalculateResult流程计算结果
*/
defcalculateAntiResult(processedData:ProcessedData,
FlowCollocations:Array[FlowCollocation],
ip:String,request:String,
ipBlockAccessCountsMap:scala.collection.Map[String,Int],
ipAccessCountsMap:scala.collection.Map[String,Int],
criticalPagesCountsMap:scala.collection.Map[String,Int],
userAgentCountsMap:scala.collection.Map[String,Int],
criticalMinIntervalMap:scala.collection.Map[String,Int],
accessIntervalLessThanDefaultMap: scala.collection.Map[(String,
String),Int],
differentTripQuerysMap:scala.collection.Map[String,Int],
criticalCookiesMap:scala.collection.Map[String,Int],
):AntiCalculateResult={

```
//当前处理这个ip的段
valindex=ip.indexOf(".")
valipBlock=try{
ip.substring( 0 ,ip.indexOf(".",index+ 1 ))
}catch{
casee:Exception=>""
}
//IP段访问量
valipBlockCounts=ipBlockAccessCountsMap.getOrElse(ipBlock, 0 )
```
```
//这条记录对应的单位时间访问量
valipAccessCounts=ipAccessCountsMap.getOrElse(ip, 0 )
```
```
//这条记录对应的单位时间内的关键页面访问总量
valcriticalPageAccessCounts=criticalPagesCountsMap.getOrElse(ip, 0 )
```

###### //这条记录对应的单位时间内的UA种类数统计

```
valuserAgentCounts=userAgentCountsMap.getOrElse(ip, 0 )
```
```
//这条记录对应的单位时间内的关键页面最短访问间隔
valcritivalPageMinInterval=criticalMinIntervalMap.getOrElse(ip, 0 )
```
//这条记录对应的单位时间内小于最短访问间隔（自设）的关键页面查询次数
val accessPageIntervalLessThanDefault = accessIntervalLessThanDefaultMap.getOrElse((ip,
request), 0 )

```
//这条记录对应的单位时间内查询不同行程的次数
valdifferentTripQuerysCounts=differentTripQuerysMap.getOrElse(ip, 0 )
```
```
//这条记录对应的单位时间内关键页面的Cookie数
valcriticalCookies=criticalCookiesMap.getOrElse(ip, 0 )
```
//这条记录对应的所有标签封装到map中
valparamMap=scala.collection.mutable.Map[String,Int]()
paramMap+=("ipBlock"->ipBlockCounts)
paramMap+=("ip"->ipAccessCounts)
paramMap+=("criticalPages"->criticalPageAccessCounts)
paramMap+=("userAgent"->userAgentCounts)
paramMap+=("criticalPagesAccTime"->critivalPageMinInterval)
paramMap+=("flightQuery"->differentTripQuerysCounts)
paramMap+=("criticalCookies"->criticalCookies)
paramMap+=("criticalPagesLessThanDefault"->accessPageIntervalLessThanDefault)
/**
* 计算打分结果
*paramMap：在 5 分钟之内统计的结果
*FlowCollocations：数据库规则，规定 5 分钟内不允许超过限制的值
* 最终结果为：Array[（流程Id，流程得分，流程阈值,是否大于阈值大于阈值定义为
爬虫）]
*/
valflowsScore:Array[FlowScoreResult]=calculateFlowsScore(paramMap,FlowCollocations)
//针对这条记录封装的打分类，包含了这条记录的所有统计结果、打分、是否命中等等
AntiCalculateResult(processedData,ip,ipBlockCounts,ipAccessCounts,
criticalPageAccessCounts,userAgentCounts,critivalPageMinInterval,
accessPageIntervalLessThanDefault,differentTripQuerysCounts,
criticalCookies,flowsScore)
}


## 50. 3. 3. 计算规则得分 calculateFlowsScore

###### /**

###### *计算规则得分

###### *

*@paramparamMapparamMap
*@paramflowList 流程列表
*@return 流程得分
*/
def calculateFlowsScore(paramMap: scala.collection.mutable.Map[String, Int], flowList:
Array[FlowCollocation]):Array[FlowScoreResult]={
//封装最终打分结果：flowId、flowScore、flowLimitedScore、是否超过阈值、
flowStrategyCode、命中规则列表、命中时间
valflowScores=newArrayBuffer[FlowScoreResult]
//循环数据库查询出来的所有流程，进行匹配打分
for(flow<-flowList){
//拿出当前流程的规则，就是我们web页面配置的那些阈值
valruleList=flow.rules
//用来封装命中的规则的rileId
valhitRules=ListBuffer[String]()

//保存规则计算结果的二维数组（ 2 行，n列），第一维是之前streaming计算统计的
结果，第二维是针对对应统计结果的数据库打分结果
valresult=Array.ofDim[Double]( 2 ,ruleList.size)
//根据每个流程对应的规则统计结果与预设的规则进行对比，若统计结果大于预设值，
则对应的规则得分有效，否则，无效（即设为 0 ）
varruleIndex= 0
//规则是否触发，也就是web页面的复选框有没有被勾选
valisTriggered=newArrayBuffer[Int]()
//循环数据库规则，循环结束，会将result填满，hitRules填满，isTriggered填满
for(rule<-ruleList){
//规则状态放到这个数组
isTriggered+=rule.ruleStatus
//规则名字
valruleName=rule.ruleName
//通过规则名字去streaming统计的结果中找数值
valruleValue=paramMap.getOrElse(ruleName, 0 )
//把streaming统计结果封装到第 0 行，第ruleIndex列，后续ruleIndex会做+ 1 操
作
result( 0 )(ruleIndex)=ruleValue
//拿出数据库对应这个规则设置的阈值
valruleValue 1 =if("accessPageIntervalLessThanDefault".equals(ruleName)){
rule.ruleValue 1


}else{
rule.ruleValue 0
}
//数据库对应这个规则的打分
valruleScore=rule.ruleScore
if(ruleValue>ruleValue 1 ){
//如果streaming统计结果超过了数据库阈值，将打分记录到result的第 1 行，
第ruleIndex列，后续ruleIndex会做+ 1 操作
result( 1 )(ruleIndex)=ruleScore
//规则命中，将规则信息添加到数组
hitRules.append(rule.ruleId)
}else{
//没命中，打分设置为 0
result( 1 )(ruleIndex)= 0
}
//ruleIndex做+ 1 操作，继续对比第二个rule规则
ruleIndex=ruleIndex+ 1
}
//计算流程打分，打分区间为：平均分-- 10 *平均分
valflowScore=calculateFlowScore(result,isTriggered.toArray)
//命中时间
valhitTime=sdf.format(newDate())
//（流程Id，流程得分，流程阈值,是否大于阈值，strategyCode，命中规则id列表，
命中时间）- 大于阈值定义为爬虫
flowScores.append(FlowScoreResult(flow.flowId,flowScore,flow.flowLimitScore,
flowScore>flow.flowLimitScore,flow.strategyCode,hitRules.toList,hitTime))
}
//将所有流程的结果信息返回
flowScores.toArray
}

## 50. 3. 4. 计算流程得分

###### /**

###### *计算流程得分-请参考详细设计说明书（规则打分，流程计算）及对应的原型设计（流

###### 程管理）

###### *系数 2 权重： 60 %，数据区间： 10 - 60

###### *系数 3 权重： 40 ，数据区间： 0 - 40

###### *系数 2 +系数 3 区间为： 10 - 100

###### *系数 1 为:平均分/ 10

```
*所以，factor 1 *(factor 2 +factor 3 )区间为:平均分-- 10 倍平均分
*@paramresult result二维数组
*@paramisTriggeredisTriggered数组
*@return 规则得分
```

###### */

defcalculateFlowScore(result:Array[Array[Double]],isTriggered:Array[Int]):Double={
//打分列表
valscores=result( 1 )
//总打分
valsum=scores.sum
//打分列表长度
valdim=scores.length
//系数 1 ：平均分/ 10
valfactor 1 =sum/( 10 *dim)
//命中数据库开放规则的score
valxa=triggeredScore(scores,isTriggered)
//命中规则中，规则分数最高的
valmaxInXa=if(xa.isEmpty){
0. 0
}else{
xa.max
}
//系数 2 ：系数 2 的权重是 60 ，指的是最高score以 6 为分界，最高score大于 6 ，就给
满权重 60 ，不足 6 ，就给对应的maxInXa* 10
valfactor 2 =if( 1 <( 1. 0 / 6. 0 )*maxInXa){
60
}else{
( 1. 0 / 6. 0 )*maxInXa* 60
}
//系数 3 ：打开的规则总分占总规则总分的百分比，并且系数 3 的权重是 40
valfactor 3 = 40 *(xa.sum/sum)
/**
* 系数 2 权重： 60 %，数据区间： 10 - 60
* 系数 3 权重： 40 ，数据区间： 0 - 40
* 系数 2 +系数 3 区间为： 10 - 100
* 系数 1 为:平均分/ 10
* 所以，factor 1 *(factor 2 +factor 3 )区间为:平均分-- 10 倍平均分
*/
factor 1 *(factor 2 +factor 3 )
}

## 50. 3. 5. 封装流程计算结果类

###### /**

###### * 封装流程计算结果

###### *

```
*@paramflowId 流程id
*@paramflowScore流程得分
```

*@paramflowLimitedScore流程阈值
*@paramisUpLimited 流程得分是否大于阈值
*@paramflowStrategyCode 流程策略代码
*/
caseclassFlowScoreResult(flowId:String,
flowScore:Double,
flowLimitedScore:Double,
isUpLimited:Boolean,
flowStrategyCode:String,
hitRules:List[String],
hitTime:String)

## 50. 3. 6. 反爬虫计算结果封装类

###### /**

###### * 反爬计算结果封装类

###### *

*@paramip 请求IP
*@paramipBlockCounts 单位时间内IP段访问量
*@paramipAccessCounts单位时间内某个IP访问量
*@paramcriticalPageAccessCounts 单位时间内的关键页面访问总量
*@paramuserAgentCounts 单位时间内的UA种类数统计
*@paramcritivalPageMinInterval单位时间内的关键页面最短访问间隔
*@paramaccessPageIntervalLessThanDefault 单位时间内小于最短访问间隔（自设）的关键
页面查询次数
*@paramdifferentTripQuerysCounts单位时间内查询不同行程的次数
*@paramcriticalCookies 单位时间内关键页面的Cookie数
*@paramflowsScore流程计算结果
*/
caseclassAntiCalculateResult(processedData:ProcessedData,
ip:String,
ipBlockCounts:Int,
ipAccessCounts:Int,
criticalPageAccessCounts:Int,
userAgentCounts:Int,
critivalPageMinInterval:Int,
accessPageIntervalLessThanDefault:Int,
differentTripQuerysCounts:Int,
criticalCookies:Int,
flowsScore:Array[FlowScoreResult])


# 51. 剔除非黑名单数据

## 51. 1. 需求

###### 之前，我们已经计算出来了个流程的打分，也已经进行了是否为黑名单数据的标记，现

在，我们需要剔除飞黑名单数据，将黑名单数据进行redis推送

## 51. 2. 设计

1 、拿到计算结果antiCalculateResults进行filter
2 、使用flowScore的isUpLimited属性进行过滤
3 、如果结果为nonEmpty则为黑名单数据

## 51. 3. 代码

###### /*

###### *剔除非黑名单数据(各流程打分结果均小于阈值,我们值设置了一个流程，所以只会循

###### 环一次)

###### */

```
valantiBlackResults=antiCalculateResults.filter{antiCalculateResult=>
valupLimitedFlows=antiCalculateResult.flowsScore.filter{flowScore=>
//阈值判断结果，打分值大于阈值，为true
flowScore.isUpLimited
}
//数据非空，说明存在大于阈值的流程打分
upLimitedFlows.nonEmpty
}
```
# 52. 黑名单数据去重

## 52. 1. 需求

拿到黑名单数据antiBlackResults我们会发现，我们之前是每隔 5 分钟进行的指标计算，
此时出现的结果就会有重复的ip黑名单数据，我们在保存redis之前，要将重复的数据进行
过滤


## 52. 2. 设计

1 、拿到黑名单数据进行map，只保留ip和打分数据
2 、Map后进行foreachRdd，然后使用reduceByKey算子将相同的ip过滤掉
3 、将过滤后的数据进行collect，用于后面推送

## 52. 3. 代码

antiBlackResults.map{antiBlackResult=>
//黑名单ip，黑名单打分
(antiBlackResult.ip,antiBlackResult.flowsScore)
}.foreachRDD{rdd=>
//过滤掉重复的数据，（ip，流程分数）
valdistincted:RDD[(String,Array[FlowScoreResult])]=rdd.reduceByKey((x,y)=>x)
//反爬虫黑名单数据（ip，流程分数）
valantiBlackList:Array[(String,Array[FlowScoreResult])]=distincted.collect()
}

# 53. 黑名单 redis 数据恢复

## 53. 1. 需求

因为redis是内存数据库，为了防止redis数据丢失，我们会在代码中设置数据恢复过程，
因为之后我们会将黑名单的结果推送到hdfs，所以在这里我们可以预先将hdfs的数据恢复
到redis，避免redis数据丢失
注意，只有在redis中dang的值不为no的时候才会进行数据恢复

## 53. 2. 设计

1 、构建hdfs的fsSystem
2 、获取当前时间，并向前递减 1 小时数据，步长为 1 小时，数据路径为：
hdfs:// 192. 168. 56. 151 : 9000 /csair/data/rule-black-list/时间戳
3 、将数据从hdfs中取出，以表结构的方式取数据
4 、将数据存储到redis，过期时间为一小时


## 53. 3. 代码

## 53. 3. 1. 数据恢复 redis

antiBlackResults.map{antiBlackResult=>
//黑名单ip，黑名单打分
(antiBlackResult.ip,antiBlackResult.flowsScore)
}.foreachRDD{rdd=>
//过滤掉重复的数据，（ip，流程分数）
valdistincted:RDD[(String,Array[FlowScoreResult])]=rdd.reduceByKey((x,y)=>x)
//反爬虫黑名单数据（ip，流程分数）
valantiBlackList:Array[(String,Array[FlowScoreResult])]=distincted.collect()
if(antiBlackList.nonEmpty){
//黑名单DataFrame-备份到HDFS
valantiBlackListDFR=newArrayBuffer[Row]()
try{
//创建jedis连接
valjedis=JedisConnectionUtil.getJedisCluster
/*
*恢复redis黑名单数据，用于防止程序停止而产生的redis数据丢失
*/
BlackListToRedis.blackListDataToRedis(jedis,sc,sqlContext)
}

## 53. 3. 2. BlackListToRedis.blackListDataToRedis

###### /**

* 将HDFS中的黑名单的数据存到Redis中，有效时间为 1 小时
*/
objectBlackListToRedis{
/**
*判断指定redis库中的表是否为空，为空则进行设置HDFS中目录的标识
*
*@paramjedis：通过jedis对redis进行操作
*@paramsc：传入的sparkContext
*@paramsqlContext ：传入的SQLContext
*/
defblackListDataToRedis(jedis:JedisCluster,sc:SparkContext,sqlContext:SQLContext):Unit={
valdangKey=jedis.get("dang")
if(dangKey=="no"){
jedis.set("dang","no")
}else{


valarrayBuffer:ArrayBuffer[String]=ArrayBuffer()
//获取hdfs的FileSystem
valconf=newConfiguration()
//配置hdfs的路径
conf.set("fs.defaultFS","hdfs:// 192. 168. 56. 151 : 9000 ");
valfs=FileSystem.get(conf)
//时间格式化类，用于按时间存储
valsdf=newSimpleDateFormat("yyyy/MM/dd/HH")
//获取当前时间
valsysCurrentTime=System.currentTimeMillis()
//添加hdfs路径到arrayBuffer
for(i<- 0 to 1 ){
//为了避免有效数据丢失，所以从前一个小时之前进行取数据
valstartTime=sdf.format(sysCurrentTime- 1000 * 60 * 60 *i)
valpath=PropertiesUtil.getStringByKey("blackListPath","HDFSPathConfig.properties")
+startTime
if(fs.exists(newPath(path))){
arrayBuffer+=path
}
}
//将黑名单数据向Redis中进行插入
dataToRedis(sqlContext,arrayBuffer.toArray,jedis)
}
}

## 53. 3. 3. dataToRedis

###### /**

*创建DataFrame
*
*@paramsqlContext
*@parampaths：HDFS路径
*@paramjedis
*/
defdataToRedis(sqlContext:SQLContext,paths:Array[String],jedis:JedisCluster):Unit={
valparquet 1 :DataFrame=sqlContext.read.parquet(paths:_*)
//对parquet 1 按key去重，取最大的keyExpTime对应的记录，这里使用表join进行
去重
parquet 1 .registerTempTable("blacklist")
val grouped = sqlContext.sql("select max(keyExpTime)keyExpTime,keyfrom blacklist
groupbykey")
grouped.registerTempTable("groupedlist")
valdistincted=sqlContext.sql("selecta.keyExpTime,a.key,a.valuefromblacklistajoin
groupedlistbona.keyExpTime=b.keyExpTimeanda.key=b.key")


distincted.collect().foreach( s =>
toRedis(jedis,s.get( 1 ).toString,s.get( 2 ).toString,s.get( 0 ).toString.toLong))
}

## 53. 3. 4. toRedis

###### /**

```
*将数据存到redis中，按照key，value的形式
*/
```
```
deftoRedis(jedis:JedisCluster,flowId:String,flowScoreStrategyCode:String,failureTime:Long){
valtime=((failureTime-System.currentTimeMillis())/ 1000 ).toInt
if(time> 0 ){
if(!jedis.exists(flowId)){
//time为过期时间（ 1 小时）
jedis.setex(flowId,time,flowScoreStrategyCode)
}
}
}
```
## 53. 3. 5. HDFSPathConfig.properties

###### #存HDFS数据路径

###### #黑名单提交到HDFS的路径

blackListPath=hdfs:// 192. 168. 56. 151 : 9000 /csair/data/rule-black-list/

# 54. 黑名单数据备份到 redis

## 54. 1. 需求

将黑名单数据实时存储到redis，redis黑名单库中的键 ip:flowId，redis黑名单库中的值：
flowScore|strategyCode|hitRules|time

## 54. 2. 设计

###### 1 、循环黑名单数据

2 、创建redis的key和value
3 、存储黑名单数据到redis，设置超时时间为 1 小时
4 、添加黑名单DataFrame-备份到ArrayBuffer


## 54. 3. 代码

###### /*

###### *推送黑名单

###### */

antiBlackResults.map{antiBlackResult=>
//黑名单ip，黑名单打分
(antiBlackResult.ip,antiBlackResult.flowsScore)
}.foreachRDD{rdd=>
//过滤掉重复的数据，（ip，流程分数）
valdistincted:RDD[(String,Array[FlowScoreResult])]=rdd.reduceByKey((x,y)=>x)
//反爬虫黑名单数据（ip，流程分数）
valantiBlackList:Array[(String,Array[FlowScoreResult])]=distincted.collect()
if(antiBlackList.nonEmpty){
//黑名单DataFrame-备份到HDFS
valantiBlackListDFR=newArrayBuffer[Row]()
try{
//创建jedis连接
valjedis=JedisConnectionUtil.getJedisCluster
/*
*恢复redis黑名单数据，用于防止程序停止而产生的redis数据丢失
*/
BlackListToRedis.blackListDataToRedis(jedis,sc,sqlContext)
/*
*将黑名单数据存储到redis
*对各个流程的反爬虫数据进行处理Array[(ip,Array[FlowScoreResult])]
*/
antiBlackList.foreach{antiBlackRecord=>
//拿到某个反爬虫数据的打分信息 Array[FlowScoreResult])
antiBlackRecord._ 2 .foreach{antiBlackRecordByFlow=>
//Redis基于key:field-value的方式保存黑名单
//redis黑名单库中的键 ip:flowId
val blackListKey = PropertiesUtil.getStringByKey("cluster.key.anti_black_list",
"jedisConfig.properties")+s"${antiBlackRecord._ 1 }:${antiBlackRecordByFlow.flowId}"
//redis黑名单库中的值：flowScore|strategyCode|hitRules|time
val blackListValue =
s"${antiBlackRecordByFlow.flowScore}|${antiBlackRecordByFlow.flowStrategyCode}|${antiBlackR
ecordByFlow.hitRules.mkString(",")}|${antiBlackRecordByFlow.hitTime}"
//更新黑名单库，超时时间为 3600 秒
jedis.setex(blackListKey,
PropertiesUtil.getStringByKey("cluster.exptime.anti_black_list", "jedisConfig.properties").toInt,
blackListValue)
//添加黑名单DataFrame-备份到ArrayBuffer


antiBlackListDFR.append(Row(((PropertiesUtil.getStringByKey("cluster.exptime.anti_black_list",
"jedisConfig.properties").toLong) * 1000 + System.currentTimeMillis()).toString, blackListKey,
blackListValue))

}
}
}catch{
casee:Exception=>
e.printStackTrace()
}
}

# 55. 黑名单数据实时存储 hdfs ，用于 redis

# 数据恢复

## 55. 1. 需求

```
为了防止redis数据丢失，我们需要将datafream实时存储到hdfs，用于redis数据恢复
```
## 55. 2. 设计

1 、构建datafream
2 、存储datafream

## 55. 3. 代码

## 55. 3. 1. 备份 datafream

###### /*

*增加黑名单数据实时存储到HDFS的功能-黑名单数据持久化-用于Redis数据恢
复
*/
BlackListToHDFS.saveAntiBlackList(sc.parallelize(antiBlackListDFR),sqlContext)

## 55. 3. 2. BlackListToHDFS.saveAntiBlackList

packagecom.air.antispider.stream.common.util.HDFS


importcom.air.antispider.stream.common.util.jedis.PropertiesUtil
importorg.apache.spark.rdd.RDD
importorg.apache.spark.sql.types.{StringType,StructField,StructType}
importorg.apache.spark.sql.{DataFrame,Row,SQLContext}

/**
* 黑名单保存到HDFS服务
*/
objectBlackListToHDFS{
/**
*保存黑名单到HDFS
*
*@paramantiBlackListRDD：传入黑名单RDD
*@paramsqlContext：传入入sqlContext创建DataFrame
*/
defsaveAntiBlackList(antiBlackListRDD:RDD[Row],sqlContext:SQLContext)={

//构建DataFrame
valtableCols=List("keyExpTime","key","value")
valschemaString=tableCols.mkString("")
valschema= StructType(schemaString.split("").map(fieldName=>StructField(fieldName,
StringType,true)))
valdataFrame:DataFrame=sqlContext.createDataFrame(antiBlackListRDD,schema)
val path: String =
PropertiesUtil.getStringByKey("blackListPath","HDFSPathConfig.properties")
HdfsSaveUtil.save(dataFrame,null,path)
}

```
defsaveAntiOcpBlackList():Unit={
```
###### }

###### }

## 55. 3. 3. HdfsSaveUtil.save

packagecom.air.antispider.stream.common.util.HDFS

importjava.text.SimpleDateFormat
importjava.util.Date

importorg.apache.spark.sql.{DataFrame,SaveMode}


importorg.apache.spark.streaming.Time

/**
* 通用HDFS数据保存服务
*/
objectHdfsSaveUtil{

```
/**
*基于时间生成文件夹保存DataFrame数据到HDFS
*@paramdataFrame 数据
*@paramtime数据时间
*@parampaths保存路径
*/
defsave(dataFrame:DataFrame,time:Time,paths:String):Unit={
valdate =if(time==null){
newSimpleDateFormat("yyyy/MM/dd/HH").format(System.currentTimeMillis())
}else{
newSimpleDateFormat("yyyy/MM/dd/HH").format(newDate(time.milliseconds))
}
valpath=paths+date
dataFrame.write.mode(SaveMode.Append).format("parquet").save(path)
}
```
}

# 56. 存储规则计算结果 RDD 到 HDFS

## 56. 1. 需求

将计算结果的Rdd存储到hdfs，便于我们后期做离线统计报表使用，此功能需要打包到线
上运行，因为本地的local模式会在本地找hiveContent环境，而本地并没有。

## 56. 2. 设计

```
将计算结果的Rdd存储到hdfs，便于我们后期做离线统计报表使用
```

## 56. 3. 代码

## 56. 3. 1. 存储计算 rdd

###### /*

*存储规则计算结果RDD（antiCalculateResults） 到HDFS
*/
lines.foreachRDD(antiCalculateResult=>{
val date = new
SimpleDateFormat("yyyy/MM/dd/HH").format(System.currentTimeMillis())
valyyyyMMddHH=date.replace("/","").toInt
val path: String =
PropertiesUtil.getStringByKey("blackListPath","HDFSPathConfig.properties")+"itheima/"+yyyyM
MddHH
try{

sc.textFile(path+"/part- 00000 ").union(antiCalculateResult).repartition( 1 ).saveAsTextFile(path)
}catch{
casee:Exception=>
antiCalculateResult.repartition( 1 ).saveAsTextFile(path)
} })

# 57. 数据展示

## 57. 1. Redis 数据


## 57. 2. Hdfs 数据

## 57. 3. 反爬虫数据分析速度

## 57. 4. 爬虫识别监控

作业：将redis中的爬虫数据统计到mysql中展示


# 58. 反爬虫规则功能设计

## 58. 1. 模块功能

###### 设计者

###### 编写日期

###### 修订者

###### 修订日期

###### 模块编号 按命名规则，填写此模块的编号。模块编号为 4 位阿拉伯数字，第一位

###### 数字为模块所在子系统的序号，后 3 位为模块顺序号

###### 模块名称 此模块英文名称或英文名称缩写，也可以使用中文名

###### 模块功能 反爬虫统计字段规则：

###### 总体统计

###### 单位时间内的IP段访问量（前两位）

###### 基于IP的统计

###### 单位时间内的访问总量

###### 单位时间内的关键页面访问总量

###### 单位时间内的UA出现次数统计

###### 单位时间内的关键页面最短访问间隔

###### 单位时间内小于最短访问间隔（自设）的关键页面查询次数

```
单位时间内关键页面的访问次数的Cookie数少于X（自设）
单位时间内查询不同行程的次数
```
```
模块背景描述 该模块主要统计反爬规则，即将目标数据按设定规则进行统计和判断
```

## 58. 2. 模块用例图

###### 图反爬虫规则用例图


## 58. 3. 模块逻辑分析

###### 图模块分析图


## 58. 4. 模块数据逻辑

## 58. 4. 1. IP 段访问量

######  将格式化数据按分隔符（#CS#）分割，即可得到IP；

```
 截取IP前两段的值作为key，value为 1 ；
 通过SparkStreaming窗口函数进行统计。
```
## 58. 4. 2. IP 总访问量

######  将格式化数据按分隔符（#CS#）分割，即可得到IP；

```
 将IP的值作为key，value为 1 ；
 通过SparkStreaming窗口函数进行统计。
```
###### 图IP段访问分析图

###### 图IP总访问分析图


## 58. 4. 3. 单位时间内某个 IP 对关键页面访问总量

######  将格式化数据按分隔符（#CS#）分割，即可得到IP和URL；

######  判断URL是否为关键页面（暂定为查询页面），若是，则构造元组（IP， 1 ）；

```
 通过SparkStreaming窗口函数进行统计。
```
## 58. 4. 4. 单位时间内某个 IP 的 UA 种类数统计

######  将格式化数据按分隔符（#CS#）分割，即可得到IP和UA；

######  构造元组（IP，UA）；

```
 通过SparkStreaming窗口函数按IP进行分组统计，得到每个IP对应的一组UA；
 统计每个IP中UA的数量。
```
###### 图IP关键页面访问分析图

###### 图IPUA种类访问分析图


## 58. 4. 5. 单位时间内某个 IP 对关键页面最短访问间隔

```
 将格式化数据按分隔符（#CS#）分割，即可得到IP，URL，Time；
 判断URL是否为关键页面（暂定为查询页面），若是，则构造元组（IP，Time）；
 通过SparkStreaming窗口函数按IP进行分组统计，得到每个IP对应的一组Time；
 将每组Time中的时刻进行排序，计算相邻时刻的间隔；
 取每个IP对应的时间间隔的最小值。
```
###### 图单位时间内某个IP对关键页面最短访问间隔


## 58. 4. 6. 单位时间内某个 IP 对小于最短访问间隔（自设）的

## 关键页面查询次数

```
 将格式化数据按分隔符（#CS#）分割，即可得到IP，URL，Time；
 判断URL是否为关键页面（暂定为查询页面），若是，则构造元组（（IP，URL），
Time）；
 通过SparkStreaming窗口函数按（IP，URL）进行分组统计，得到每个元组（IP，
URL）对应的一组Time；
 将每组Time中的时刻进行排序，计算相邻时刻的间隔；
 取每个IP对应的一组时间间隔与最短访问间隔（自设）比较；
 统计符合条件的数量。
```
###### 图单位时间内某个IP对小于最短访问间隔（自设）的关键页面查询次数


## 58. 4. 7. 统计单位时间内某个 IP 查询不同行程的次数

```
 将格式化数据按分隔符（#CS#）分割，即可得到IP，始发地dep，目的地des；
 构造元组（IP，（dep，des））
 通过SparkStreaming窗口函数按IP进行分组统计，得到每个IP对应的一组（dep，
des）；
 统计每个IP中（dep，des）的数量。
```
## 58. 4. 8. 规则打分

###### 图单位时间内某个IP查询不同行程的次数

###### 图规则打分


## 58. 5. 模块应用逻辑

# 59. 课程目标

###### 1 、设置任务监控

###### 2 、统计报表

3 、对接web端（练习）
4 、测试数据改装程序

# 60. 设置任务监控

## 60. 1. 需求

```
对当前streaming任务设置任务监控
```
###### 图模块应用逻辑


## 60. 2. 代码

## 60. 2. 1. 任务监控代码

lines.foreachRDD(antiCalculateResult=>{
val date = new
SimpleDateFormat("yyyy/MM/dd/HH").format(System.currentTimeMillis())
valyyyyMMddHH=date.replace("/","").toInt
val path: String =
PropertiesUtil.getStringByKey("blackListPath","HDFSPathConfig.properties")+"itheima/"+yyyyM
MddHH
try{

sc.textFile(path+"/part- 00000 ").union(antiCalculateResult).repartition( 1 ).saveAsTextFile(path)
}catch{
casee:Exception=>
antiCalculateResult.repartition( 1 ).saveAsTextFile(path)
}
//设置任务监控
SparkStreamingMonitor.queryMonitor(sc,antiCalculateResult)
})

## 60. 2. 2. SparkStreamingMonitor.queryMonitor

defqueryMonitor(sc:SparkContext,messageRDD:RDD[String]):Unit={
//done:Spark性能实时监控
//监控数据获取
valsourceCount=messageRDD.count()
// val sparkDriverHost =
sc.getConf.get("spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY
_URI_BASES")
//监控信息页面路径为集群路径+/proxy/+应用id+/metrics/json
//valurl=s"${sparkDriverHost}/metrics/json"
//local模式的路径
valurl="http://localhost: 4041 /metrics/json/"
valjsonObj=SparkMetricsUtils.getMetricsJson(url)
//应用的一些监控指标在节点gauges下
valresult=jsonObj.getJSONObject("gauges")
//监控信息的json路径：应用id+.driver.+应用名称+具体的监控指标名称
//最近完成批次的处理开始时间-Unix时间戳（Unixtimestamp）-单位：毫秒
valstartTimePath=sc.applicationId+".driver."+"csair_streaming_rulecompute_antispider"
+".StreamingMetrics.streaming.lastCompletedBatch_processingStartTime"


valstartValue=result.getJSONObject(startTimePath)
varprocessingStartTime:Long= 0
if(startValue!=null){
processingStartTime=startValue.getLong("value")
}
//最近完成批次的处理结束时间-Unix时间戳（Unixtimestamp）-单位：毫秒
valendTimePath=sc.applicationId+".driver."+"csair_streaming_rulecompute_antispider"
+".StreamingMetrics.streaming.lastCompletedBatch_processingEndTime"
valendValue=result.getJSONObject(endTimePath)
valprocessingEndTime=endValue.getLong("value")
//流程所用时间
valprocessTime=processingEndTime-processingStartTime

//监控数据推送
//done:实时处理的速度监控指标-monitorIndex需要写入Redis，由web端读取Redis并
持久化到Mysql
valendTime=processingEndTime
valcostTime=processTime
valcountPerMillis=sourceCount.toFloat/costTime.toFloat
valmonitorIndex=(endTime,"csair_streaming_rulecompute_antispider",sc.applicationId,
sourceCount,costTime,countPerMillis)
//产生不重复的key值
val keyName = PropertiesUtil.getStringByKey("cluster.key.monitor.query",
"jedisConfig.properties")+System.currentTimeMillis.toString
val keyNameLast = PropertiesUtil.getStringByKey("cluster.key.monitor.query",
"jedisConfig.properties")+"_LAST"
valformat=newSimpleDateFormat("yyyy-MM-ddHH:mm:ss")
valprocessingEndTimeString=format.format(newDate(processingEndTime))
valfieldMap=scala.collection.mutable.Map(
"endTime"->processingEndTimeString,
"applicationUniqueName"->monitorIndex._ 2 .toString,
"applicationId"->monitorIndex._ 3 .toString,
"sourceCount"->monitorIndex._ 4 .toString,
"costTime"->monitorIndex._ 5 .toString,
"countPerMillis"->monitorIndex._ 6 .toString,
"serversCountMap"->Map[String,Int]())
//将数据存入redis中
try{
valjedis=JedisConnectionUtil.getJedisCluster
//监控Redis库
//jedis.select(redis_db_monitor);
//保存监控数据
jedis.setex(keyName, PropertiesUtil.getStringByKey("cluster.exptime.monitor",
"jedisConfig.properties").toInt,Json(DefaultFormats).write(fieldMap))


###### //更新最新的监控数据

jedis.setex(keyNameLast, PropertiesUtil.getStringByKey("cluster.exptime.monitor",
"jedisConfig.properties").toInt,Json(DefaultFormats).write(fieldMap))

```
//JedisConnectionUtil.returnRes(jedis)
}catch{
casee:Exception=>
e.printStackTrace()
}
}
```
# 61. 报表模块加载

## 61. 1. 加载数据，注册表

## 61. 1. 1. 需求

```
加载kafka数据清洗后的数据（过滤掉html等），加载爬虫数据，将数据注册成表
```
## 61. 1. 2. 代码

packagecom.csair.b 2 c.gciantispider.offlinecalculation.statisticaloffline

/**
*purpose:
*version:
*date:
*author:wangsf.
*/

importjava.sql.Date
importjava.util.UUID

importcom.csair.b 2 c.gciantispider.offlinecalculation.util.{LoggerLevels,SparkMySqlProperties}
importcom.csair.b 2 c.gciantispider.util.PropertiesUtil
importorg.apache.spark.sql.Row
importorg.apache.spark.sql.types._
importorg.apache.spark.{SparkConf,SparkContext}

/**


###### * 计算可视化数据

* 数据来源是streaming存储到sparksql中的数据
*
*@authorchengxc
*/
caseclassRequests(requestMethod:String,request:String,remoteAddr:String,httpUserAgent:
String,timeIso 8601 :String,serverAddr:String,criticalCookie:String,highFrqIPGroup: String,
flightType:String,behaviorType:String,travelType:String,flightDate:String,depcity:String,
arrcity: String, JSESSIONID: String, USERID: String, queryRequestDataStr: String,
bookRequestDataStr:String,httpReferrer:String,StageTag:String,Spider:Int)

caseclassBlackList(remoteAddr:String,FlowID:String,Score:String,StrategicID:String)

objectVisualizationIndicators{
defmain(args:Array[String]):Unit={
LoggerLevels.setStreamingLogLevels()
val conf = new
SparkConf().setMaster("local").setAppName("visualization").setMaster("local[*]")
valsc=newSparkContext(conf)
valsqlContext=neworg.apache.spark.sql.SQLContext(sc)
importsqlContext.implicits._
/**
* 读取原始数据，注册为表
*/
//数据路径
valdefaultPathConfig="offlineConfig.properties"
valfilePath=PropertiesUtil.getStringByKey("inputFilePath",defaultPathConfig)
//读取kafka中的原始日志信息
valrequest=sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资
料\\数据样本\\test\\part- 00000 ")
.map(
_.split("#CS#")).map(
p=>Requests(p( 1 ),p( 2 ),p( 3 ),p( 4 ),p( 5 ),p( 6 ),"",p( 7 ),p( 8 ),p( 9 ),p( 10 ),p( 11 ),p( 12 ),p( 13 ),
p( 14 ),p( 15 ),p( 16 ),p( 17 ),p( 18 ),"", 0 )).toDF()
//注册为表request
request.registerTempTable("request")
//读取黑名单信息
valSpiderIP=sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资
料\\数据样本\\SpiderIP.txt").map(_.split("\\|")).map(p=>BlackList(p( 0 ),p( 1 ),p( 2 ),p( 3 ))).toDF()
//注册为表SpiderIP
SpiderIP.registerTempTable("SpiderIP")
}


## 61. 2. 标记爬虫

## 61. 2. 1. 需求

```
将两张表用remoteAddr关联，标记爬虫数据
```
## 61. 2. 2. 代码

###### /**

###### * 标记爬虫

###### */

val AddSpiderTag = request.join(SpiderIP, request("remoteAddr") ===
SpiderIP("remoteAddr"),"left_outer")
//查询转化率必须的字段
val TransformRateNeeded = AddSpiderTag.select(AddSpiderTag("request"),
AddSpiderTag("JSESSIONID"), AddSpiderTag("StageTag"), AddSpiderTag("FlowID"),
AddSpiderTag("travelType"),AddSpiderTag("flightType"))
//注册为TransformRateNeeded表
TransformRateNeeded.registerTempTable("TransformRateNeeded")
val date: String =
AddSpiderTag.select(AddSpiderTag("timeIso 8601 ")).first().get( 0 ).toString.split("T")( 0 )
valdataTime:Date=Date.valueOf(date)

## 61. 3. 打阶段标签

## 61. 3. 1. 需求

```
根据request信息匹配url，标记request类型，通过sessionid标记爬虫
```
## 61. 3. 2. 代码

###### /**

###### * 打阶段标签

###### */

```
sqlContext.udf.register("Stage",(request:String)=>{
if(request.matches("^.*/bookingnew/.*$")//预定、商城
||request.matches("^.*/bookingGroup/.*$")
||request.matches("^.*/ita/intl/zh/shop/.*$")){
" 1 "
```

```
}elseif(request.matches("^.*/modules/permissionnew/.*$")//权限、乘客信息
||request.matches("^.*/ita/intl/zh/passengers/.*$")){
" 2 "
}elseif(request.matches("^.*upp_payment/pay/.*$")){
//支付
" 3 "
}else{
//其他，查询等操作
" 0 "
}
})
//FlowID不为空时对应为爬虫
sqlContext.udf.register("Spider",(FlowID:String)=>{
if(FlowID==null||"null".equalsIgnoreCase(FlowID)){
//非爬虫
" 0 "
}else{
//爬虫
" 1 "
}
})
```
## 61. 4. 数据过滤

## 61. 4. 1. 需求

单独过滤出sessionid问空的数据

## 61. 4. 2. 代码

###### /**

###### * 过滤数据

###### */

//原始数据：request,JSESSIONID,Stage(request)asStageTag,Spider(FlowID)asSpiderTag,
flightType,travelType
val request 1 = sqlContext.sql("select request,JSESSIONID,Stage(request) as StageTag,
Spider(FlowID)asSpiderTag,flightType,travelTypefromTransformRateNeeded")
//从表TransformRateNeeded中过滤掉JSESSIONID为空的数据
valrequest 1 _transformed=request 1 .filter(!request 1 ("JSESSIONID").contains("NULL"))


# 62. 转化率

## 62. 1. 国内查询转化率

## 62. 1. 1. 代码

###### /*

###### 国内查询转化率

###### */

val NatinalRate_ 1 =
request 1 _transformed.filter(request 1 _transformed("flightType").equalTo("National")).filter(requ
est 1 _transformed("StageTag").equalTo(" 2 ")).count().toFloat/

request 1 .filter(request 1 ("flightType").equalTo("National")).filter(request 1 ("StageTag").equalTo(" 1
")).count().toFloat
//通过并行化创建RDD
val nh_domestic_inter_conversion_rate_RDD =
sc.parallelize(Array(UUID.randomUUID().toString() + "," + " 0 ," + " 0 ," +
NatinalRate_ 1 )).map(_.split(","))
//通过StructType直接指定每个字段的schema
val schema = StructType(List(StructField("id", StringType, true), StructField("step_type",
IntegerType,true),StructField("flight_type",IntegerType,true), StructField("conversion_rate",
FloatType,true),StructField("record_time",DateType,true)))
//将RDD映射到rowRDD
val rowRDD = nh_domestic_inter_conversion_rate_RDD.map(p => Row(p( 0 ), p( 1 ).toInt,
p( 2 ).toInt,p( 3 ).toFloat,dataTime))
//将schema信息应用到rowRDD上
valpersonDataFrame=sqlContext.createDataFrame(rowRDD,schema)
//将数据追加到数据库

personDataFrame.write.mode("append").jdbc("jdbc:mysql:// 192. 168. 56. 204 : 3306 /gciantispider",
"gciantispider.nh_domestic_inter_conversion_rate",SparkMySqlProperties.getProperty())
println("国内查询转化率")
println(NatinalRate_ 1 )


## 62. 1. 2. 可视化表

## 62. 1. 3. 可视化代码


## 62. 1. 4. 可视化页面

## 62. 2. 国际查询转化率

## 62. 2. 1. 代码

###### /*

###### 国际查询转化率

###### */

val InternatinalRate_ 1 =
request 1 _transformed.filter(request 1 _transformed("flightType").equalTo("Internatinal")).filter(re
quest 1 _transformed("StageTag").equalTo(" 2 ")).count().toFloat/

request 1 .filter(request 1 ("flightType").equalTo("Internatinal")).filter(request 1 ("StageTag").equalTo
(" 1 ")).count().toFloat
//通过并行化创建RDD
val nh_domestic_inter_conversion_rate_RDD 2 =
sc.parallelize(Array(UUID.randomUUID().toString() + "," + " 0 ," + " 1 ," +
InternatinalRate_ 1 )).map(_.split(","))
//通过StructType直接指定每个字段的schema
val rowRDD 2 = nh_domestic_inter_conversion_rate_RDD 2 .map(p => Row(p( 0 ),
p( 1 ).toInt,p( 2 ).toInt,p( 3 ).toFloat,dataTime))
//将schema信息应用到rowRDD上
valpersonDataFrame 2 =sqlContext.createDataFrame(rowRDD 2 ,schema)
//将数据追加到数据库

personDataFrame 2 .write.mode("append").jdbc("jdbc:mysql:// 192. 168. 56. 204 : 3306 /gciantispider
","gciantispider.nh_domestic_inter_conversion_rate",SparkMySqlProperties.getProperty())


```
println("国际查询转化率")
println(InternatinalRate_ 1 )
```
## 62. 2. 2. 可视化表

## 62. 2. 3. 可视化代码


## 62. 2. 4. 可视化页面

# 63. 转化率

## 63. 1. 国内航班选择 - 旅客信息 转化率

## 63. 1. 1. 代码

###### /*

###### 国内航班选择-旅客信息转化率

###### */

val NatinalRate_ 2 =
request 1 _transformed.filter(request 1 _transformed("flightType").equalTo("National")).filter(requ
est 1 _transformed("StageTag").equalTo(" 3 ")).count().toFloat/

request 1 .filter(request 1 ("flightType").equalTo("National")).filter(request 1 ("StageTag").equalTo(" 2
")).count().toFloat
println("国内航班选择-旅客信息 转化率")
println(NatinalRate_ 2 )


## 63. 1. 2. 对接 web 端

###### 练习

1 、找到web端对应的代码
2 、找到web端对应的表结构
3 、编写代码实现web端对接

## 63. 2. 国际航班选择 - 旅客信息 转化率

## 63. 2. 1. 代码

###### /*

###### 国际航班选择-旅客信息转化率

###### */

val InternatinalRate_ 2 =
request 1 _transformed.filter(request 1 _transformed("flightType").equalTo("Internatinal")).filter(re
quest 1 _transformed("StageTag").equalTo(" 3 ")).count().toFloat/

request 1 .filter(request 1 ("flightType").equalTo("Internatinal")).filter(request 1 ("StageTag").equalTo
(" 2 ")).count().toFloat
println("国际航班选择-旅客信息 转化率")
println(InternatinalRate_ 2 )

## 63. 2. 2. 对接 web 端

###### 练习

4 、找到web端对应的代码
5 、找到web端对应的表结构
6 、编写代码实现web端对接

## 63. 3. 用户转化率

## 63. 3. 1. 代码

###### /*

###### 爬虫用户转化率

###### */

val SpiderUserRate_ 1 =
request 1 _transformed.filter(request 1 _transformed("SpiderTag").equalTo(" 1 ")).filter(request 1 _tra


nsformed("StageTag").equalTo(" 2 ")).count().toFloat/

request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).filter(request 1 ("StageTag").equalTo(" 1 ")).cou
nt().toFloat
println("爬虫用户 转化率")
println(SpiderUserRate_ 1 )
/*
正常用户转化率
*/
val NormalUserRate_ 1 =
request 1 _transformed.filter(request 1 _transformed("SpiderTag").equalTo(" 0 ")).filter(request 1 _tra
nsformed("StageTag").equalTo(" 2 ")).count().toFloat/

request 1 .filter(request 1 ("SpiderTag").equalTo(" 0 ")).filter(request 1 ("StageTag").equalTo(" 1 ")).cou
nt().toFloat
println("正常用户 转化率")
println(NormalUserRate_ 1 )
/*
全部用户转化率
*/
val TotalUserRate_ 1 =
request 1 _transformed.filter(request 1 _transformed("StageTag").equalTo(" 2 ")).count().toFloat/
request 1 .filter(request 1 ("StageTag").equalTo(" 1 ")).count().toFloat
println("全部用户 转化率")
println(TotalUserRate_ 1 )

## 63. 3. 2. 对接 web 端

###### 练习

7 、找到web端对应的代码
8 、找到web端对应的表结构
9 、编写代码实现web端对接

## 63. 4. 爬虫用户 航班选择 - 旅客信息转化率

## 63. 4. 1. 代码

###### /*

###### 爬虫用户 航班选择-旅客信息转化率

###### */

val SpiderUserRate_ 2 =
request 1 _transformed.filter(request 1 _transformed("SpiderTag").equalTo(" 1 ")).filter(request 1 _tra


nsformed("StageTag").equalTo(" 3 ")).count().toFloat/

request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).filter(request 1 ("StageTag").equalTo(" 2 ")).cou
nt().toFloat
println("爬虫用户 航班选择-旅客信息转化率")
println(SpiderUserRate_ 2 )
/*
正常用户 航班选择-旅客信息转化率
*/
val NormalUserRate_ 2 =
request 1 _transformed.filter(request 1 _transformed("SpiderTag").equalTo(" 0 ")).filter(request 1 _tra
nsformed("StageTag").equalTo(" 3 ")).count().toFloat/

request 1 .filter(request 1 ("SpiderTag").equalTo(" 0 ")).filter(request 1 ("StageTag").equalTo(" 2 ")).cou
nt().toFloat
println("正常用户 航班选择-旅客信息转化率")
println(NormalUserRate_ 2 )
/*
全部用户 航班选择-旅客信息转化率
*/
val TotalUserRate_ 2 =
request 1 _transformed.filter(request 1 _transformed("StageTag").equalTo(" 3 ")).count().toFloat/
request 1 .filter(request 1 ("StageTag").equalTo(" 2 ")).count().toFloat
println("全部用户 航班选择-旅客信息转化率")
println(TotalUserRate_ 2 )

## 63. 4. 2. 对接 web 端

###### 练习

10 、 找到web端对应的代码
11 、 找到web端对应的表结构
12 、 编写代码实现web端对接


# 64. 航班查询爬取规律

## 64. 1. 国内单程、国内双程、国际单程、国际双程查询爬取

## 频次

## 64. 1. 1. 代码

###### /*

###### 国内单程、国内双程、国际单程、国际双程查询爬取频次

###### */

val SpiderRuleByTravel =
request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).groupBy(request 1 ("flightType"),
request 1 ("travelType")).count()
println("国内单程、国内双程、国际单程、国际双程查询爬取频次")
SpiderRuleByTravel.show()

## 64. 1. 2. 对接 web 端

###### 练习

13 、 找到web端对应的代码
14 、 找到web端对应的表结构
15 、 编写代码实现web端对接


# 65. 爬虫对查定比的影响

## 65. 1. 爬虫用户查定比

## 65. 1. 1. 代码

###### /*

###### 爬虫用户查定比

###### */

val SpiderQueryBookRatio =
request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).filter(request 1 ("StageTag").equalTo(" 1 ")).cou
nt().toFloat /
request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).filter(request 1 ("StageTag").equalTo(" 0 ")).cou
nt().toFloat
println("爬虫用户查定比")
println(SpiderQueryBookRatio)

## 65. 1. 2. 对接 web 端

###### 练习

16 、 找到web端对应的代码
17 、 找到web端对应的表结构


18 、 编写代码实现web端对接

## 65. 2. 正常用户查定比

## 65. 2. 1. 代码

###### /*

###### 正常用户查定比

###### */

val NormalQueryBookRatio =
request 1 .filter(request 1 ("SpiderTag").equalTo(" 0 ")).filter(request 1 ("StageTag").equalTo(" 1 ")).cou
nt().toFloat /
request 1 .filter(request 1 ("SpiderTag").equalTo(" 0 ")).filter(request 1 ("StageTag").equalTo(" 0 ")).cou
nt().toFloat
println("正常用户查定比")
println(NormalQueryBookRatio)

## 65. 2. 2. 对接 web 端

###### 练习

19 、 找到web端对应的代码
20 、 找到web端对应的表结构
21 、 编写代码实现web端对接


# 66. 爬虫对系统稳定性的影响

## 66. 1. 爬虫流量情况

## 66. 1. 1. 代码

###### /**

###### * 爬虫对系统稳定性的影响

###### */

###### /*

###### 爬虫流量情况

###### */

valSpiderFlowVolumn=request 1 .filter(request 1 ("SpiderTag").equalTo(" 1 ")).count()
println("爬虫流量情况")
println(SpiderFlowVolumn)

## 66. 1. 2. 对接 web 端

###### 练习

22 、 找到web端对应的代码
23 、 找到web端对应的表结构
24 、 编写代码实现web端对接


## 66. 2. 正常流量情况

## 66. 2. 1. 代码

###### /*

###### 正常流量情况

###### */

```
valNormalFlowVolumn=request 1 .filter(request 1 ("SpiderTag").equalTo(" 0 ")).count()
println("正常流量情况")
println(NormalFlowVolumn)
```
## 66. 2. 2. 对接 web 端

###### 练习

25 、 找到web端对应的代码
26 、 找到web端对应的表结构
27 、 编写代码实现web端对接


# 67. 附：测试数据改装

## 67. 1. 需求

###### 在我们产生的真实测试文件中，并不包含我们离线分析的很多指标，如：国际查询，普

通用户查询等，我们可以通过下面的程序将我们通过lua生成的测试数据进行一次改装，变
成供离线统计使用的数据。
下面的代码中，我们是将hdfs文件下载到了本地进行操作，当然你也可以直接对hds
的文件进行操作，将每一天的数据重新合并到一个文件中，使用repartition函数即可做到，
同时合并结束后，你可以删除原有的每小时的文件。在这里我们就不提供这样的代码了，因
为这些代码极其简单。
下面我们只提供改装测试数据的代码，规则比较简单，我们采取抽样的形式修改数据，
以满足我们图形显示的需要。当然，在真实数据的情况下，会和我们改装后的数据类似，但
是数据的形式可能会更离散化，而不是像我们改装的那么有规律。

## 67. 2. 代码

packagecom.csair.b 2 c.gciantispider.offlinecalculation.data

importcom.csair.b 2 c.gciantispider.offlinecalculation.util.LoggerLevels
importorg.apache.spark.{SparkConf,SparkContext}
/**
*Createdbyitheima */
objectDataTransformation{

defmain(args:Array[String]):Unit={
LoggerLevels.setStreamingLogLevels()
val conf = new
SparkConf().setMaster("local").setAppName("visualization").setMaster("local[*]")
valsc=newSparkContext(conf)
vari= 0
println("/ita/intl/zh/shop/ss".matches("^.*/ita/intl/zh/shop/.*$"))
println("/modules/permissionnew/csair".matches("^.*/modules/permissionnew/.*$"))
println("upp_payment/pay/csair".matches("^.*upp_payment/pay/.*$"))
valrequest=sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资
料\\数据样本\\part- 00000 ")
.map(x=>{
vary=x
if(i% 50 == 0 ){
y=y.replace("/B 2 C 40 /query/jaxb/direct/query.ao","/ita/intl/zh/shop/csair")
}


if(i% 201 == 0 ){
y = y.replace("/B 2 C 40 /query/jaxb/direct/query.ao",
"/modules/permissionnew/csair").replace(" 192. 168. 56. 1 "," 243. 234. 12. 43 ")
}
if(i% 701 == 0 ){
y = y.replace("/B 2 C 40 /query/jaxb/direct/query.ao",
"/modules/permissionnew/csair")
}
if(i% 1001 == 0 ){
y=y.replace("/B 2 C 40 /query/jaxb/direct/query.ao","upp_payment/pay/csair")
}
if(i% 2001 == 0 ){
y=y.replace(" 192. 168. 56. 1 "," 243. 234. 12. 43 ")
}
if(i% 200 == 0 ||i% 402 == 0 ||i% 2002 == 0 ||i% 502 == 0 ){
y=y.replace("National","Internatinal")
}
i=i+ 1
y
}
).repartition( 1 ).saveAsTextFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案
\\参考资料\\数据样本\\test")
//sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资料\\数据样
本\\test").filter(_.matches("^.*/ita/intl/zh/shop/.*$")).foreach(println)
//sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资料\\数据样
本\\test").filter(_.matches("^.*/modules/permissionnew/.*$")).foreach(println)
//sc.textFile("E:\\here\\工作文档\\ 23 、项目研发\\反爬虫项目\\教案\\参考资料\\数据样
本\\test").filter(_.matches("^.*upp_payment/pay/.*$")).foreach(println)
}
}